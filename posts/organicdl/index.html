<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alicia">
<meta name="dcterms.date" content="2025-03-12">

<title>PyTorch: Deep Learning Organic Chemistry – Alicia’s Side Projects</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Alicia’s Side Projects</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../ai.html"> 
<span class="menu-text">AI &amp; Deep Learning</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../data-analysis.html"> 
<span class="menu-text">Data Analysis</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/akey7"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@aliciascience"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/akey7/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">PyTorch: Deep Learning Organic Chemistry</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">pytorch</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">chemistry</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Alicia </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 12, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#technology-used-source-code-and-image-source" id="toc-technology-used-source-code-and-image-source" class="nav-link" data-scroll-target="#technology-used-source-code-and-image-source">Technology used, source code, and image source</a></li>
  <li><a href="#dataset-description-and-classification-problem" id="toc-dataset-description-and-classification-problem" class="nav-link" data-scroll-target="#dataset-description-and-classification-problem">Dataset description and classification problem</a></li>
  <li><a href="#network-architecture" id="toc-network-architecture" class="nav-link" data-scroll-target="#network-architecture">Network architecture</a></li>
  <li><a href="#size-of-dataset" id="toc-size-of-dataset" class="nav-link" data-scroll-target="#size-of-dataset">Size of dataset</a></li>
  <li><a href="#initial-training-loss-history-binary-crossentropy" id="toc-initial-training-loss-history-binary-crossentropy" class="nav-link" data-scroll-target="#initial-training-loss-history-binary-crossentropy">Initial Training Loss History (Binary Crossentropy)</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a>
  <ul class="collapse">
  <li><a href="#binary-classification-scores" id="toc-binary-classification-scores" class="nav-link" data-scroll-target="#binary-classification-scores">Binary classification scores:</a></li>
  <li><a href="#false-negative-cases" id="toc-false-negative-cases" class="nav-link" data-scroll-target="#false-negative-cases">False negative cases</a></li>
  <li><a href="#false-positive-cases" id="toc-false-positive-cases" class="nav-link" data-scroll-target="#false-positive-cases">False positive cases</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#python-source" id="toc-python-source" class="nav-link" data-scroll-target="#python-source">Python source</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional neural networks</a> (CNNs) are a deep learning technology to use for classifying images. For this demonstration I used images from an introductory organic chemistry class. My problem was one of binary classification: could the CNN distinguish images with a structure called a benzene ring from images without a benzene ring? While I encountered challenges of working with a small dataset (with 205 images in each class), I did train the CNN to 76% accuracy on the test data (see complete metrics below).</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<section id="technology-used-source-code-and-image-source" class="level3">
<h3 class="anchored" data-anchor-id="technology-used-source-code-and-image-source">Technology used, source code, and image source</h3>
<p>The software I used was Python, TensorFlow, and Keras. <a href="https://github.com/akey7/organicml">The source code is on GitHub</a> along with all the training and test images. I obtained 410 images of molecular structures by taking photographs at various angles under many lighting conditions from the text <em>Organic Chemistry, 6th Edition by John McMurry</em> along with its accompanying student solutions manual. For each image, I cropped out all the text and converted the images to grayscale so that the classifier would not learn color as an indicator in the classification task.</p>
</section>
<section id="dataset-description-and-classification-problem" class="level3">
<h3 class="anchored" data-anchor-id="dataset-description-and-classification-problem">Dataset description and classification problem</h3>
<p>A detailed description of the organic chemistry involved in the training, test and validation datasets is beyond the scope of this article; however, a quick explanation about the data, as well as why the data have meaning in chemistry applications, should prove useful in understanding the CNN setup described here. An important part of studying chemical molecules involves breaking them into smaller substructures and analyzing each substructure individually. By understanding how each part works in the entire chemical structure, one can understand the properties of the compound as a whole. One important strucuture in organic chemistry is called a <a href="https://en.wikipedia.org/wiki/Aromatic_hydrocarbon">“benzene ring”</a>. When the ring is found in a standalone configuration, with no other atoms surrounding it, it forms the compound <a href="https://en.wikipedia.org/wiki/Benzene">benzene</a>. Here is a diagram of benzene:</p>
<p><img src="benzene.png" class="img-fluid"></p>
<section id="other-parts-of-a-molecule-can-be-attached-to-these-benzene-rings-as-shown-below" class="level4">
<h4 class="anchored" data-anchor-id="other-parts-of-a-molecule-can-be-attached-to-these-benzene-rings-as-shown-below">Other parts of a molecule can be attached to these benzene rings, as shown below:</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="benzene_ring.0024.jpg" class="img-fluid"></td>
<td><img src="benzene_ring.0044.jpg" class="img-fluid"></td>
<td><img src="benzene_ring.0036.jpg" class="img-fluid" alt="3-Bromobenzoic acid"></td>
</tr>
</tbody>
</table>
</section>
<section id="other-compounds-do-not-contain-these-rings-as-seen-in-the-following-molecules" class="level4">
<h4 class="anchored" data-anchor-id="other-compounds-do-not-contain-these-rings-as-seen-in-the-following-molecules">Other compounds do not contain these rings, as seen in the following molecules:</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<tbody>
<tr class="odd">
<td><img src="non_benzene_ring.0183.jpg" class="img-fluid"></td>
<td><img src="non_benzene_ring.9916.jpg" class="img-fluid"></td>
<td><img src="non_benzene_ring.9927.jpg" class="img-fluid"></td>
</tr>
</tbody>
</table>
<p>The task for the CNN was binary classification to distinguish images of molecules with benzene rings from those without benzene rings.</p>
</section>
</section>
<section id="network-architecture" class="level3">
<h3 class="anchored" data-anchor-id="network-architecture">Network architecture</h3>
<p>Below is a snippet of Python code that defines the network (the complete code is at the bottom of this document). Basically, it consists of frozen VGG16 layers with linear classification layers on top of the VGG16 layers.</p>
<pre><code>class Net(nn.Module):
    def __init__(self, pretrained=True, freeze=True):
        super(Net, self).__init__()
        self.vgg16_layers = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
        if freeze:
            for param in self.vgg16_layers.parameters():
                param.requires_grad = False
        dummy_input = torch.randn(1, 3, 256, 256) # Assume input size is 256x256
        output_size = self.vgg16_layers(dummy_input).view(1, -1).shape[1]
        self.classifier = nn.Sequential(
            nn.Linear(output_size, 256),
            nn.Linear(256, 128),
            nn.Linear(128, 64),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.vgg16_layers(x)
        x = torch.flatten(x, start_dim=1)
        return self.classifier(x)</code></pre>
</section>
<section id="size-of-dataset" class="level3">
<h3 class="anchored" data-anchor-id="size-of-dataset">Size of dataset</h3>
<p>In each of the train and test datasets I kept the classes roughly balanced, as shown in the table below:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Set</th>
<th>Positive (benzene ring) class</th>
<th>Negative class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Train</td>
<td>151</td>
<td>149</td>
</tr>
<tr class="even">
<td>Test</td>
<td>57</td>
<td>53</td>
</tr>
</tbody>
</table>
</section>
<section id="initial-training-loss-history-binary-crossentropy" class="level3">
<h3 class="anchored" data-anchor-id="initial-training-loss-history-binary-crossentropy">Initial Training Loss History (Binary Crossentropy)</h3>
<p><img src="initial_training_loss_history.png" class="img-fluid"></p>
<p>For the initial runs to determine how many epochs create the best validation loss, I trained the model for 50 epochs. Ultiamtely, I chose 9 epochs for the final training because that was the lowest training loss before it the model started to overfit.</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="binary-classification-scores" class="level3">
<h3 class="anchored" data-anchor-id="binary-classification-scores">Binary classification scores:</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>0.7636</td>
</tr>
<tr class="even">
<td>Precision</td>
<td>0.7067</td>
</tr>
<tr class="odd">
<td>Recall</td>
<td>0.9298</td>
</tr>
<tr class="even">
<td>F1</td>
<td>0.8030</td>
</tr>
</tbody>
</table>
</section>
<section id="false-negative-cases" class="level3">
<h3 class="anchored" data-anchor-id="false-negative-cases">False negative cases</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Image</th>
<th>What the model got wrong</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="misclassified_image_0_1_14.png" class="img-fluid"></td>
<td>-OH and -NO2 groups confused the model.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_0_1_2.png" class="img-fluid"></td>
<td>-NO2 groups confused the model.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_0_1_20.png" class="img-fluid"></td>
<td>-CH3 and -Cl groups confused the model.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_0_1_8.png" class="img-fluid"></td>
<td>-OH and -NO2 groups confused the model.</td>
</tr>
</tbody>
</table>
</section>
<section id="false-positive-cases" class="level3">
<h3 class="anchored" data-anchor-id="false-positive-cases">False positive cases</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>Image</th>
<th>What the model got wrong</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="misclassified_image_1_0_1.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_10.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_11.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_12.png" class="img-fluid"></td>
<td>A pentagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_13.png" class="img-fluid"></td>
<td>A pentagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_15.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_16.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_17.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_18.png" class="img-fluid"></td>
<td>A single double bond is not correct.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_19.png" class="img-fluid"></td>
<td>This one is way off base.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_21.png" class="img-fluid"></td>
<td>A single double bond is not correct.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_22.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_23.png" class="img-fluid"></td>
<td>A single double bond is not correct.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_24.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_25.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_26.png" class="img-fluid"></td>
<td>A single double bond is not correct.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_3.png" class="img-fluid"></td>
<td>A pentagon with an O in it is incorrect.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_4.png" class="img-fluid"></td>
<td>A hexagon with single lines does not represent alternating double bonds.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_5.png" class="img-fluid"></td>
<td>A single double bond is not correct.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_6.png" class="img-fluid"></td>
<td>A single double bond is not correct.</td>
</tr>
<tr class="odd">
<td><img src="misclassified_image_1_0_7.png" class="img-fluid"></td>
<td>This one is way off base.</td>
</tr>
<tr class="even">
<td><img src="misclassified_image_1_0_9.png" class="img-fluid"></td>
<td>A double bond and a triple bond with a hexagon are not correct.</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>False positives dominated the model’s classification errors, where it seemed to confuse single-line hexagons and pentagons for benzene rings. On the flase negative side, too many groups off the benzene ring confused the model into false negative cases. If I were to do this project again in the future, I would want to get many more images for training and testing. I think that this model had problems due to the small dataset.</p>
</section>
<section id="python-source" class="level2">
<h2 class="anchored" data-anchor-id="python-source">Python source</h2>
<p>This is the source code that trained the network in a Google Colab notebook. Note that I did a lot of performance monitoring (to ensure I was using the GPU to its full potential) that I did not show in this document.</p>
<pre><code>"""OrganicDL Classifier V03.ipynb

Automatically generated by Colab.

Original file is located at
    REDACTED

## Organic DL v02: Classifying images of molecules with a VGG16 based classifier using PyTorch

First, setup the environment by determining the CUDA version, installing the dependencies with pip, and verifying GPU availability.
"""

!nvcc --version

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') &gt;= 0:
    print('Not connected to a GPU')
else:
    print(gpu_info)

from psutil import virtual_memory
ram_gb = virtual_memory().total / 1e9
print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))

if ram_gb &lt; 20:
    print('Not using a high-RAM runtime')
else:
    print('You are using a high-RAM runtime!')

!pip install torchmetrics

"""### Import necessary modules"""

import os
import random
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import ImageFolder
from torchvision import transforms
from torchmetrics import Accuracy, Precision, Recall, F1Score
import torchvision.models as models
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive

"""### Report GPU environment"""

print("CUDA Available:", torch.cuda.is_available())
print("CUDA Version:", torch.version.cuda)
print("Current GPU:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None")

"""### Force PyTorch to use CUDA device 0"""

# Check if CUDA is available
print("CUDA available:", torch.cuda.is_available())

# If available, get device count and device names
if torch.cuda.is_available():
    print("Device count:", torch.cuda.device_count())
    for i in range(torch.cuda.device_count()):
        print(f"Device {i} name:", torch.cuda.get_device_name(i))

# Set default device to GPU if available, otherwise CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# To force operations to use GPU, move tensors to the device
x = torch.rand(5, 3)
x = x.to(device)  # This moves the tensor to GPU if available

# To verify a tensor is on GPU
print("Tensor is on CUDA:", x.is_cuda)

"""### Set the RNG seeds"""

def set_seed(seed=42):
    """Set all seeds to make results reproducible"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

    print(f"Random seed set as {seed}")

set_seed()

"""### Mount Google Drive

Training, test data is on Google Drive.
"""

drive.mount('/content/drive')

"""### Prepare data augmentation and image loader"""

train_transforms = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(45),
    transforms.ToTensor(),
    transforms.Resize((256, 256)),
])

test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((256, 256)),
])

dataset_train = ImageFolder('/content/drive/MyDrive/Colab Data/OrganicDL Train Test/train', transform=train_transforms)
dataset_test = ImageFolder('/content/drive/MyDrive/Colab Data/OrganicDL Train Test/test', transform=test_transforms)
dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)
dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=True)

"""### Look at sample images

One image each from the negative (non-benzene) and positive (benzene) classes.
"""

first_negative_image_idx = None
first_positive_image_idx = None

for idx, (image, label) in enumerate(dataset_train):
    if first_negative_image_idx is None and label == 0:
        first_negative_image_idx = idx
        break

for idx, (image, label) in enumerate(dataset_train):
    if first_positive_image_idx is None and label == 1:
        first_positive_image_idx = idx
        break

image, label = dataset_train[first_negative_image_idx]
plt.imshow(image.permute(1, 2, 0))
plt.title(f"Label: {label} (negative class)")
plt.savefig('/content/drive/MyDrive/Colab Data/OrganicDL Results/negative_class_image.png')
plt.show()

image, label = dataset_train[first_positive_image_idx]
plt.imshow(image.permute(1, 2, 0))
plt.title(f"Label: {label} (Positive Class)")
plt.savefig('/content/drive/MyDrive/Colab Data/OrganicDL Results/positive_class_image.png')
plt.show()

"""### Define the `Net` model class that will perform the classification"""

class Net(nn.Module):
    def __init__(self, pretrained=True, freeze=True):
        super(Net, self).__init__()
        self.vgg16_layers = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
        if freeze:
            for param in self.vgg16_layers.parameters():
                param.requires_grad = False
        dummy_input = torch.randn(1, 3, 256, 256) # Assume input size is 256x256
        output_size = self.vgg16_layers(dummy_input).view(1, -1).shape[1]
        self.classifier = nn.Sequential(
            nn.Linear(output_size, 256),
            nn.Linear(256, 128),
            nn.Linear(128, 64),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.vgg16_layers(x)
        x = torch.flatten(x, start_dim=1)
        return self.classifier(x)

"""### Train the model, part 1

Train the model for a lot of epochs to find where it starts to overfit. Below, a fresh model will be trained for final evaluation.
"""

def train_model(num_epochs=16):
    # Enable benchmarking for better CUDA performance
    torch.backends.cudnn.benchmark = True

    # Make a new model
    model = Net().to(device)

    # Move criterion to device
    criterion = nn.BCELoss().to(device)
    print(f"Model is on GPU: {next(model.parameters()).is_cuda}")
    optimizer = optim.Adam(model.parameters(), lr=1.0e-3)

    # Print batch size information
    print(f"Batch size: {dataloader_train.batch_size}")
    print(f"Training dataset size: {len(dataloader_train.dataset)}")
    print(f"Steps per epoch: {len(dataloader_train)}")

    epoch_running_losses = []
    for epoch in range(num_epochs):
        running_loss = 0.0
        # Reset GPU stats for this epoch
        torch.cuda.reset_peak_memory_stats()

        for i, data in enumerate(dataloader_train, 0):
            inputs, labels = data
            inputs = inputs.float().to(device)
            labels = labels.float().view(-1, 1).to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

            # Monitor GPU usage every 10 batches
            if i % 5 == 0:
                print(f"Batch {i}, GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")

        epoch_running_loss = running_loss / len(dataloader_train)
        epoch_running_losses.append(epoch_running_loss)
        print(f"Epoch {epoch + 1}, Loss: {epoch_running_loss}")
        print(f"Peak GPU memory in epoch: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB")

    # Final GPU stats
    print(f"Final GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")

    # Return the final model and training losses.
    return model, epoch_running_losses

initial_model, initial_epoch_running_losses = train_model(num_epochs=50)

"""### History of training losses"""

xs = np.arange(1, len(initial_epoch_running_losses) + 1)
plt.plot(xs, initial_epoch_running_losses)
plt.title("Initial Training Loss History (Binary Crossentropy)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.savefig('/content/drive/MyDrive/Colab Data/OrganicDL Results/initial_training_loss_history.png')
plt.show()

"""### Train final model

Train the final model, stopping at the epoch where the model starts to overfit.
"""

final_model, final_epoch_running_losses = train_model(num_epochs=9)

"""### Evaluate the final trained model"""

# Move metrics to device
metric_precision = Precision(task="binary").to(device)
metric_recall = Recall(task="binary").to(device)
metric_f1 = F1Score(task="binary").to(device)
metric_accuracy = Accuracy(task="binary").to(device)

# Prepare for evaluation
final_model.eval()
torch.cuda.reset_peak_memory_stats()
eval_start = time.time()

# Check dataloader batch size
print(f"Evaluation batch size: {dataloader_test.batch_size}")
print(f"Test dataset size: {len(dataloader_test.dataset)}")
print(f"Evaluation steps: {len(dataloader_test)}")

# On-CPU list of images that are misclassified so they can be saved for
# diagnostic purposes
misclassified_images = []

# Main evaluation loop
with torch.no_grad():
    for i, data in enumerate(dataloader_test, 0):
        # Move data to GPU
        inputs, labels = data
        inputs = inputs.float().to(device)  # Ensure correct data type
        labels = labels.to(device)

        # Forward pass
        outputs = final_model(inputs)

        # Proper handling for binary classification
        if outputs.shape[1] == 1:  # If output is [batch_size, 1] (sigmoid output)
            preds = (outputs &gt; 0.5).int().squeeze()
        else:  # If output is [batch_size, 2] (two-class output)
            _, preds = torch.max(outputs, 1)

        # Update metrics directly with tensors (keeping computation on GPU)
        metric_precision.update(preds, labels)
        metric_recall.update(preds, labels)
        metric_f1.update(preds, labels)
        metric_accuracy.update(preds, labels)

        # Print sample predictions (only for first few batches to avoid excessive GPU-CPU transfers)
        if i &lt; 2:
            print(f"\nSample predictions from batch {i}:")
            for j in range(min(5, len(preds))):
                print(f"  Prediction: {preds[j].item()}, Label: {labels[j].item()}")

        # Add misclassified images/data to the list
        misclassified = (preds != labels).nonzero(as_tuple=True)[0]
        for idx in misclassified:
            misclassified_images.append((inputs[idx].cpu(), preds[idx].item(), labels[idx].item()))

        # Periodically report GPU memory usage
        if i % 10 == 0:
            print(f"Batch {i}, GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")

# Force synchronization to ensure GPU operations complete
torch.cuda.synchronize()

# Compute final metrics (results stay on GPU until final .compute() call)
precision = metric_precision.compute().item()
recall = metric_recall.compute().item()
f1 = metric_f1.compute().item()
accuracy = metric_accuracy.compute().item()

# Report performance
eval_time = time.time() - eval_start
print("\nEvaluation Results:")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Accuracy: {accuracy:.4f}")
print(f"Evaluation completed in {eval_time:.2f} seconds")
print(f"Peak GPU memory: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB")

# Save misclassififed images
counter = 0
for misclassified_image in misclassified_images:
    counter += 1
    image, prediction, label = misclassified_image
    plt.imshow(image.permute(1, 2, 0))
    plt.title(f"Prediction: {prediction}, True Label: {label}")
    plt.savefig(f'/content/drive/MyDrive/Colab Data/OrganicDL Results/misclassified_image_{prediction}_{label}_{counter}.png')
    print(f"Saved misclassified image {counter} with prediction {prediction} and label {label}")</code></pre>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>