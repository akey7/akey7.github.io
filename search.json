[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "akey7",
    "section": "",
    "text": "Julia: Molecular Dynamics of HCl\n\n\n\n\n\n\njulia\n\n\nphysics\n\n\nchemistry\n\n\n\n\n\n\n\n\n\nMar 23, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nJulia: Positive and Negative Definite Matrices\n\n\n\n\n\n\njulia\n\n\nmath\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Simple Gradient Descent\n\n\n\n\n\n\npython\n\n\nmath\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nR: FFT with fftpipe\n\n\n\n\n\n\nr\n\n\nmath\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Lennard-Jones Potential\n\n\n\n\n\n\npython\n\n\nphysics\n\n\nchemistry\n\n\n\n\n\n\n\n\n\nSep 10, 2022\n\n\nAlicia Key\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Hydrogen Atom\n\n\n\n\n\n\npython\n\n\nphysics\n\n\nchemistry\n\n\n\n\n\n\n\n\n\nJul 16, 2021\n\n\nAlicia Key\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Rydberg Equation and Hydrogen Spectra\n\n\n\n\n\n\npython\n\n\nphysics\n\n\nchemistry\n\n\n\n\n\n\n\n\n\nJul 15, 2021\n\n\nAlicia Key\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Quantum harmonic oscillator\n\n\n\n\n\n\npython\n\n\nphysics\n\n\nchemistry\n\n\n\n\n\n\n\n\n\nJul 13, 2021\n\n\nAlicia Key\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Quantum mechanics: two non-interacting particles\n\n\n\n\n\n\npython\n\n\nphysics\n\n\nchemistry\n\n\n\n\n\n\n\n\n\nJul 11, 2021\n\n\nAlicia Key\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Quantum mechanics: particle in a box\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nJul 5, 2021\n\n\nAlicia Key\n\n\n\n\n\n\n\n\n\n\n\n\nR: Solubility Clustering\n\n\n\n\n\n\nchemistry\n\n\nr\n\n\nsolubility\n\n\n\n\n\n\n\n\n\nApr 25, 2021\n\n\nAlicia Key\n\n\n\n\n\n\n\n\n\n\n\n\nR: Solubility Regression with Linear and Random Forest Models\n\n\n\n\n\n\nchemistry\n\n\nr\n\n\nsolubility\n\n\n\n\n\n\n\n\n\nJan 18, 2021\n\n\nAlicia Key\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "The Rydberg formula is used to predict emission spectrum lines from hydrogen. The significance of the Rydberg formula is that it was one of the first studies of quantum effects of energy transitions in atoms. Furthermore, it demonstrates that energy emitted is in specific wavelengths, corresponding to a particular energy level transition.\n\\[ \\tilde\\nu = R_H\\Biggl(\\frac{1}{n_{1}^2} + \\frac{1}{n_{2}^2}\\Biggr)\\]\nWhere \\(\\) is the wavenumber in \\(cm^{-1}\\) and \\(R_H = 109,677 cm^{-1}\\). I transform these wavenumbers to nanometers with the equation \\(= \\).\nIn this post, I create spectrum lines using the Rydberg equation and create plots of the series named after the physicists that discovered them.\nPython code to generate the plots is spread throughout the post.\n\nimport matplotlib.pyplot as plt\n\ndef rydberg_nm(n1, n2):\n    \"\"\"\n    Calculates the Rydberg wavenumber between n1 and n2.\n\n    Parameters\n    ----------\n    n1: int\n        The n1 level\n\n    n2: int\n        The n2 level\n\n    Returns\n    -------\n    float\n        Nanometer wavelength of the transition\n    \"\"\"\n    rh = 109677  # cm^-1, Rydberg constant \n    t1 = 1 / n1 ** 2\n    t2 = 1 / n2 ** 2\n    wavenumber = rh * (t1 - t2)\n    return 1 / wavenumber * 1e7\n\n\n\nhttps://en.wikipedia.org/wiki/Lyman_series\nFigure 1 is the Lyman series of hydrogen spectrum lines, as calculated by the Rydberg formula.\nn1=1, n2=2, nm=121.56909227398026\nn1=1, n2=3, nm=102.57392160617086\nn1=1, n2=4, nm=97.25527381918421\nn1=1, n2=5, nm=94.97585333904709\nn1=1, n2=6, nm=93.78187118278477\nn1=1, n2=7, nm=93.07633627226615\nn1=1, n2=8, nm=92.62407030398496\nn1=1, n2=9, nm=92.31652944555377\nn1=1, n2=10, nm=92.09779717725777\nn1=1, n2=11, nm=91.93662603219758\n\nlyman_n2s = range (2, 12)\nlyman = [rydberg_nm(1, n2) for n2 in lyman_n2s]\nfor nm, n2 in zip(lyman, lyman_n2s):\n    print(f'n1=1, n2={n2}, nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Lyman Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(90, 130)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in lyman:\n    ax.axvline(nm, color='r')\n\nn1=1, n2=2, nm=121.56909227398026\nn1=1, n2=3, nm=102.57392160617086\nn1=1, n2=4, nm=97.25527381918421\nn1=1, n2=5, nm=94.97585333904709\nn1=1, n2=6, nm=93.78187118278477\nn1=1, n2=7, nm=93.07633627226615\nn1=1, n2=8, nm=92.62407030398496\nn1=1, n2=9, nm=92.31652944555377\nn1=1, n2=10, nm=92.09779717725777\nn1=1, n2=11, nm=91.93662603219758\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Balmer_series\nFigure 2 is the Balmer series of hydrogen spectral lines, as calculated by the Rydberg formula.\nn1=2, n2=3, nm=656.4730982794933\nn1=2, n2=4, nm=486.27636909592104\nn1=2, n2=5, nm=434.1753295499296\nn1=2, n2=6, nm=410.2956864246834\nn1=2, n2=7, nm=397.12570142833556\nn1=2, n2=8, nm=389.02109527673684\nn1=2, n2=9, nm=383.6531093841195\n\nbalmer_n2s = range(3, 10)\nbalmer = [rydberg_nm(2, n2) for n2 in balmer_n2s]\nfor nm, n2 in zip(balmer, balmer_n2s):\n    print(f'n1=2, n2={n2}, nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Balmer Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(350, 700)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in balmer:\n    ax.axvline(nm, color='r')\n\nn1=2, n2=3, nm=656.4730982794933\nn1=2, n2=4, nm=486.27636909592104\nn1=2, n2=5, nm=434.1753295499296\nn1=2, n2=6, nm=410.2956864246834\nn1=2, n2=7, nm=397.12570142833556\nn1=2, n2=8, nm=389.02109527673684\nn1=2, n2=9, nm=383.6531093841195\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://en.wikipedia.org/wiki/Hydrogen_spectral_series#Paschen_series_(Bohr_series,n′=_3)\nFigure 3 is the Paschen series of spectral lines as calculated by the Rydberg formula.\nn1=2, n2=4 nm=1875.6374236556956\nn1=2, n2=5 nm=1282.1740200771358\nn1=2, n2=6 nm=1094.1218304658223\nn1=2, n2=7 nm=1005.2244317404744\nn1=2, n2=8 nm=954.8699611338087\n\npaschen_n2s = range(4, 9)\npaschen = [rydberg_nm(3, n2) for n2 in paschen_n2s]\nfor nm, n2 in zip(paschen, paschen_n2s):\n    print(f'n1=2, n2={n2} nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Paschen Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(800, 1900)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in paschen:\n    ax.axvline(nm, color='r')\n\nn1=2, n2=4 nm=1875.6374236556956\nn1=2, n2=5 nm=1282.1740200771358\nn1=2, n2=6 nm=1094.1218304658223\nn1=2, n2=7 nm=1005.2244317404744\nn1=2, n2=8 nm=954.8699611338087\n\n\n\n\n\n\n\n\n\n\n\n\nSee Physical Chemistry, 8th ed by Atkins and de Paula, page 320 for the Rydberg equation in wavenumbers. On https://www.powertechnology.com/calculators/ I found that you could convert wavenumbers in inverse centimeters to nanometers with \\(= \\), with wavelength in nanometers (search for the keyword “nanometer”.) Wikipedia contains an extensive article about the Rydberg formula and its history back to the 1880s https://en.wikipedia.org/wiki/Rydberg_formula"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html#lyman-series",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html#lyman-series",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Lyman_series\nFigure 1 is the Lyman series of hydrogen spectrum lines, as calculated by the Rydberg formula.\nn1=1, n2=2, nm=121.56909227398026\nn1=1, n2=3, nm=102.57392160617086\nn1=1, n2=4, nm=97.25527381918421\nn1=1, n2=5, nm=94.97585333904709\nn1=1, n2=6, nm=93.78187118278477\nn1=1, n2=7, nm=93.07633627226615\nn1=1, n2=8, nm=92.62407030398496\nn1=1, n2=9, nm=92.31652944555377\nn1=1, n2=10, nm=92.09779717725777\nn1=1, n2=11, nm=91.93662603219758\n\nlyman_n2s = range (2, 12)\nlyman = [rydberg_nm(1, n2) for n2 in lyman_n2s]\nfor nm, n2 in zip(lyman, lyman_n2s):\n    print(f'n1=1, n2={n2}, nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Lyman Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(90, 130)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in lyman:\n    ax.axvline(nm, color='r')\n\nn1=1, n2=2, nm=121.56909227398026\nn1=1, n2=3, nm=102.57392160617086\nn1=1, n2=4, nm=97.25527381918421\nn1=1, n2=5, nm=94.97585333904709\nn1=1, n2=6, nm=93.78187118278477\nn1=1, n2=7, nm=93.07633627226615\nn1=1, n2=8, nm=92.62407030398496\nn1=1, n2=9, nm=92.31652944555377\nn1=1, n2=10, nm=92.09779717725777\nn1=1, n2=11, nm=91.93662603219758"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html#balmer-series",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html#balmer-series",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Balmer_series\nFigure 2 is the Balmer series of hydrogen spectral lines, as calculated by the Rydberg formula.\nn1=2, n2=3, nm=656.4730982794933\nn1=2, n2=4, nm=486.27636909592104\nn1=2, n2=5, nm=434.1753295499296\nn1=2, n2=6, nm=410.2956864246834\nn1=2, n2=7, nm=397.12570142833556\nn1=2, n2=8, nm=389.02109527673684\nn1=2, n2=9, nm=383.6531093841195\n\nbalmer_n2s = range(3, 10)\nbalmer = [rydberg_nm(2, n2) for n2 in balmer_n2s]\nfor nm, n2 in zip(balmer, balmer_n2s):\n    print(f'n1=2, n2={n2}, nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Balmer Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(350, 700)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in balmer:\n    ax.axvline(nm, color='r')\n\nn1=2, n2=3, nm=656.4730982794933\nn1=2, n2=4, nm=486.27636909592104\nn1=2, n2=5, nm=434.1753295499296\nn1=2, n2=6, nm=410.2956864246834\nn1=2, n2=7, nm=397.12570142833556\nn1=2, n2=8, nm=389.02109527673684\nn1=2, n2=9, nm=383.6531093841195"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html#paschen-series",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html#paschen-series",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "See https://en.wikipedia.org/wiki/Hydrogen_spectral_series#Paschen_series_(Bohr_series,n′=_3)\nFigure 3 is the Paschen series of spectral lines as calculated by the Rydberg formula.\nn1=2, n2=4 nm=1875.6374236556956\nn1=2, n2=5 nm=1282.1740200771358\nn1=2, n2=6 nm=1094.1218304658223\nn1=2, n2=7 nm=1005.2244317404744\nn1=2, n2=8 nm=954.8699611338087\n\npaschen_n2s = range(4, 9)\npaschen = [rydberg_nm(3, n2) for n2 in paschen_n2s]\nfor nm, n2 in zip(paschen, paschen_n2s):\n    print(f'n1=2, n2={n2} nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Paschen Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(800, 1900)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in paschen:\n    ax.axvline(nm, color='r')\n\nn1=2, n2=4 nm=1875.6374236556956\nn1=2, n2=5 nm=1282.1740200771358\nn1=2, n2=6 nm=1094.1218304658223\nn1=2, n2=7 nm=1005.2244317404744\nn1=2, n2=8 nm=954.8699611338087"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html#references",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html#references",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "See Physical Chemistry, 8th ed by Atkins and de Paula, page 320 for the Rydberg equation in wavenumbers. On https://www.powertechnology.com/calculators/ I found that you could convert wavenumbers in inverse centimeters to nanometers with \\(= \\), with wavelength in nanometers (search for the keyword “nanometer”.) Wikipedia contains an extensive article about the Rydberg formula and its history back to the 1880s https://en.wikipedia.org/wiki/Rydberg_formula"
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html",
    "href": "posts/quantum-harmonic-oscillator/index.html",
    "title": "Python: Quantum harmonic oscillator",
    "section": "",
    "text": "In this post, I will define Python code that models the quantum harmonic oscillator. This page follows page 290 to 297 in Physical Chemistry, 8th Ed. by Peter Atkins and Julio de Paula for the math to create and examples to test the code in this post.\nThe following equations describe its energy levels:\n\\[ \\omega = \\sqrt{\\frac{k}{m}} \\]\n\\[ E_{\\nu} = \\Bigl(\\nu + \\frac{1}{2}\\Bigr) \\hbar \\omega \\]\n\\[ \\nu = 0, 1, 2, \\dots \\]\nThe following equations describe its wavefunction:\n\\[ \\alpha = \\Biggl(\\frac{\\hbar^2}{mk}\\Biggr)^{1/4} \\]\n\\[ \\gamma = \\frac{x}{\\alpha} \\]\n\\[ \\psi_{\\nu}(x) = N_{\\nu} H_{\\nu}(\\gamma) e^{-\\gamma^{2}/2} \\]\nwhere \\(H\\) is a Hermite polynomial. The form of the first six Hermite polynomials are on page 293 of the text and are also implemented in the QuantumHarmonicOscillator class shown in the Python source code section below. Here is a sampling of the first three:\n\\[ H_0(\\gamma) = 1 \\]\n\\[ H_1(\\gamma) = 2 \\gamma \\]\n\\[ H_2(\\gamma) = 4 \\gamma^2 - 2 \\]\nExample 9.3 on page 294 demonstrates the derivation of the normalization constant \\(N_{}\\), which results in the following expression for the normalization constant:\n\\[ N_{\\nu} = \\sqrt{\\frac{1}{\\alpha \\pi^{1/2} 2^{\\nu} \\nu!}} \\]"
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html#introduction",
    "href": "posts/quantum-harmonic-oscillator/index.html#introduction",
    "title": "Python: Quantum harmonic oscillator",
    "section": "",
    "text": "In this post, I will define Python code that models the quantum harmonic oscillator. This page follows page 290 to 297 in Physical Chemistry, 8th Ed. by Peter Atkins and Julio de Paula for the math to create and examples to test the code in this post.\nThe following equations describe its energy levels:\n\\[ \\omega = \\sqrt{\\frac{k}{m}} \\]\n\\[ E_{\\nu} = \\Bigl(\\nu + \\frac{1}{2}\\Bigr) \\hbar \\omega \\]\n\\[ \\nu = 0, 1, 2, \\dots \\]\nThe following equations describe its wavefunction:\n\\[ \\alpha = \\Biggl(\\frac{\\hbar^2}{mk}\\Biggr)^{1/4} \\]\n\\[ \\gamma = \\frac{x}{\\alpha} \\]\n\\[ \\psi_{\\nu}(x) = N_{\\nu} H_{\\nu}(\\gamma) e^{-\\gamma^{2}/2} \\]\nwhere \\(H\\) is a Hermite polynomial. The form of the first six Hermite polynomials are on page 293 of the text and are also implemented in the QuantumHarmonicOscillator class shown in the Python source code section below. Here is a sampling of the first three:\n\\[ H_0(\\gamma) = 1 \\]\n\\[ H_1(\\gamma) = 2 \\gamma \\]\n\\[ H_2(\\gamma) = 4 \\gamma^2 - 2 \\]\nExample 9.3 on page 294 demonstrates the derivation of the normalization constant \\(N_{}\\), which results in the following expression for the normalization constant:\n\\[ N_{\\nu} = \\sqrt{\\frac{1}{\\alpha \\pi^{1/2} 2^{\\nu} \\nu!}} \\]"
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html#the-python-class-for-the-harmonic-oscillator",
    "href": "posts/quantum-harmonic-oscillator/index.html#the-python-class-for-the-harmonic-oscillator",
    "title": "Python: Quantum harmonic oscillator",
    "section": "The Python class for the harmonic oscillator",
    "text": "The Python class for the harmonic oscillator\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.integrate import quad\nfrom math import pi, sqrt, exp, factorial\n\n\nclass QuantumHarmonicOscillator:\n    \"\"\"\n    This models a harmonic oscillator. Parameters used throughout the methods\n    are:\n\n    v: The quantum of the harmonic oscillator\n    k: The force constant\n    mass_r: The reduced mass of the system. If you need to calculate this for a \n            diatomic molecule, see the diatomic_reduced_mass() method below.\n\n    The nomenclature of variable names follows Atkins and de Paula Physical\n    Chemistry 8th ed, pp. 290-297.\n\n    Atkins, P. W. & De Paula, J. Atkins’ Physical chemistry. (W.H. Freeman, 2006).\n    \"\"\"\n\n    def __init__(self, mass_r, k):\n        \"\"\"\n        Setup the values used by all methods in this model.\n\n        Parameters\n        ----------\n        mass: float\n            The reduced mass of the system, in kg\n\n        k: float\n            The force constant, in N/m\n        \"\"\"\n        self.hbar = 1.054571817e-34\n        self.k = k\n        self.mass_r = mass_r\n        self.omega = sqrt(k / mass_r)\n        self.max_nu = 6\n\n    @staticmethod\n    def diatomic_reduced_mass(mass_1, mass_2):\n        \"\"\"\n        Computes the reduced mass of a diatomic system. In order to provide\n        meaningful results, please use the exact isotopic mass for your molecule.\n\n        Parameters\n        ----------\n        mass_1: float\n            The mass of the first atom in kg.\n\n        mass_2: float\n            The mass of the second atom in kg.\n        \"\"\"\n        return mass_1*mass_2/(mass_1+mass_2)\n\n    def hermite(self, v, gamma):\n        \"\"\"\n        Returns the value of the v-th (nth) Hermite polynomial evaluated on gamma\n\n        The v and gamma notation follows Atkins' Physical Chemistry 8th ed.\n\n        Parameters\n        ----------\n        v: int\n            The v-th (nth) Hermite polynomial\n\n        gamma: float\n            The value to calculate with the Hermite polynomial\n\n        Returns\n        -------\n        float\n            The value of the v-th Hermite polynomial evaluated with gamma.\n\n        Raises\n        ------\n        Exception\n            Raises an exception if the nth Hermite polynomial is not\n            supported.\n        \"\"\"\n        if v == 0:\n            return 1\n        elif v == 1:\n            return 2 * gamma\n        elif v == 2:\n            return 4 * gamma**2 - 2\n        elif v == 3:\n            return 8 * gamma**3 - 12 * gamma\n        elif v == 4:\n            return 16 * gamma**4 - 48 * gamma**2 + 12\n        elif v == 5:\n            return 32 * gamma**5 - 160 * gamma**3 + 120 * gamma\n        elif v == 6:\n            return 64 * gamma**6 - 480 * gamma**4 + 720 * gamma**2 - 120\n        else:\n            raise Exception(f\"Hermite polynomial {v} is not supported\")\n\n    def max_v(self):\n        \"\"\"\n        Returns\n        -------\n        int\n            The maximum v value for the instance.\n        \"\"\"\n        return 6\n\n    def energy(self, v):\n        \"\"\"\n        Calculate the energy at the given level v of the system\n\n        Parameters\n        ----------\n        v: int\n            The quantum vmber v for the energy level of this system\n\n        Returns\n        -------\n        float\n            Energy of the system in Joules.\n        \"\"\"\n        return (v + 0.5) * self.hbar * self.omega\n\n    def energy_separation(self):\n        \"\"\"\n        Returns\n        -------\n        float\n            The energy difference between adjacent energy levels in Joules.\n        \"\"\"\n        return self.hbar * self.omega\n\n    def wavefunction(self, v, x):\n        \"\"\"\n        Returns the value of the wavefunction at energy level v\n        at coordinate x.\n\n        Parameters\n        ----------\n        v: float\n            Energy level of the system.\n\n        x: float\n            x coordinate of the particle in m.\n\n        Returns\n        -------\n        float\n            Value of the wavefunction v at x.\n        \"\"\"\n        alpha = (self.hbar**2 / self.mass_r / self.k) ** 0.25\n        gamma = x / alpha\n        normalization = sqrt(1 / (alpha * sqrt(pi) * 2**v * factorial(v)))\n        gaussian = exp((-(gamma**2)) / 2)\n        hermite = self.hermite(v, gamma)\n        return normalization * hermite * gaussian\n\n    def wavefunction_across_range(self, v, x_min, x_max, points=100):\n        \"\"\"\n        Calculates the wavefunction across a range.\n\n        Parameters\n        ----------\n        v: int\n            The quantum vmber of the system.\n\n        x_min: float\n            The minimum x value to calculate.\n\n        x_max: float\n            The maximum x value to calculate.\n\n        points: int\n            The vmber of points across the range\n\n        Returns\n        -------\n        np.array, list\n            The first array are the x coordinates, the second list are the\n            float values of the wavefunction.\n        \"\"\"\n        xs = np.linspace(x_min, x_max, points)\n        ys = [self.wavefunction(v, x) for x in xs]\n        return xs, ys\n\n    def prob_density(self, v, x_min, x_max, points=100):\n        \"\"\"\n        Returns the probability density between x_min and x_max for a given\n        vmber of points at energy level v.\n\n        Parameters\n        ----------\n        v: int\n            Quantum vmber of the system.\n\n        x_min: float\n            Minimum of length being calculated. Probably negative. Units are\n            meters.\n\n        x_max: float\n            Maximum of length being calculated. Probably positive. Units are\n            meters.\n\n        points: int\n            The vmber of points to compute the probability density for\n\n        Returns\n        -------\n        np.array, list\n            The first array is the list of x coordinates. The list are the\n            corresponding values of the probability density.\n        \"\"\"\n        xs = np.linspace(x_min, x_max, points)\n        ys = [self.wavefunction(v, x) ** 2 for x in xs]\n        return xs, ys\n\n    def integrate_prob_density_between_limits(self, v, x_min, x_max):\n        \"\"\"\n        As a way of testing the methods in this class, provide a way to\n        integrate across the wavefunction squared between limits.\n\n        Parameters\n        ----------\n        v: int\n            Quantum vmber of the system.\n\n        x_min: float\n            lower bound of integration\n\n        x_max: float\n            upper bound of integration\n        \"\"\"\n\n        def integrand(x):\n            return self.wavefunction(v, x) ** 2\n\n        result, _ = quad(integrand, x_min, x_max)\n        return result"
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html#test-the-class-with-1h35cl-properties",
    "href": "posts/quantum-harmonic-oscillator/index.html#test-the-class-with-1h35cl-properties",
    "title": "Python: Quantum harmonic oscillator",
    "section": "Test the class with 1H35Cl Properties",
    "text": "Test the class with 1H35Cl Properties\nThe 1H35Cl molecule has a bond with the following physical properties (the mass is of the proton in Hydrogen).\n\\[ k = 516.3 N/m \\]\n\\[ m = 1.7 \\times 10^{-27} kg \\]\n\nk = 516.3\nmass = 1.7e-27\nx_min = -0.5e-10\nx_max = 0.5e-10\npoints = 1000\nqho = QuantumHarmonicOscillator(k=k, mass_r=mass)\nnus = range(qho.max_nu + 1)\nfig, axs = plt.subplots(nrows=len(nus), ncols=2, figsize=(10, len(nus) * 5), sharex=True)\nfor idx, nu in enumerate(nus):\n    xs_wavefunction, ys_wavefunction = qho.wavefunction_across_range(nu, x_min, x_max, points)\n    xs_prob_density, ys_prob_density = qho.prob_density(nu, x_min, x_max, points)\n    integrated = qho.integrate_prob_density_between_limits(nu, x_min, x_max)\n    axs[nu, 0].set_title(f'ν={nu}', size=15, color='r')\n    axs[nu, 0].set_ylabel(f'Ψ{nu}(x)', size=15, color='b')\n    axs[nu, 0].set_xlabel('x', size=15)\n    axs[nu, 0].set_xlim(x_min, x_max)\n    axs[nu, 1].set_title(f'ν={nu}, ∫Ψ{nu}^2={round(integrated)}', size=15, color='r')\n    axs[nu, 1].set_ylabel(f'Ψ{nu}(x)^2', size=15, color='b')\n    axs[nu, 1].set_xlabel('x', size=15)\n    axs[nu, 1].set_xlim(x_min, x_max)\n    axs[nu, 0].plot(xs_wavefunction, ys_wavefunction)\n    axs[nu, 1].plot(xs_prob_density, ys_prob_density, color='g')\n\nplt.show()\n\n\n\n\n\n\n\n\nIn the plots of Figure 1, there are two columns. The left column is a plot of wavefunctions at different nu levels, each with a title indicating the level of the plot. The right column has plots of the squares of the wavefunctions. Like the left column plots, the titles of these plots contain the nu value that created them. In addition, the titles include an integral in their titles. These integrals all have a value of one, meaning each square of the wavefunction displays the probability throughout the entire space of wavefunction."
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html#testing-the-energy-level-calculation-from-illustration-9.3",
    "href": "posts/quantum-harmonic-oscillator/index.html#testing-the-energy-level-calculation-from-illustration-9.3",
    "title": "Python: Quantum harmonic oscillator",
    "section": "Testing the energy level calculation from Illustration 9.3",
    "text": "Testing the energy level calculation from Illustration 9.3\nUsing the 1H35Cl specifications from above, I will calculate the zero point energy and energy separation following Atkins and de Paula, Illustration 9.3. The 1H35Cl specifications are:\n\\[ k = 516.3 N/m \\]\n\\[ m = 1.7 \\times 10^{-27} kg \\]\nHowever, the book uses k=500 N/m and I will make that assumption to maintain consistency.\n\nk = 500\nmass = 1.7e-27\nmol = 6.022e23\nqho = QuantumHarmonicOscillator(k=k, mass_r=mass)\nenergy_sep_kj_mol = qho.energy_separation() * mol / 1000  # Convert to kJ/mol\nprint(f'energy sepration between ν and ν+1, {energy_sep_kj_mol} kJ/mol')\nzero_point_kj_mol = qho.energy(v=0) * mol / 1000\nprint(f'zero-point energy {zero_point_kj_mol} kJ/mol')\nfirst_excitation_thz = (qho.energy(v=1) - qho.energy(v=0)) / 6.626e-34 / 1e12\nprint(f'First excitation energy {first_excitation_thz} THz')\n\nenergy sepration between ν and ν+1, 34.44113487055476 kJ/mol\nzero-point energy 17.22056743527738 kJ/mol\nFirst excitation energy 86.31480043180733 THz\n\n\nFrom the answers given in the book, this gives my model the following differences.\n\ndelta_energy_sep = (round(energy_sep_kj_mol) / 34.0 - 1) * 100\nprint(f'delta energy separation {delta_energy_sep}%')\ndelta_zero_point = (round(zero_point_kj_mol) / 15.0 - 1) * 100\nprint(f'delta zero point energy {delta_zero_point}%')\ndelta_first_excitation = (round(first_excitation_thz) / 86.0 - 1) * 100\nprint(f'delta first excitation {delta_first_excitation}%')\n\ndelta energy separation 0.0%\ndelta zero point energy 13.33333333333333%\ndelta first excitation 0.0%"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html",
    "href": "posts/simple-gradient-descent/index.html",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "This is a simple polynomial gradient descent. It is a naive implmentation that is not optimized or vectorized. It is meant to be a simple demo on how gradient descent can be accomplished. To keep it simple, it uses base Python. I don’t intend for it to be used for anything important!\n\n\nThe “target” polynomial that will be fitted is defined as Eqn. 1:\n\\[ y_i = \\sum_{i=0}^{n} a_i x^i \\]\nThe “estimate” polynomial that will be fitted is defined the same way but with a hat of the y Eqn. 2:\n\\[ \\hat y_i = \\sum_{i=0}^{n} a_i x^i \\]\nTo fit the polynomial the code will minimize the residual sum of squares (RSS) function Eqn. 3:\n\\[ RSS = \\sum_{i=0}^{n} (y_i - \\hat y_i)^2 \\]\nGradient desscent needs the partial derivative of the RSS with respect to each ceofficient, which is Eqn. 4:\n\\[ {\\partial \\over \\partial a_k}  = -2 \\sum_{i=1}^n \\left(y_i-\\hat y_i\\right) x_i^{k} \\]\nTo define each polynomial, I will gather the coefficients into a vector Eqn. 5:\n\\[ \\mathbf a = [a_1, a_2, ..., a_i] \\]\n\n\n\nThe code in this example is built around functions, so I will define these functions in the first part of the notebook and use these functions in the second part of the notebook.\n\n\nSince I am not using numpy, I am making my own linspace function. Later, this function will make the x-axis to evaluate the polynomials with.\n\ndef linspace(min_val, max_val, npoints):\n    \"\"\"\n    Creates a list of floating point values between the minimum and\n    maximum values, with the number of points (+1) to cover the entire\n    span specified.\n    \n    Parameters\n    ----------\n    min_val\n        The minimum value in the range as a float.\n        \n    max_val\n        The maximum value in the range as a float.\n        \n    npoints\n        The number of points in the range as a float.\n        \n    Returns\n    -------\n    list[float]\n        A list of floating point values as specified.\n    \"\"\"\n    \n    return [min_val+(max_val-min_val)/npoints*i for i in range(npoints+1)]\n\n\n\n\nThis will evaluate a list of y values from a polynomial according to Eqns 1 and 2 with an x-axis and vector of coefficients.\n\ndef polynomial(x_axis, coeffs):\n    \"\"\"\n    Evaluates a polynomial along a given axis given coefficients.\n    \n    Parameters\n    ----------\n    x_axis:\n         A list of floats that is the x-axis to supply as x_i.\n         \n    coeffs:\n        The coefficients of the polynomial as defined in the vector\n        in Eqn. 4.\n        \n    Returns\n    -------\n    list[float]\n        Returns a list of floats that are the values of the polynomial.\n    \"\"\"\n    \n    ys = []\n    for x in x_axis:\n        y = 0.\n        for i, coeff in enumerate(coeffs):\n            y += coeff * x**i\n        ys.append(y)\n    return ys\n\n\n\n\nThis function will calculate the error between the target and estimate polynomials according to Eqn. 3.\n\ndef rss(x_axis, y_coeffs, y_hat_coeffs):\n    \"\"\"\n    Calculates the RSS as defined in Eqn 3 between the two polynomials \n    specified in Eqns 1 or 2.\n    \n    Parameters\n    ----------\n    x_axis\n        The x-axis as a list of floats with which to compare the \n        polynomials.\n        \n    y_coeffs:\n        The coefficients as a list of floats for the target polynomial.\n        \n    y_hat_coeffs\n        The coefficients as a list of floats for the estimate polynomial.\n        \n    Returns\n    -------\n    float\n        An RSS value between the target and estimate.\n    \"\"\"\n    \n    target_ys = polynomial(x_axis, y_coeffs)\n    estimate_ys = polynomial(x_axis, y_hat_coeffs)\n    return sum((target_y - estimate_y)**2 for target_y, estimate_y in zip(target_ys, estimate_ys))\n\n\n\n\nThis function calculates the components of the gradient of the RSS error between the target and estimate coefficients. It implements Eqn 4.\n\ndef gradient(x_axis, target_ys, estimate_ys, y_hat_coeffs):\n    \"\"\"\n    Calculates the gradient of the error between the target and estimate\n    polynomials and returns the components of the gradient in a list of\n    floats.\n    \n    Parameters\n    ----------\n    x_axis\n        The x axis as a list of floats.\n    \n    target_ys\n        List of floats that is the target polynomial y values.\n        \n    estimate_ys\n        List of floats that is the estimate polynomial y values.\n        \n    y_hat_coeffs\n        The estimate coefficients as a list of floats.\n        \n    Returns\n    -------\n    list[float]\n        The components of the gradient as a list of floats.\n    \"\"\"\n    \n    components = []\n    for k, _ in enumerate(y_hat_coeffs):\n        component = 0.\n        for i, (target_y, estimate_y) in enumerate(zip(target_ys, estimate_ys)):\n            component += (target_y - estimate_y) * x_axis[i] ** k\n        components.append(-2 * component)\n    return components\n\n\n\n\nThis function uses the gradient to iteravely refine the estimate coefficients to move the estimate closer to the target. It returns the history of the RSS values along the way.\n\ndef gradient_descent(x_axis, target_ys, target_coeffs, y_hat_coeffs_initial, learn_rate=1e-6, max_iter=1000, rss_threshold=50.):\n    \"\"\"\n    Performs gradient descent optimization to make the estimate coefficients\n    converge to values that give a polynomial function with a shape similar\n    to the target values.\n    \n    Training continues until max iterations are reached or the RSS diminishes\n    below the given threshold.\n    \n    Parameters\n    ----------\n    x_axis\n        List of floats that is the x-axis for the polynomials.\n        \n    target_ys\n        List of floats that is the target polynomial.\n        \n    y_hat_coeffs_initial\n        The initial guess for the estimate coefficients\n        \n    learn_rate\n        The rate at which to descend the gradient during fitting. Higher numbers\n        descend quicker but may not find the true minimum.\n        \n    max_iter\n        Integer of the maximum number of iterations the algorithm will attempt.\n        Used to prevent infinite loops.\n        \n    rss_threshold\n        If RSS diminishes below this threshold, training iterations will stop.\n        \n    Returns\n    -------\n    list[dict]\n        A list of dictionaries containing the training history at each iteration.\n    \"\"\"\n    \n    fit_history = []\n    y_hat_coeffs = y_hat_coeffs_initial[:]\n    for i in range(max_iter):\n        estimate_ys = polynomial(x_axis, y_hat_coeffs)\n        estimate_rss = rss(x_axis, target_coeffs, y_hat_coeffs)\n        fit_history.append({\n            'rss': estimate_rss,\n            'y_hat_coeffs': y_hat_coeffs[:]\n        })\n        if estimate_rss &lt; rss_threshold:\n            break\n        current_gradient = gradient(x_axis, target_ys, estimate_ys, y_hat_coeffs)\n        y_hat_coeffs = [y_hat_coeff-learn_rate*gi for y_hat_coeff, gi in zip(y_hat_coeffs, current_gradient)]\n    return fit_history\n\n\n\n\n\n\n\n\ncommon_x_axis = linspace(-5., 5., 100)\n\n\n\n\nThe target polynomial will be defined by a vector called target_coeffs. The coefficients for the initial estimated coefficients will be estimate_coeffs. target_coeffs will not change since they represent the truth in the estimate polynomial. The estiamte_coeffs will be iteratively updated as the estimate moves closer to the target.\n\ntarget_coeffs = [-2.0, 0.0, 2.5, 1.2]\nestimate_coeffs = [-2.5, 0.0, 2.0, -1.7]\n\n\n\n\n\n\n\nFor the remainder of this notebook, the target polynomial will be in blue and the estimate polynomial will be in orange. I will include the initial RSS in the title.\n\ntarget_0 = polynomial(common_x_axis, target_coeffs)\nestimate_0 = polynomial(common_x_axis, estimate_coeffs)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.scatter(common_x_axis, target_0, color='blue', alpha=0.6, label='target')\nax.scatter(common_x_axis, estimate_0, color='orange', alpha=0.6, label='estimate')\nax.legend()\n\n\n\n\n\n\n\n\nAs a reminder, here are the target coefficients and the initial estimate coefficients:\n\nprint(f'Target coefficients {target_coeffs}')\nprint(f'Initial estimate coefficients {estimate_coeffs}')\nprint(f'Initial RSS {rss(common_x_axis, target_coeffs, estimate_coeffs)}')\n\nTarget coefficients [-2.0, 0.0, 2.5, 1.2]\nInitial estimate coefficients [-2.5, 0.0, 2.0, -1.7]\nInitial RSS 2015004.0007105004\n\n\n\n\n\n\ngradient_descent_history = gradient_descent(common_x_axis, target_0, target_coeffs, estimate_coeffs)\n\n\n\n\nAs gradient descent fits the estimate coefficients, the RSS should drop with each iteration.\n\nrss_history = [step['rss'] for step in gradient_descent_history]\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.set_yscale('log')\nax.plot(list(range(len(rss_history))), rss_history, color='green')\nax.set_title(f'RSS vs Iteration')\nax.set_ylabel('RSS')\nax.set_xlabel('iteration')\n\nText(0.5, 0, 'iteration')\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a graphical representation of how close the fit is after training.\n\nestimate_final = polynomial(common_x_axis, gradient_descent_history[-1]['y_hat_coeffs'])\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.scatter(common_x_axis, target_0, color='blue', alpha=0.6, label='target')\nax.scatter(common_x_axis, estimate_final, color='orange', alpha=0.6, label='estimate')\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nThese are the numeric results of the training.\n\nfinal_estimate_coeffs = gradient_descent_history[-1]['y_hat_coeffs']\ninitial_rss = gradient_descent_history[0]['rss']\nprint(f'Training iterations {len(gradient_descent_history)}')\nprint(f'Target coefficients {target_coeffs}')\nprint(f'Final estimate coefficients {final_estimate_coeffs}')\nprint(f'Initial RSS {initial_rss}')\nprint(f'Final RSS {rss(common_x_axis, target_coeffs, final_estimate_coeffs)}')\n\nTraining iterations 88\nTarget coefficients [-2.0, 0.0, 2.5, 1.2]\nFinal estimate coefficients [-2.4649992311439837, 0.1551299749689758, 2.4784435198598582, 1.1914759552905416]\nInitial RSS 2015004.0007105004\nFinal RSS 48.45559484188128\n\n\n\n\n\n\n\nGöbel, Börge. Computational Physics, Section 3.\nJames, Gareth et al. An Introduction to Statistical Learning with Applications in R, Eqn 3.16."
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#some-equations",
    "href": "posts/simple-gradient-descent/index.html#some-equations",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "The “target” polynomial that will be fitted is defined as Eqn. 1:\n\\[ y_i = \\sum_{i=0}^{n} a_i x^i \\]\nThe “estimate” polynomial that will be fitted is defined the same way but with a hat of the y Eqn. 2:\n\\[ \\hat y_i = \\sum_{i=0}^{n} a_i x^i \\]\nTo fit the polynomial the code will minimize the residual sum of squares (RSS) function Eqn. 3:\n\\[ RSS = \\sum_{i=0}^{n} (y_i - \\hat y_i)^2 \\]\nGradient desscent needs the partial derivative of the RSS with respect to each ceofficient, which is Eqn. 4:\n\\[ {\\partial \\over \\partial a_k}  = -2 \\sum_{i=1}^n \\left(y_i-\\hat y_i\\right) x_i^{k} \\]\nTo define each polynomial, I will gather the coefficients into a vector Eqn. 5:\n\\[ \\mathbf a = [a_1, a_2, ..., a_i] \\]"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#custom-functions",
    "href": "posts/simple-gradient-descent/index.html#custom-functions",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "The code in this example is built around functions, so I will define these functions in the first part of the notebook and use these functions in the second part of the notebook.\n\n\nSince I am not using numpy, I am making my own linspace function. Later, this function will make the x-axis to evaluate the polynomials with.\n\ndef linspace(min_val, max_val, npoints):\n    \"\"\"\n    Creates a list of floating point values between the minimum and\n    maximum values, with the number of points (+1) to cover the entire\n    span specified.\n    \n    Parameters\n    ----------\n    min_val\n        The minimum value in the range as a float.\n        \n    max_val\n        The maximum value in the range as a float.\n        \n    npoints\n        The number of points in the range as a float.\n        \n    Returns\n    -------\n    list[float]\n        A list of floating point values as specified.\n    \"\"\"\n    \n    return [min_val+(max_val-min_val)/npoints*i for i in range(npoints+1)]\n\n\n\n\nThis will evaluate a list of y values from a polynomial according to Eqns 1 and 2 with an x-axis and vector of coefficients.\n\ndef polynomial(x_axis, coeffs):\n    \"\"\"\n    Evaluates a polynomial along a given axis given coefficients.\n    \n    Parameters\n    ----------\n    x_axis:\n         A list of floats that is the x-axis to supply as x_i.\n         \n    coeffs:\n        The coefficients of the polynomial as defined in the vector\n        in Eqn. 4.\n        \n    Returns\n    -------\n    list[float]\n        Returns a list of floats that are the values of the polynomial.\n    \"\"\"\n    \n    ys = []\n    for x in x_axis:\n        y = 0.\n        for i, coeff in enumerate(coeffs):\n            y += coeff * x**i\n        ys.append(y)\n    return ys\n\n\n\n\nThis function will calculate the error between the target and estimate polynomials according to Eqn. 3.\n\ndef rss(x_axis, y_coeffs, y_hat_coeffs):\n    \"\"\"\n    Calculates the RSS as defined in Eqn 3 between the two polynomials \n    specified in Eqns 1 or 2.\n    \n    Parameters\n    ----------\n    x_axis\n        The x-axis as a list of floats with which to compare the \n        polynomials.\n        \n    y_coeffs:\n        The coefficients as a list of floats for the target polynomial.\n        \n    y_hat_coeffs\n        The coefficients as a list of floats for the estimate polynomial.\n        \n    Returns\n    -------\n    float\n        An RSS value between the target and estimate.\n    \"\"\"\n    \n    target_ys = polynomial(x_axis, y_coeffs)\n    estimate_ys = polynomial(x_axis, y_hat_coeffs)\n    return sum((target_y - estimate_y)**2 for target_y, estimate_y in zip(target_ys, estimate_ys))\n\n\n\n\nThis function calculates the components of the gradient of the RSS error between the target and estimate coefficients. It implements Eqn 4.\n\ndef gradient(x_axis, target_ys, estimate_ys, y_hat_coeffs):\n    \"\"\"\n    Calculates the gradient of the error between the target and estimate\n    polynomials and returns the components of the gradient in a list of\n    floats.\n    \n    Parameters\n    ----------\n    x_axis\n        The x axis as a list of floats.\n    \n    target_ys\n        List of floats that is the target polynomial y values.\n        \n    estimate_ys\n        List of floats that is the estimate polynomial y values.\n        \n    y_hat_coeffs\n        The estimate coefficients as a list of floats.\n        \n    Returns\n    -------\n    list[float]\n        The components of the gradient as a list of floats.\n    \"\"\"\n    \n    components = []\n    for k, _ in enumerate(y_hat_coeffs):\n        component = 0.\n        for i, (target_y, estimate_y) in enumerate(zip(target_ys, estimate_ys)):\n            component += (target_y - estimate_y) * x_axis[i] ** k\n        components.append(-2 * component)\n    return components\n\n\n\n\nThis function uses the gradient to iteravely refine the estimate coefficients to move the estimate closer to the target. It returns the history of the RSS values along the way.\n\ndef gradient_descent(x_axis, target_ys, target_coeffs, y_hat_coeffs_initial, learn_rate=1e-6, max_iter=1000, rss_threshold=50.):\n    \"\"\"\n    Performs gradient descent optimization to make the estimate coefficients\n    converge to values that give a polynomial function with a shape similar\n    to the target values.\n    \n    Training continues until max iterations are reached or the RSS diminishes\n    below the given threshold.\n    \n    Parameters\n    ----------\n    x_axis\n        List of floats that is the x-axis for the polynomials.\n        \n    target_ys\n        List of floats that is the target polynomial.\n        \n    y_hat_coeffs_initial\n        The initial guess for the estimate coefficients\n        \n    learn_rate\n        The rate at which to descend the gradient during fitting. Higher numbers\n        descend quicker but may not find the true minimum.\n        \n    max_iter\n        Integer of the maximum number of iterations the algorithm will attempt.\n        Used to prevent infinite loops.\n        \n    rss_threshold\n        If RSS diminishes below this threshold, training iterations will stop.\n        \n    Returns\n    -------\n    list[dict]\n        A list of dictionaries containing the training history at each iteration.\n    \"\"\"\n    \n    fit_history = []\n    y_hat_coeffs = y_hat_coeffs_initial[:]\n    for i in range(max_iter):\n        estimate_ys = polynomial(x_axis, y_hat_coeffs)\n        estimate_rss = rss(x_axis, target_coeffs, y_hat_coeffs)\n        fit_history.append({\n            'rss': estimate_rss,\n            'y_hat_coeffs': y_hat_coeffs[:]\n        })\n        if estimate_rss &lt; rss_threshold:\n            break\n        current_gradient = gradient(x_axis, target_ys, estimate_ys, y_hat_coeffs)\n        y_hat_coeffs = [y_hat_coeff-learn_rate*gi for y_hat_coeff, gi in zip(y_hat_coeffs, current_gradient)]\n    return fit_history"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#define-the-target-and-estimate-polynomials",
    "href": "posts/simple-gradient-descent/index.html#define-the-target-and-estimate-polynomials",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "common_x_axis = linspace(-5., 5., 100)\n\n\n\n\nThe target polynomial will be defined by a vector called target_coeffs. The coefficients for the initial estimated coefficients will be estimate_coeffs. target_coeffs will not change since they represent the truth in the estimate polynomial. The estiamte_coeffs will be iteratively updated as the estimate moves closer to the target.\n\ntarget_coeffs = [-2.0, 0.0, 2.5, 1.2]\nestimate_coeffs = [-2.5, 0.0, 2.0, -1.7]"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#fitting-the-polynomial",
    "href": "posts/simple-gradient-descent/index.html#fitting-the-polynomial",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "For the remainder of this notebook, the target polynomial will be in blue and the estimate polynomial will be in orange. I will include the initial RSS in the title.\n\ntarget_0 = polynomial(common_x_axis, target_coeffs)\nestimate_0 = polynomial(common_x_axis, estimate_coeffs)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.scatter(common_x_axis, target_0, color='blue', alpha=0.6, label='target')\nax.scatter(common_x_axis, estimate_0, color='orange', alpha=0.6, label='estimate')\nax.legend()\n\n\n\n\n\n\n\n\nAs a reminder, here are the target coefficients and the initial estimate coefficients:\n\nprint(f'Target coefficients {target_coeffs}')\nprint(f'Initial estimate coefficients {estimate_coeffs}')\nprint(f'Initial RSS {rss(common_x_axis, target_coeffs, estimate_coeffs)}')\n\nTarget coefficients [-2.0, 0.0, 2.5, 1.2]\nInitial estimate coefficients [-2.5, 0.0, 2.0, -1.7]\nInitial RSS 2015004.0007105004\n\n\n\n\n\n\ngradient_descent_history = gradient_descent(common_x_axis, target_0, target_coeffs, estimate_coeffs)\n\n\n\n\nAs gradient descent fits the estimate coefficients, the RSS should drop with each iteration.\n\nrss_history = [step['rss'] for step in gradient_descent_history]\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.set_yscale('log')\nax.plot(list(range(len(rss_history))), rss_history, color='green')\nax.set_title(f'RSS vs Iteration')\nax.set_ylabel('RSS')\nax.set_xlabel('iteration')\n\nText(0.5, 0, 'iteration')\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a graphical representation of how close the fit is after training.\n\nestimate_final = polynomial(common_x_axis, gradient_descent_history[-1]['y_hat_coeffs'])\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.scatter(common_x_axis, target_0, color='blue', alpha=0.6, label='target')\nax.scatter(common_x_axis, estimate_final, color='orange', alpha=0.6, label='estimate')\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nThese are the numeric results of the training.\n\nfinal_estimate_coeffs = gradient_descent_history[-1]['y_hat_coeffs']\ninitial_rss = gradient_descent_history[0]['rss']\nprint(f'Training iterations {len(gradient_descent_history)}')\nprint(f'Target coefficients {target_coeffs}')\nprint(f'Final estimate coefficients {final_estimate_coeffs}')\nprint(f'Initial RSS {initial_rss}')\nprint(f'Final RSS {rss(common_x_axis, target_coeffs, final_estimate_coeffs)}')\n\nTraining iterations 88\nTarget coefficients [-2.0, 0.0, 2.5, 1.2]\nFinal estimate coefficients [-2.4649992311439837, 0.1551299749689758, 2.4784435198598582, 1.1914759552905416]\nInitial RSS 2015004.0007105004\nFinal RSS 48.45559484188128"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#references",
    "href": "posts/simple-gradient-descent/index.html#references",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "Göbel, Börge. Computational Physics, Section 3.\nJames, Gareth et al. An Introduction to Statistical Learning with Applications in R, Eqn 3.16."
  },
  {
    "objectID": "posts/solubility-clustering/index.html",
    "href": "posts/solubility-clustering/index.html",
    "title": "R: Solubility Clustering",
    "section": "",
    "text": "In my prior aqueous solubility regression study, I did an exploratory data visualization and found intriguing plots of solubility versus other variables in the study. I didn’t perform any experimental modeling of those relationships in that study. Here, I followup by performing a cluster analysis of solubility relationships to help future regression modeling efforts. My question is: do clusters within each of these relationships explain each feature’s effect on solubility?\nTable 1 is a sample of some of the compounds in the dataset:\ndf &lt;- as_tibble(read.csv(\"data/delaney-processed.csv\")) %&gt;%\n  select(\n    compound = Compound.ID, \n    mw = Molecular.Weight, \n    h_bond_donors = Number.of.H.Bond.Donors, \n    rings = Number.of.Rings, \n    rotatable_bonds = Number.of.Rotatable.Bonds, \n    psa = Polar.Surface.Area, \n    solubility = measured.log.solubility.in.mols.per.litre\n)\nknitr::kable(head(df))\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompound\nmw\nh_bond_donors\nrings\nrotatable_bonds\npsa\nsolubility\n\n\n\n\nAmigdalin\n457.432\n7\n3\n7\n202.32\n-0.77\n\n\nFenfuram\n201.225\n1\n2\n2\n42.24\n-3.30\n\n\ncitral\n152.237\n0\n0\n4\n17.07\n-2.06\n\n\nPicene\n278.354\n0\n5\n0\n0.00\n-7.87\n\n\nThiophene\n84.143\n0\n1\n0\n0.00\n-1.33\n\n\nbenzothiazole\n135.191\n0\n2\n0\n12.89\n-1.50"
  },
  {
    "objectID": "posts/solubility-clustering/index.html#dataset-description",
    "href": "posts/solubility-clustering/index.html#dataset-description",
    "title": "R: Solubility Clustering",
    "section": "Dataset description",
    "text": "Dataset description\nSee the previous post for a description of the dataset and variables I use in this study."
  },
  {
    "objectID": "posts/solubility-clustering/index.html#review-of-prior-figures",
    "href": "posts/solubility-clustering/index.html#review-of-prior-figures",
    "title": "R: Solubility Clustering",
    "section": "Review of prior figures",
    "text": "Review of prior figures\nFigure 1 contains histograms and bar plots showing the distributions of variables in the datasets.\n\np1 &lt;- ggplot(df, aes(x = mw)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(a)\")\n\np2 &lt;- ggplot(df, aes(x = psa)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(b)\")\n\np3 &lt;- ggplot(df, aes(x = solubility)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(c)\")\n\np4 &lt;- ggplot(df, aes(x = h_bond_donors)) +\n  geom_bar() +\n  labs(title = \"(d)\")\n\np5 &lt;- ggplot(df, aes(x = rings)) +\n  geom_bar() +\n  labs(title = \"(e)\")\n\np6 &lt;- ggplot(df, aes(x = rotatable_bonds)) +\n  geom_bar() +\n  labs(title = \"(f)\")\n\ngrid.arrange(p1, p2, p3, p4, p5, p6, nrow = 3)\n\n\n\n\nHistograms and bar plots of variables\n\n\n\n\nAs shown above, the subplots 1a, 1b, 1d, 1e, and 1f show distributions that favor the low end of the distribution. This low-end favorability is essential when extracting relationships for values greater in these distributions. For example, data about what happens to solubility with large ring counts are relatively sparse.\nFigure 2 contains the plots of solubility versus other variables I want to explore with clustering. Note that the plots have jittered points to prevent overplotting. Also, note that the solubility is in log(mol/L) because solubility in this dataset spans many orders of magnitude.\n\nalpha &lt;- 0.1\n\np1 &lt;- ggplot(df, aes(x = mw, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(a)\")\n\np2 &lt;- ggplot(df, aes(x = psa, y = solubility)) +\n  geom_jitter(alpha = alpha) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(b)\")\n\np3 &lt;- ggplot(df, aes(x = h_bond_donors, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(c)\")\n\np4 &lt;- ggplot(df, aes(x = rings, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(d)\")\n\np5 &lt;- ggplot(df, aes(x = rotatable_bonds, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(e)\")\n\ngrid.arrange(p1, p2, p3, p4, p5, nrow = 3)\n\n\n\n\nRelationship of molecular weight to other variables\n\n\n\n\nEach variable has a different trend of its effect on solubility, as shown above. Figures 2b (of polar surface area) and 2c (of h-bond donors) show increasing trends of solubility. Figures 2a (molecular weight), 2d (rings), and 2e (rotatable bonds) show decreasing trends in solubility."
  },
  {
    "objectID": "posts/solubility-clustering/index.html#solubility-trends-for-each-variable-by-cluster",
    "href": "posts/solubility-clustering/index.html#solubility-trends-for-each-variable-by-cluster",
    "title": "R: Solubility Clustering",
    "section": "Solubility trends for each variable by cluster",
    "text": "Solubility trends for each variable by cluster\nI will display the clustering information as scatter plots, with each point’s color indicating the group from the hierarchical clustering algorithm. For each variable, I make tests with two, three, and four clusters. Cluster_2, cluster_3, and cluster_4 are the variable names for each of these cluster counts, respectively. On each plot, solubility is on the y axis. I am particularly interested in clusters that separate the compounds by their solubilities, i.e., by dividing groups with a line parallel to the x-axis while minimizing the number of groups needed to make this division.\n\nhierarchical_clusters &lt;- function(features_df) {\n  cluster_labels = list()\n  \n  distances &lt;- features_df %&gt;%\n    as.matrix() %&gt;%\n    scale() %&gt;%\n    dist()\n  \n  hclust_out &lt;- hclust(distances, method = \"complete\")\n  \n  features_df %&gt;%\n    mutate(\n      cluster_2 = as.factor(cutree(hclust_out, k = 2)),\n      cluster_3 = as.factor(cutree(hclust_out, k = 3)),\n      cluster_4 = as.factor(cutree(hclust_out, k = 4))\n    )\n}\n\n\nSolubility versus polar surface area (PSA)\n\nalpha &lt;- 0.3\nsize &lt;- 3\n\npsa_cluster_df &lt;- hierarchical_clusters(select(df, solubility, psa))\n\ncluster_2 &lt;- ggplot(psa_cluster_df, aes(x = psa, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(a)\")\n\ncluster_3 &lt;- ggplot(psa_cluster_df, aes(x = psa, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(b)\")\n\ncluster_4 &lt;- ggplot(psa_cluster_df, aes(x = psa, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(c)\")\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus polar surface area (PSA)\n\n\n\n\nFigure 3a shows that two clusters are not enough to separate the solubility axis into high and low solubilities. Figure 3c reveals that four clusters show the necessary separation, but the fourth only covers five compounds with large polar surface areas and high solubilities. Figure 3b is just right: it displays a clear break of high and low solubilities with only three clusters."
  },
  {
    "objectID": "posts/solubility-clustering/index.html#solubility-versus-h-bond-donors-by-cluster",
    "href": "posts/solubility-clustering/index.html#solubility-versus-h-bond-donors-by-cluster",
    "title": "R: Solubility Clustering",
    "section": "Solubility versus h-bond donors by cluster",
    "text": "Solubility versus h-bond donors by cluster\n\nh_bond_donor_cluster_df &lt;- hierarchical_clusters(select(df, solubility, h_bond_donors))\n\ncluster_2 &lt;- ggplot(h_bond_donor_cluster_df, aes(x = h_bond_donors, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha) +\n  labs(title = \"(a)\") +\n  theme_minimal()\n\ncluster_3 &lt;- ggplot(h_bond_donor_cluster_df, aes(x = h_bond_donors, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha) +\n  labs(title = \"(b)\") +\n  theme_minimal()\n\ncluster_4 &lt;- ggplot(h_bond_donor_cluster_df, aes(x = h_bond_donors, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha) +\n  labs(title = \"(c)\") +\n  theme_minimal()\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus h-bond donors by cluster\n\n\n\n\nSimilar to Figure 3, Figure 4a with two clusters provides no clear break between high and low solubility. Figure 4c shows that the fourth cluster covers a few points a corner of the plot. Once again, Figure 4b shows that three clusters provide a clear break between high and low solubility, with a few points covered by a third cluster. I’ll choose 3 clusters as optimal for h-bond donors.\n\nSolubility versus rings by cluster\n\nrings_cluster_df &lt;- hierarchical_clusters(select(df, solubility, rings))\n\ncluster_2 &lt;- ggplot(rings_cluster_df, aes(x = rings, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(a)\")\n\ncluster_3 &lt;- ggplot(rings_cluster_df, aes(x = rings, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(b)\")\n\ncluster_4 &lt;- ggplot(rings_cluster_df, aes(x = rings, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(c)\")\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus rings\n\n\n\n\nFigure 5a shows two clusters, each of which straddles high and low solubility. Figure 5b, with three clusters, exhibits the same problem, though the third cluster is generally of low solubility. Figure 5c represents the separation best: cluster 1 is mostly high solubility, cluster 4 is primarily low solubility, and cluster 3 is low solubility. Cluster 2 aggregates between -3 and -5, so it to provides a useful solubility range. I’ll choose four as the optimum number of clusters."
  },
  {
    "objectID": "posts/solubility-clustering/index.html#solubility-versus-rotatable_bonds-by-cluster",
    "href": "posts/solubility-clustering/index.html#solubility-versus-rotatable_bonds-by-cluster",
    "title": "R: Solubility Clustering",
    "section": "Solubility versus rotatable_bonds by cluster",
    "text": "Solubility versus rotatable_bonds by cluster\n\nrotatable_bonds_cluster_df &lt;- hierarchical_clusters(select(df, solubility, rotatable_bonds))\n\ncluster_2 &lt;- ggplot(rotatable_bonds_cluster_df, aes(x = rotatable_bonds, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(a)\")\n\ncluster_3 &lt;- ggplot(rotatable_bonds_cluster_df, aes(x = rotatable_bonds, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(b)\")\n\ncluster_4 &lt;- ggplot(rotatable_bonds_cluster_df, aes(x = rotatable_bonds, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(c)\")\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus rotatable_bonds by cluster\n\n\n\n\nFigure 6a shows two clusters of rotatable bond counts that don’t provide a break between high and low solubility. Figure 6c adds the fourth cluster with a wide span of solubilities and, therefore, not much value. Once again, Figure 6b shows three clusters, with reasonably well-defined breaks around solubilities of -3 and -4. I’ll choose three as the optimum number of clusters for rotatable bonds.\n\nSolubility versus molecular weight by cluster\n\nmw_cluster_df &lt;- hierarchical_clusters(select(df, solubility, mw))\n\ncluster_2 &lt;- ggplot(mw_cluster_df, aes(x = mw, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(a)\") +\n  theme_minimal()\n\ncluster_3 &lt;- ggplot(mw_cluster_df, aes(x = mw, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(b)\") +\n  theme_minimal()\n\ncluster_4 &lt;- ggplot(mw_cluster_df, aes(x = mw, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(c)\") +\n  theme_minimal()\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus molecular weight by cluster\n\n\n\n\n\n\nSummary of optimum number of clusters\n\n\n\nVariable name\nOptimal cluster count\n\n\n\n\npolar surface area (PSA)\n3\n\n\nh-bond donors\n3\n\n\nrings\n4\n\n\nrotatable bonds\n3\n\n\nmolecular weight\n4"
  },
  {
    "objectID": "posts/solubility-clustering/index.html#conclusion",
    "href": "posts/solubility-clustering/index.html#conclusion",
    "title": "R: Solubility Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nWhen I use hierarchiical clustering to group solubilities, each variable in the dataset needs a different number of clusters to adequately specify its relastionship to solubility."
  },
  {
    "objectID": "posts/solubility-regression/index.html",
    "href": "posts/solubility-regression/index.html",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "",
    "text": "Aqueous solubility (ability to dissolve in water) is an essential property of a chemical compound important in the laboratory. Can the solubility of a compound be predicted based on a chemical structure alone? John Delaney posed this predictions question in 2004 (Delaney 2004) and wrote a paper with numerous citations in the chemistry literature. This study will take a dataset similar to that study and use linear and random forest regression to predict the compounds’ solubilities.\nThe random forest model is a much better predictor of solubilities.\nThis code for this study is implemented in R and is available in its entirety on GitHub.\nA number of compounds in this dataset are well-known, even outside the chemistry community. Here is a sample of what lies inside the dataset:"
  },
  {
    "objectID": "posts/solubility-regression/index.html#dataset-description",
    "href": "posts/solubility-regression/index.html#dataset-description",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe original report published a dataset of compounds represented as SMILES strings. SMILES strings are a compact and text-based method of specifying chemical structures. This study will use a preprocessed dataset mentioned on moleculenet.ai and distributed by deepchem.io, which contains features parsed from these SMILES strings. You can browse the file on GitHub.This study uses a subset of these preprocessed features, which are listed in Table 2.\n\nTable 2: Features of each compound used in the regression\n\n\n\n\n\n\n\n\nFeature name\nUnits\nDescription\n\n\n\n\nmw\ng/mol\nThe molecular weight of the compound.\n\n\nsolubility\nlog(mol/L)\nThe log solubility, in mol/L. Solubility is the response variable of this study.\n\n\npsa\nÅ2\nThe polar surface area of a molecule.\n\n\nh_bond_donors\nunitless\nThe number of hydrogen bond donors on a molecule.1\n\n\nrotatable_bonds\nunitless\nThe number of rotatable bonds within a molecule.2"
  },
  {
    "objectID": "posts/solubility-regression/index.html#exploratory-visualization",
    "href": "posts/solubility-regression/index.html#exploratory-visualization",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Exploratory visualization",
    "text": "Exploratory visualization\n\ndf &lt;- as_tibble(read.csv(\"data/delaney-processed.csv\")) %&gt;%\n  select(\n    compound = Compound.ID, \n    mw = Molecular.Weight, \n    h_bond_donors = Number.of.H.Bond.Donors, \n    rings = Number.of.Rings, \n    rotatable_bonds = Number.of.Rotatable.Bonds, \n    psa = Polar.Surface.Area, \n    solubility = measured.log.solubility.in.mols.per.litre\n)\n\nBefore I dive into the machine learning model, let’s examine exploratory plots to get a feel for the data distribution. Figure 1 has histograms (for continuous variables) and bar plots (for discrete variables) to demonstrate the dataset’s values’ distributions. Figure 1a, 1b, 1d, 1e, and 1f show distributions of values favoring their respective range’s low ends. Solubility, our response variable, has a broader spread above and below its mean of -3.05.\n\np1 &lt;- ggplot(df, aes(x = mw)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(a)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df, aes(x = psa)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(b)\") +\n  theme_minimal()\n\np3 &lt;- ggplot(df, aes(x = solubility)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(c)\") +\n  theme_minimal()\n\np4 &lt;- ggplot(df, aes(x = h_bond_donors)) +\n  geom_bar() +\n  labs(title = \"(d)\") +\n  theme_minimal()\n\np5 &lt;- ggplot(df, aes(x = rings)) +\n  geom_bar() +\n  labs(title = \"(e)\") +\n  theme_minimal()\n\np6 &lt;- ggplot(df, aes(x = rotatable_bonds)) +\n  geom_bar() +\n  labs(title = \"(f)\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, p3, p4, p5, p6, nrow = 3)\n\n\n\n\nHistograms and bar plots of variables\n\n\n\n\nFigure 1a, 1b, 1d, 1e, and 1f show distributions of values favoring their respective range’s low ends. Solubility, in Figure 1c, has a broader spread above and below its mean of -3.05.\nA number of the features of the molecules require lots of atoms. For example, a five-ring molecule will likely have a higher molecular weight than a three-ring molecule. Molecular weight has a special relationship with all other variables that denote each molecule’s increasing structural complexity. Figure 2 plots molecular weight against all other variables, with a trend line for each relationship, as shown in the jittered scatter plots below.\n\nalpha = 0.1\np1 &lt;- ggplot(df, aes(x = psa, y = mw)) +\n  geom_jitter(alpha = alpha) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(a)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df, aes(x = solubility, y = mw)) +\n  geom_jitter(alpha = alpha) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(b)\") +\n  theme_minimal()\n\np3 &lt;- ggplot(df, aes(x = h_bond_donors, y = mw)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(c)\") +\n  theme_minimal()\n\np4 &lt;- ggplot(df, aes(x = rings, y = mw)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(d)\") +\n  theme_minimal()\n\np5 &lt;- ggplot(df, aes(x = rotatable_bonds, y = mw)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(e)\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, p3, p4, p5, nrow = 3)\n\n\n\n\nRelationship of molecular weight to other variables\n\n\n\n\nFigures 2a, 2c, 2d, 2e all exhibit increasing molecular weight with increased structural complexity. Figure 2b stands out: in general, as molecular weight increases, solubility decreases."
  },
  {
    "objectID": "posts/solubility-regression/index.html#traintest-split",
    "href": "posts/solubility-regression/index.html#traintest-split",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Train/test split",
    "text": "Train/test split\nAll models use the same randomized train/test split. First, I shuffled the rows of the original dataset. Then, I selected the first 846 rows for the training set and the last 282 rows for the test dataset. I seeded the random number generator with a constant to ensure the same shuffles between model runs.\n\nsolubilityTrainTestSplit &lt;- function(all_data, split_fraction = 0.75, random_seed = 13) {\n  # Shuffle based on random seed\n  set.seed(random_seed)\n  sample_indecies &lt;- sample(nrow(all_data), nrow(all_data))\n  shuffled &lt;- all_data[sample_indecies, ]\n  \n  # Train test split\n  train_row &lt;- round(nrow(shuffled) * split_fraction)\n  test_row &lt;- train_row + 1\n  train &lt;- shuffled[1:train_row, ]\n  test &lt;- shuffled[test_row:nrow(shuffled), ]\n  \n  # Now create the final list that is returned\n  list(\n    train = train,\n    test = test\n  )\n}\n\nsplit &lt;- solubilityTrainTestSplit(df)\ntest &lt;- split$test\ntrain &lt;- split$train\n\nTable 3 is the first few rows of the train dataset:\n\nTable 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompound\nmw\nh_bond_donors\nrings\nrotatable_bonds\npsa\nsolubility\n\n\n\n\n2-Ethyl-2-hexanal\n126.199\n0\n0\n4\n17.07\n-2.460\n\n\n3-Butanoyloxymethylphenytoin\n352.390\n1\n3\n6\n75.71\n-5.071\n\n\nTrichloromethane\n119.378\n0\n0\n0\n0.00\n-1.170\n\n\nIndole\n117.151\n1\n2\n0\n15.79\n-1.520\n\n\n1,2,3,4-Tetrahydronapthalene\n132.206\n0\n2\n0\n0.00\n-4.370\n\n\n2,2’,3,4,5-PCB\n326.437\n0\n2\n1\n0.00\n-7.210\n\n\n\n\n\nTable 4 is the first few rows of the test dataset:\n\n\nTable 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompound\nmw\nh_bond_donors\nrings\nrotatable_bonds\npsa\nsolubility\n\n\n\n\nXipamide\n354.815\n3\n2\n3\n109.49\n-3.790\n\n\nDiallate\n270.225\n0\n0\n4\n20.31\n-4.286\n\n\nm-Fluorobromobenzene\n175.000\n0\n1\n0\n0.00\n-2.670\n\n\n3,5-Dimethylphenol\n122.167\n1\n1\n0\n20.23\n-1.400\n\n\nparabanic acid\n114.060\n2\n1\n0\n75.27\n-0.400\n\n\np-Nitroaniline\n138.126\n1\n1\n1\n69.16\n-2.370"
  },
  {
    "objectID": "posts/solubility-regression/index.html#formula",
    "href": "posts/solubility-regression/index.html#formula",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Formula",
    "text": "Formula\nAll models use the same formula based on all the predictors.\n\nformula &lt;- solubility ~ mw + h_bond_donors + rings + rotatable_bonds + psa"
  },
  {
    "objectID": "posts/solubility-regression/index.html#linear-model",
    "href": "posts/solubility-regression/index.html#linear-model",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Linear Model",
    "text": "Linear Model\nOur first stop is the linear model. Table 3 shows the performance of the linear model after training when I use it to predict the solubilties in the test dataset.\n\nTable 3: linear model performance on test data\n\nanalyzeSolubilityLinearModel &lt;- function(model, test_data) {\n  # Run the prediction\n  predicted_solbilities &lt;- predict(model, test_data)\n  \n  # Assemble the compounds and predictions back onto the\n  # train features\n  \n  test_results &lt;- test_data %&gt;%\n    mutate(prediction = predicted_solbilities) %&gt;%\n    mutate(residual = solubility - prediction)\n  \n  # Calculate the standard deviation and RMSE\n  sd_solubilities &lt;- sd(test_results$solubility)\n  rmse &lt;- sqrt(mean(test_results$residual ^ 2))\n  \n  # calculate r_squared\n  rss &lt;- sum(test_results$residual ^ 2)\n  total_error &lt;- test_results$solubility - mean(test_results$solubility)\n  tss &lt;- sum(total_error ^ 2)\n  r_squared &lt;- 1 - (rss / tss)\n  \n  # Create a list to return to the caller\n  list(\n    sd_solubilities = sd_solubilities,\n    rmse = rmse,\n    test_results = test_results,\n    r_squared = r_squared\n  )\n}\n\nlm_model &lt;- lm(solubility ~ mw + h_bond_donors + rings + rotatable_bonds + psa, train)\nlm_result &lt;- analyzeSolubilityLinearModel(lm_model, test)\nlm_result_df &lt;- tibble(\n  metric = c(\"RMSE\", \"R^2\"),\n  value = c(lm_result$rmse, lm_result$r_squared)\n)\nknitr::kable(lm_result_df)\n\n\n\n\nmetric\nvalue\n\n\n\n\nRMSE\n1.1955559\n\n\nR^2\n0.6721196\n\n\n\n\n\nThe RMSE of 1.196 is less than the test solubility’s standard deviation of 2.092, but the R^2 of 0.672 shows the linear model does not explain much of the variance of underlying solubility."
  },
  {
    "objectID": "posts/solubility-regression/index.html#random-forest-regression",
    "href": "posts/solubility-regression/index.html#random-forest-regression",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Random Forest Regression",
    "text": "Random Forest Regression\nThe second model is the random forest model. For the random forest, I use the ranger package. The model uses 500 trees and is trained and tested with the same train and test data as the original linear model. Table 4 shows the random forest’s prediction performance on the test data after training.\n\nTable 4: Random forest test data performance\n\nanalyzeSolubilityRandomForestModel &lt;- function(model, test_data) {\n  # Run the prediction\n  predicted_solbilities &lt;- predict(model, test_data)$predictions\n  \n  # Assemble the compounds and predictions back onto the\n  # train features\n  \n  test_results &lt;- test_data %&gt;%\n    mutate(prediction = predicted_solbilities) %&gt;%\n    mutate(residual = solubility - prediction)\n  \n  # Calculate the standard deviation and RMSE\n  sd_solubilities &lt;- sd(test_results$solubility)\n  rmse &lt;- sqrt(mean(test_results$residual ^ 2))\n  \n  # calculate r_squared\n  rss &lt;- sum(test_results$residual ^ 2)\n  total_error &lt;- test_results$solubility - mean(test_results$solubility)\n  tss &lt;- sum(total_error ^ 2)\n  r_squared &lt;- 1 - (rss / tss)\n  \n  # Create a list to return to the caller\n  list(\n    sd_solubilities = sd_solubilities,\n    rmse = rmse,\n    test_results = test_results,\n    r_squared = r_squared\n  )\n}\n\nrf_model &lt;- ranger(\n  formula,\n  train,\n  num.trees = 500,\n  respect.unordered.factors = \"order\"\n)\n\nrf_result &lt;- analyzeSolubilityRandomForestModel(rf_model, test)\nrf_result_df = tibble(\n  metric = c(\"RMSE\", \"R^2\"),\n  value = c(rf_result$rmse, rf_result$r_squared)\n)\n\nknitr::kable(rf_result_df)\n\n\n\n\nmetric\nvalue\n\n\n\n\nRMSE\n0.8634554\n\n\nR^2\n0.8289766"
  },
  {
    "objectID": "posts/solubility-regression/index.html#comparing-linear-and-random-forest-models",
    "href": "posts/solubility-regression/index.html#comparing-linear-and-random-forest-models",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Comparing linear and random forest models",
    "text": "Comparing linear and random forest models\nTable 5 shows the performance metrics of the linear and random forest models.\n\nTable 5\n\nmodel_comparison_df &lt;- tibble(\n  model = c(\"Linear\", \"Random forest\"),\n  rmse = c(lm_result$rmse, rf_result$rmse),\n  r_squared = c(lm_result$r_squared, rf_result$r_squared)\n)\n\nknitr::kable(model_comparison_df)\n\n\n\n\nmodel\nrmse\nr_squared\n\n\n\n\nLinear\n1.1955559\n0.6721196\n\n\nRandom forest\n0.8634554\n0.8289766\n\n\n\n\n\nExamining Table 5 shows reveals two comparisons between the models that are immediately apparent. The first comparison is their respective RMSE values when evaluated on the training dataset. Lower RMSE values are better. The standard deviation of log solubility in the test dataset is 2.092. This value compares nicely with the random forest’s RMSE of 0.867, but it does not compare well with the linear model’s RMSE of 1.196. The clear winner here, unsurprisingly, the random forest. The second comparison is their respective R^2 values. R^2 values closer to 1.0 are better. The R^2 value of the linear model is a lowly 0.672, while the R^2 value for the random forest is much better at 0.828.\nFigure 3, which plots actual versus predicted solubilities, is my favorite comparison between the linear and random forest models. Recall that all solubilities are log solubilities, hence the negative values on the axes.\n\nalpha &lt;- 0.5\n\nlinear_result_plot &lt;- ggplot(lm_result$test_results, aes(x = prediction, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(a) Linear\") +\n  theme_minimal()\n\nrf_result_plot &lt;- ggplot(rf_result$test_results, aes(x = prediction, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(b) Random forest\") +\n  theme_minimal()\n\ngrid.arrange(linear_result_plot, rf_result_plot, nrow = 1)\n\n\n\n\nactual vs. predicted values for both models\n\n\n\n\nIn Figure 3a, the linear model fit is lacking in the area of high predicted solubility. In this area, the actual solubilities take a wide range, from relatively insoluble to very soluble. At the edge of low predicted solubility, we find a range of actual solubilities.\nIn Figure 3b, the random forest model trends slightly more reliable. The random forest predicts high solubilities where there are actual high solubilities. Near the low end of the actual solubilities, there are numerous under and over predictions from the random forest."
  },
  {
    "objectID": "posts/solubility-regression/index.html#conclusion",
    "href": "posts/solubility-regression/index.html#conclusion",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe random forest performs better than the linear model but still has deficiencies around molecular features causing over or under predicted solubilities."
  },
  {
    "objectID": "posts/fftpipe/index.html",
    "href": "posts/fftpipe/index.html",
    "title": "R: FFT with fftpipe",
    "section": "",
    "text": "fftpipe is a package of functions that wrap around the base R fft() function. The fftpipe package enables workflows around the fft() function that use the pipe (%&gt;%) operator. I took inspiration for the interface to fftpipe from the Tidyverse and tidymodels packages.\nSpecifically, fftpipe offers the following functionality:\n\nWaveform generation,\nFFT and inverse FFT transformation,\nPlotting of these waveforms and FFTs."
  },
  {
    "objectID": "posts/fftpipe/index.html#what-is-fftpipe",
    "href": "posts/fftpipe/index.html#what-is-fftpipe",
    "title": "R: FFT with fftpipe",
    "section": "",
    "text": "fftpipe is a package of functions that wrap around the base R fft() function. The fftpipe package enables workflows around the fft() function that use the pipe (%&gt;%) operator. I took inspiration for the interface to fftpipe from the Tidyverse and tidymodels packages.\nSpecifically, fftpipe offers the following functionality:\n\nWaveform generation,\nFFT and inverse FFT transformation,\nPlotting of these waveforms and FFTs."
  },
  {
    "objectID": "posts/fftpipe/index.html#installation",
    "href": "posts/fftpipe/index.html#installation",
    "title": "R: FFT with fftpipe",
    "section": "Installation",
    "text": "Installation\nInstall fftpipe from GitHub with devtools. If you don’t have devtools installed already, install it with install.packages(\"devtools\"). Once devtools is installed, you can then install fftpipe by typing the following command into the R console:\ndevtools::install_github(\"akey7/fftpipe\")"
  },
  {
    "objectID": "posts/fftpipe/index.html#quick-start",
    "href": "posts/fftpipe/index.html#quick-start",
    "title": "R: FFT with fftpipe",
    "section": "Quick Start",
    "text": "Quick Start\nMore detail about each of these steps is described below in this document. But this will get you started!\n\nLoad the necessary packages\nWhile you can use fftpipe on its own, it is designed to work within the Tidyverse ecosystem, so I recommend you load the tidyverse as well. Also, I will set a theme for ggplot().\n\nsuppressPackageStartupMessages(library(fftpipe))\nsuppressPackageStartupMessages(library(tidyverse))\ntheme_set(theme_linedraw(base_size = 15))\n\n\n\nGenerate a waveform\nWaveforms can either be built from external data or synthesized from within the fftpipe package. For this quick start demo, let’s compose a waveform to feed into the FFT with the following properties:\n\nIts duration will be 1 second with a sample rate of 200 samples/second.\nIt will be the sum of 2 cosines, with the second cosine being 3x the frequency and 50% the amplitude of the first cosine.\nIts total amplitude will decrease with an exponential decay function, with an exponential time constant (tau) of 0.5.\nThe final result will be normalized by dividing the vector of values by the length of that vector to prepare the waveform for FFT.\n\nWe can compose this waveform using functions from fftpipe as shown in this code block:\n\nwv &lt;- waveform(duration_s = 1.0, sr = 100) %&gt;%\n  cos_sum(freqs = c(2.0, 6.0), amplitudes = c(1.0, 0.9)) %&gt;%\n  length_norm()\n\n\n\nPlot the input waveform\nWe can streamline plotting the input waveform with the following function call. This plot can be refined in ways I’ll describe later in the document.\n\nwaveform_plot(wv)\n\n\n\n\n\n\n\n\n\n\nPerform the FFT\nWe can perform an FFT on the waveform we just made. To do that, run the following line:\n\nwv_fft &lt;- wv %&gt;%\n  compute_fft()\n\nWe can then use this FFT data.frame to plot the FFT and reconstruct the original waveform in the next two steps.\n\n\nPlot the FFT\nfft_plot() plots the FFT as shown below. By default, the plot only shows the frequency components that are at or below the Nyquist frequency (half the sample rate). Here, our sample rate is 100 Hz, so the maximum frequency is 50 Hz. This plot can be further customized, which will be shown later in the document.\nNote how our original component cosines we summed together appear here as peaks at 2 Hz and 6 Hz. The first peak is taller than the second peak, which corresponds to our 6 Hz component having less amplitude than the 2 Hz component. Hence, our FFT captured the frequencies of our original waveform!\n\nwv_fft %&gt;%\n  fft_plot()\n\n\n\n\n\n\n\n\n\n\nReconstruct the original waveform from the FFT\nFinally, we can reconstruct the original waveform from the FFT. First, we need to make a waveform that defines the sample rate and values for the waveform to be reconstructed. This should match the waveform() call above. Then we need to feed both the new waveform and FFT into the inverse_FFT(). Then, we can pass the reconstruction through a length_norm() call and plot the reconstructed waveform.\n\nwaveform(duration_s = 1.0, sr = 100) %&gt;%\n  inverse_fft(wv_fft) %&gt;%\n  length_norm() %&gt;%\n  waveform_plot()\n\n\n\n\n\n\n\n\nBy using length_norm() before and after the FFT, the amplitudes of our waveform are the same before and after the FFT."
  },
  {
    "objectID": "posts/fftpipe/index.html#in-depth",
    "href": "posts/fftpipe/index.html#in-depth",
    "title": "R: FFT with fftpipe",
    "section": "In Depth",
    "text": "In Depth\nIn this section, I will go over the capabilities of the fftpipe package in more detail. This section will cover:\n\nfftpipe is built around dataframes.\nHow do you customize plots made with waveform_plot() and fft_plot()?\nHow do you compose a waveform with waveform(), cos_sum(), exp_decay(), white_noise(), and length_norm()?\nHow do you do FFT operations with compute_fft() and inverse_fft(). What options are available for fft_plot()?\nHow do you do signal denoising with denoise_fft()?\n\nYou can read the source code for all the functions on GitHub\n\nfftpipe Is Built Around Dataframes\nfftpipe functions pass data frames to each other with the pipe (%&gt;%) operator. The columns differ depending on if a dataframe represents an FFT or a waveform. A waveform is what you pass into an FFT or get out of an inverse FFT.\nWaveform dataframes contain the following columns:\n\n\n\nColumn\nPurpose\n\n\n\n\n.sample\nAn index of the sample in waveform the row represents.\n\n\n.sec\nt: Time of that row in the waveform.\n\n\n.value\nf(t): Value of the waveform at that time.\n\n\n\nFFT dataframes contain the following columns:\n\n\n\n\n\n\n\nColumn\nPurpose\n\n\n\n\n.idx\nIndex of the coefficient. Used to compute the frequency of each coefficient.\n\n\n.value\nThe complex value of the Fourier coefficient.\n\n\n.psd\nThe power spectral density of that Fourier coefficient.\n\n\n\nBy passing these values around in dataframes, the functions of fftpipe can share data amongst themselves without needing the same information specified repeatedly in code.\n\n\nHow Do You Customize Plots?\nThe plots above showed some information but they were not great. The axes didn’t have descriptive labels and the plots lacked titles. Also, the theme was a bit drab. Let’s fix that!\nThe plots created by waveform_plot() and fft_plot() are simply ggplot objects that can be displayed or saved as you wish. Both waveform_plot() and fft_plot() accept ... arguments which are passed directly to the labs() function. Let’s generate a new waveform and then customize its plot\n(You will see some new functions when we create the waveform; these will be explained later.)\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  exp_decay(decay_tau = 0.5) %&gt;%\n  length_norm()\n\nwv_to_customize %&gt;%\n  waveform_plot()\n\n\n\n\n\n\n\n\nSince waveform_plot() and fft_plot() accept arguments to pass to label(), we can add labels to this plot. In your plots, you don’t need to specify all the labels, but I’ve put them all in here to demonstrate what you can do.\n\nwv_to_customize %&gt;%\n  waveform_plot(\n    title = \"My Favorite Waveform!\",\n    subtitle = \"Let's Customize This Plot.\",\n    caption = \"Generated completely with fftpipe\",\n    x = \"t (s)\",\n    y = \"f(t)\"\n  )\n\n\n\n\n\n\n\n\nSince this is just a ggplot object, we can add themeing:\n\nwv_to_customize %&gt;%\n  waveform_plot(\n    title = \"My Favorite Waveform!\",\n    subtitle = \"Let's Customize This Plot.\",\n    caption = \"Generated completely with fftpipe\",\n    x = \"t (s)\",\n    y = \"f(t)\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.caption = element_text(colour = \"blue\", face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nFor theming, the sky is the limit here. Go for it!\n\n\nHow Do You Compose a Waveform for Testing?\nHow do the functions waveform(), cos_sum(), exp_decay(), white_noise(), and length_norm() work together to make waveforms for testing? Let’s illustrate this with an example where we build operations up to make the example waveform shown in the plots above.\nwaveform() creates the primary attributes of a waveform: a duration in seconds and a sample rate. It also fills the values of the waveform with zero to enable further addition operations. Here, I define a waveform that is one second long with a sample rate of 200 Hz.\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200)\n\nwv_to_customize %&gt;%\n  waveform_plot()\n\n\n\n\n\n\n\n\nNow the waveform needs something. That is what cos_sum() is for. cos_sum() takes vectors of frequencies, amplitudes, and phases of cos() functions to sum into a final waveform. It sums them up and adds them to the incoming waveform which is specified by the first argument to the function:\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  )\n\nwv_to_customize %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nNow we have a waveform we can do something with. But there is still more that we can do!\nWe can add noise to the signal with white_noise(). Speicifcally, this function adds random numbers drawn from a normal distribution into the incoming waveform. It takes two arguments, both of which are optional: mean and sd. Adjusting sd is like adjusting the amplitude of the noise (lower values are less noise).\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  white_noise(mean = 0, sd = 1e-5)\n\nwv_to_customize %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nWe can multiply by an exponential decay to create a waveform whose amplitude diminishes over time with exp_decay(). In addition to an incoming dataframe which specifies a waveform, exp_decay() accepts one other parameter decay_tau which specifies the rate of decay (lower values mean faster decay rate).\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  white_noise(mean = 0, sd = 1e-5) %&gt;%\n  exp_decay(decay_tau = 0.5)\n\nwv_to_customize %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nFinally, it’s good to normalize the waveform after creating it before applying a Fourier transform. A common technique I have seen is to divide each element of the signal’s vector by the length of the vector. This functionality is accomplished by the length_norm() function. Note how the amplitude drops in the following plot compared to the plots above.\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  white_noise(mean = 0, sd = 1e-5) %&gt;%\n  exp_decay(decay_tau = 0.5) %&gt;%\n  length_norm()\n\nwv_to_customize %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nAll the steps except the waveform() creation are optional. Mix and match to your heart’s content!\n\n\nHow Do You Do FFT Operations?\ncompute_fft() and inverse_fft() perform fast fouriere transform (FFT) operations and the results can be plotted with fft_plot(). Fourier transforms, Fourier series, and Fourier analysis are HUGE topics that span many areas of math and physics. I won’t go into those applications here. But I will show you how to use these functions as fundamental building blocks as a starting point for more advanced FFT applications.\nLet’s revisit our most recent waveform we generated:\n\nwv_for_fft &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  white_noise(mean = 0, sd = 1e-5) %&gt;%\n  exp_decay(decay_tau = 0.5) %&gt;%\n  length_norm()\n\nwv_for_fft %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nWe can take and plot the FFT of this waveform simply with compute_fft() (remember to length_norm() before you run an FFT). Here we will take our noisy signal, do an FFT, and make a PSD plpot of the first 100 Fourier coefficients:\n\nour_first_fft &lt;- wv_for_fft %&gt;%\n  compute_fft()\n\nour_first_fft %&gt;%\n  fft_plot(show = \"psd\", x = \"Hz\", y = \"PSD\")\n\n\n\n\n\n\n\n\n(Read below to find out how to remove noise with FFT denoising!)\nSince there is more than just spectral density, we can also plot everything out of the FFT. What we will see is the real, imaginary, modulus of every Fourier coefficient as well as every PSD value.\n\nour_first_fft %&gt;%\n  fft_plot(show = \"everything\")\n\n\n\n\n\n\n\n\nNow let’s go backwards with the inverse FFT to regenerate a waveform from FFT data. We do this with the inverse_fft() function. The inverse_fft() function is somewhat tricky: It takes a data.frame of an incoming FFT as the second argument, and a data.frame of an incoming waveform as the first argument. The waveform is necessary so that the inverse operation has somewhere to put the data; it must be a waveform compatible with the original waveform. Here waveform(duration_s = 1.0, sr = 200) is the same call we started the original waveform with. And, as usual, we will call length_norm() as the last step in producing a waveform.\n\nwv_recreated &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  inverse_fft(our_first_fft) %&gt;%\n  length_norm()\n\nwv_recreated %&gt;%\n  waveform_plot(title = \"Recreated Waveform\", x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\n\n\nSignal Denoising\nSignal denoising is a simple way to remove noise from a signal through the following steps:1\n\nFFT the noisy signal\nInspect the power spectral density (PSD) for each Fourier coefficient\nObserve that the coefficients encoding the signal have a higher PSD than the noise coefficients.\nZero the coefficients that do not meet a PSD threshold.\nRun and inverse FFT to reconstruct a waveform with reduced noise.\n\nAll of these steps can be accomplished with functions in fftpipe.\n\nCreate Clean and Noisy Signals\nLet’s make the sample rate higher than we have been using so far so that it can capture the high frequency white noise better. First, let’s make a clean signal and inspect its waveform and PSD plot.\n\nwv &lt;- waveform(duration_s = 1.0, sr = 500) \n\nwv_clean &lt;- wv %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  )\n\nwv_clean_fft &lt;- wv_clean %&gt;%\n  length_norm() %&gt;%\n  compute_fft()\n\nCreate the plots of the clean signal:\n\nwv_clean %&gt;%\n  length_norm() %&gt;%\n  waveform_plot(\n    title = \"Clean Signal\",\n    x = \"t (s)\", \n    y = \"f(t)\"\n  )\n\n\n\n\n\n\n\n\n\nwv_clean_fft %&gt;%\n  fft_plot(title = \"PSD of Fourier Coefficients, Clean Signal\", x = \"Hz\", y = \"PSD\")\n\n\n\n\n\n\n\n\nNow let’s add some noise!\nThe white_noise() function adds noise from a Gaussian distribution to the signal.\n\nset.seed(123)\n\nwv_noisy &lt;- wv_clean %&gt;%\n  white_noise(mean = 0, sd = 1e-3) %&gt;%\n    length_norm()\n\nwv_noisy_fft &lt;- wv_noisy %&gt;%\n  compute_fft()\n\nAs before, let’s plot the waveform and the PSD of the first half of Fourier coefficients.\n\nwv_noisy %&gt;%\n  waveform_plot(\n    title = \"Noisy Signal\",\n    x = \"t (s)\", \n    y = \"f(t)\"\n  )\n\n\n\n\n\n\n\n\n\nwv_noisy_fft %&gt;%\n  fft_plot(title = \"PSD of Fourier Coefficients, Noisy Signal\", x = \"Hz\", y = \"PSD\")\n\n\n\n\n\n\n\n\nNote that a bunch of Fourier coefficients have lit up with very small PSD values. When we drop these coefficients to zero, we will recover clean Fourier coefficients. Then, when we do an inverse Fourier transform on the cleaned coefficients, we will recover the signal from the noise.\n\nwv_noisy_but_cleaned &lt;- wv_noisy_fft %&gt;%\n  denoise_fft(psd_thresh = 5e-2) %&gt;%\n  inverse_fft(wv, .) %&gt;%\n  length_norm()\n\nwv_noisy_but_cleaned %&gt;%\n  waveform_plot(\n    title = \"Waveform With Noise Removed\", \n    x = \"t (s)\", \n    y = \"f(t)\"\n  )"
  },
  {
    "objectID": "posts/fftpipe/index.html#additional-resources",
    "href": "posts/fftpipe/index.html#additional-resources",
    "title": "R: FFT with fftpipe",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nBrunton, Steve. Denoising Data with FFT [Python] https://youtu.be/s2K1JfNR7Sc\nNeto, João, Fourier Transform: A R Tutorial http://www.di.fc.ul.pt/~jpn/r/fourier/fourier.html"
  },
  {
    "objectID": "posts/two-non-interacting-particles/index.html",
    "href": "posts/two-non-interacting-particles/index.html",
    "title": "Python: Quantum mechanics: two non-interacting particles",
    "section": "",
    "text": "This code and these plots explore Molecular Modelling for Beginners, 2nd Ed. by Alan Hinchliffe Section 12.6, pages 181 to 184. In this Section, the author explores the concepts of particle indistinguishability, symmetric, and antisymmetric wavefunctions. I recommend you read the text for a complete treatment of these concepts. However, I cover the essentials here.\nWe can write a wavefunction and total energy for two non-interacting particles in a one-dimensional box as:\n\\[ \\psi_{n_A,n_B}(x_A, x_B) = \\frac{2}{L} \\sin\\Bigl(\\frac{n_A \\pi x_A}{L}\\Bigr) \\sin\\Bigl(\\frac{n_B \\pi x_B}{L}\\Bigr) \\]\n\\[ E_{n_A,n_B} = (n_{A}^2+n_{B}^2) \\frac{h^2}{8mL^2} \\]\n\\[ n_A, n_B = 1, 2, 3,\\dots \\]\n\\[ 0 \\leqslant x_A \\leqslant L \\]\n\\[ 0 \\leqslant x_B \\leqslant L \\]\nWe need a wavefunction which preserves an indistinguishability requirement. We achieve this by adding or subtracting two wavefunctions. Adding gives us a symmetric wavefunction; subtracting gives us an antisymmetric wavefunction. Symmetric total wavefunctions maintain their sign if we swap the underlying individual wavefunctions. On the other hand, antisymmetric wavefunctions change their sign when we switch the underlying individual wavefunctions. This gives us two more equations, each with an appropriate normalization constant.\nSymmetric for energy levels 1 and 2 (Eqn. 12.19 from Hinchliffe):\n\\[ \\psi_s(x_A, x_B) = \\sqrt\\frac{1}{2} \\bigl(\\psi_{1,2}(x_A, x_B) + \\psi_{2,1}(x_A, x_B)\\bigr) \\]\nAntisymmetric for energy levels 1 and 2 (Eqn. 12.20 from Hinchliffe):\n\\[ \\psi_a(x_A, x_B) = \\sqrt\\frac{1}{2} \\bigl(\\psi_{1,2}(x_A, x_B) - \\psi_{2,1}(x_A, x_B)\\bigr) \\]"
  },
  {
    "objectID": "posts/two-non-interacting-particles/index.html#python-code",
    "href": "posts/two-non-interacting-particles/index.html#python-code",
    "title": "Python: Quantum mechanics: two non-interacting particles",
    "section": "Python code",
    "text": "Python code\nHere is the main class that runs the plots.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom math import sin, sqrt, pi\n\nclass TwoNonInteractingInABox:\n    \"\"\"\n    This class models two non-interacting particles in a one dimensional box.\n    \"\"\"\n    \n    def __init__(self, mass=1, length=1):\n        \"\"\"\n        Mass and length can be in whatever units you like, but\n        make sure these units are consistent with each other\n        to give meaningful results.\n        \n        This class assumes the mass of each particle is the same.\n        \n        Parameters\n        ----------\n        mass: float\n            The mass of each particle\n            \n        length: float\n            The length of the one-dimensional box\n        \"\"\"\n        self.mass = mass\n        self.length = length\n    \n    def wavefunction(self, na, nb, xa, xb):\n        \"\"\"\n        Returns the value of the wavefunction of the two non-interacting\n        particles.\n        \n        For xa and xb:\n        \n        0 &lt; xa &lt; self.length\n        0 &lt; xb &lt; self.length\n        \n        Parameters\n        ----------\n        na: int\n            The quantum number of particle a.\n        \n        nb: int\n            The quantum number of particle b.\n            \n        xa: float\n            The position of particle a.\n        \n        xb: float\n            The position of particle b.\n        \n        Returns\n        -------\n        float\n            value of the wavefunction.\n        \"\"\"\n        return 2 / self.length * sin(na * pi * xa / self.length) * sin(nb * pi * xb / self.length)\n    \n    def symmetric(self, na, nb, xa, xb):\n        \"\"\"\n        Returns the value symmetric wavefunction of the two non-interacting\n        particles.\n        \n        na: int\n            Quantum number of particle a.\n            \n        nb: int\n            Quantum number of particle b.\n            \n        xa: float\n            Position of particle a.\n            \n        xb: float\n            Position of particle b\n            \n        Returns\n        -------\n        float\n            Value of the symmetric wavefunction\n        \"\"\"\n        return sqrt(0.5) * (self.wavefunction(na, nb, xa, xb) + self.wavefunction(nb, na, xa, xb))\n    \n    def antisymmetric(self, na, nb, xa, xb):\n        \"\"\"\n        Returns the value antisymmetric wavefunction of the two non-interacting\n        particles.\n        \n        na: int\n            Quantum number of particle a.\n            \n        nb: int\n            Quantum number of particle b.\n            \n        xa: float\n            Position of particle a.\n            \n        xb: float\n            Position of particle b\n            \n        Returns\n        -------\n        float\n            Value of the antisymmetric wavefunction\n        \"\"\"\n        return sqrt(0.5) * (self.wavefunction(na, nb, xa, xb) - self.wavefunction(nb, na, xa, xb))\n    \n    def prob_density(self, na, nb, symmetric=True, points=100):\n        \"\"\"\n        Returns all the arrays to plot surface or contour plots of the \n        probability density as a function of xa and xb (the positions of\n        each particle).\n        \n        Parameters\n        ----------\n        na: int\n            Quantum number of particle a\n            \n        nb: int\n            Quantum number of particle b\n            \n        symmetric: bool\n            True if the symmetric probability density is needed. False if the\n            antisymmetric desnity is needed.\n        \n        points: int\n            How many points along each axis to sample on the surface.\n        \n        Returns\n        -------\n        np.array, np.array, np.array\n            First array is the 1d array of xa points. Second array is 1d array of\n            xb points. Third array is two dimensional array of probability density\n            at the intersection of the positions of the two particles.\n        \"\"\"\n        xas = np.linspace(0.0, self.length, points)\n        xbs = np.linspace(0.0, self.length, points)\n        zs = np.zeros((points, points), np.float64)\n        for ixa, xa in enumerate(xas):\n            for ixb, xb in enumerate(xbs):\n                if symmetric:\n                    zs[ixa, ixb] = self.symmetric(na, nb, xa, xb) ** 2\n                else:\n                    zs[ixa, ixb] = self.antisymmetric(na, nb, xa, xb) ** 2\n        return xas, xbs, zs\n\n\nSymmetric wavefunction squared\nSurface plot and contour plot of the probability density of two interacting particles.\nThese plots expand upon Fig. 12.12 in Hinchliffe, with an important exception: These plots are of the symmetric wavefunction squared, which matches the caption of the Fig. 12.12 in Hinchliffe. However, the plot in the textbook is not of the squared wavefunction; rather, it is a plot of the wavefunction that is not squared, as can be seen in the book with the negative values on the contour lines. Hence, the plot in the book has a misleading caption. I follow the text’s caption in this example, which makes my contour plots disagree with image in the text’s figure.\n\nbox = TwoNonInteractingInABox(mass=1.0, length=1.0)\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), subplot_kw={\"projection\": \"3d\"})\nnas = [1, 2, 3]\nnbs = [1, 2, 3]\nfor ina, na in enumerate(nas):\n    for inb, nb in enumerate(nbs):\n        xas, xbs, zs = box.prob_density(na=na, nb=nb, symmetric=False)\n        xs, ys = np.meshgrid(xas, xbs)\n        ax = axs[ina, inb]\n        ax.set_xlabel('xa', size=15)\n        ax.set_ylabel('xb', size=15)\n        ax.set_zlabel(f'(Ψa{na},{nb})^2', size=15, color='b')\n        ax.set_title(f'na={na}, nb={nb}', size=20, color='r')\n        ax.plot_surface(ys, xs, zs, cmap=cm.coolwarm, linewidth=0, antialiased=True)\n\n\n\n\n\n\n\n\n\nbox = TwoNonInteractingInABox(mass=1.0, length=1.0)\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), sharex=True, sharey=True)\nnas = [1, 2, 3]\nnbs = [1, 2, 3]\nfor ina, na in enumerate(nas):\n    for inb, nb in enumerate(nbs):\n        xas, xbs, zs = box.prob_density(na=na, nb=nb, symmetric=False)\n        xs, ys = np.meshgrid(xas, xbs)\n        ax = axs[ina, inb]\n        ax.set_xlabel('xa', size=15)\n        ax.set_ylabel('xb', size=15)\n        ax.set_title(f'(Ψa{na},{nb})^2', size=15, color='b')\n        cs = ax.contour(ys, xs, zs, cmap=cm.coolwarm, antialiased=True)\n        ax.clabel(cs, inline=True, fontsize=10)\n\n\n\n\n\n\n\n\n\n\nAntisymmetric wavefunction squared\nSurface plot and contour plot of the probability density of two interacting particles.\nThese plots expand upon Fig. 12.13 in Hinchliffe, which depcits the square of the antisymmetric wavefunction. In contrast to Figure 12.12 in the text, the caption in Fig. 12.13 agrees with the square of wavefunction which is plotted. Hence the values in these plots match those in the text.\nNote in the plots where “na=nb,” the probability density is zero. This satisifies the Pauli exclusion principle for antisymmetric wavefunctions. Our antisymmetric probability densities go to zero when we attempt to place the two particles in the same quantum state.\n\nbox = TwoNonInteractingInABox(mass=1.0, length=1.0)\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), subplot_kw={\"projection\": \"3d\"})\nnas = [1, 2, 3]\nnbs = [1, 2, 3]\nfor ina, na in enumerate(nas):\n    for inb, nb in enumerate(nbs):\n        xas, xbs, zs = box.prob_density(na=na, nb=nb, symmetric=False)\n        xs, ys = np.meshgrid(xas, xbs)\n        ax = axs[ina, inb]\n        ax.set_xlabel('xa', size=15)\n        ax.set_ylabel('xb', size=15)\n        ax.set_zlabel(f'(Ψa{na},{nb})^2', size=15, color='b')\n        ax.set_title(f'na={na}, nb={nb}', size=20, color='r')\n        ax.plot_surface(ys, xs, zs, cmap=cm.coolwarm, linewidth=0, antialiased=True)\n\n\n\n\n\n\n\n\n\nbox = TwoNonInteractingInABox(mass=1.0, length=1.0)\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), sharex=True, sharey=True)\nnas = [1, 2, 3]\nnbs = [1, 2, 3]\nfor ina, na in enumerate(nas):\n    for inb, nb in enumerate(nbs):\n        xas, xbs, zs = box.prob_density(na=na, nb=nb, symmetric=True)\n        xs, ys = np.meshgrid(xas, xbs)\n        ax = axs[ina, inb]\n        ax.set_xlabel('xa', size=15)\n        ax.set_ylabel('xb', size=15)\n        ax.set_title(f'(Ψs{na},{nb})^2', size=15, color='b')\n        cs = ax.contour(ys, xs, zs, cmap=cm.coolwarm, antialiased=True)\n        ax.clabel(cs, inline=True, fontsize=10)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I am Alicia Michelle Farley Key, a PhD student in Structural Biology, Biochemistry, and Biophysics. This is a site where I put fun side projects I do from time to time."
  },
  {
    "objectID": "posts/lennard-jones/index.html",
    "href": "posts/lennard-jones/index.html",
    "title": "Python: Lennard-Jones Potential",
    "section": "",
    "text": "\\[ V(r) = 4 \\epsilon \\biggl[ \\biggl (\\frac{\\sigma}{r}\\biggr)^{12} -  \\biggl (\\frac{\\sigma}{r}\\biggr)^6 \\biggr] \\]\n\\[ \\sigma = \\frac{r_m}{2^{1/6}} \\]\nHere, epsilon is the energy minimum and r_m is the distance of the energy minimum.\nNote the part of the equation inside the square brackets. Recall that negative energies represent a more favorable interaction. The attractive term (raised to the power of 6) is subtracted from the repulsive term (raised to the power of 12).\nSee the Lennard-Jones (L-J) potential on Wikipedia for more details. Also, L-J models are part of a wider set of models of Interatomic potentials.\n\n\n\nIn the calculations and plot that follows, all distance units are in meters and energy units are in kJ/mol. I chose kJ/mol as the energy unit arbitrarily. I chose meters as the distance unit because the orders of magnitude at the atomic scale are the easiest to plot (note that the x-scale of the plot is marked in nanometers.)\n\n\nrmin and epsilon define shape of the potential and rmax is the maximum distance at which I calculate and plot the potential. My selection of values for rmin and epsilon are arbitrary and I selected them for ease of interpretation in the plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrmin = 0.5e-9   # 2 angstroms\nrmax = 1.0e-9  # 10 angstroms\nepsilon = 1  # kJ/mol\n\n\n\n\nrs contains the radii that are in plot and vs contains the corresponding potentials. I calculate vs by NumPy array broadcasting which eliminates the need for a loop.\n\nrs = np.linspace(rmin - 0.75e-10, rmax, 200)\nsigma = rmin / 2**(1/6)\nvs = 4 * epsilon * ((sigma/rs)**12 - (sigma/rs)**6)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,7))\nax.set_xlim(min(rs), max(rs))\nax.set_ylim(min(vs)*1.1, max(vs))\nax.axhline(y=0, color='r', linestyle='dotted')\nax.plot(rs, vs)\nax.axvline(rs[vs.argmin()], color='orange')\nax.set_title(f'L-J Potential', size=20)\nax.set_xlabel('r (m)', size=15)\nax.set_ylabel('V (kJ/mol)', size=15)\n\nText(0, 0.5, 'V (kJ/mol)')\n\n\n\n\n\n\n\n\n\n\n\n\nI noted V(r) = 0 kJ/mol with a horizontal red dashed line and the position of the minimum potential with a vertical orange line.\nRecall that the equation contains a repulsive term and an attractive term. The most favorable distance for this interaction is at 0.5 nm (5 angstroms) because that has the minimum energy. The repulsive term lies to the left of the orange line and rapidly increases as the radius approaches zero. This represents repulsion between atoms if they get too close. The attractive part of the potential lies to the right of the orange line and asymptotically approaches 0 as the atoms drift apart."
  },
  {
    "objectID": "posts/lennard-jones/index.html#equation",
    "href": "posts/lennard-jones/index.html#equation",
    "title": "Python: Lennard-Jones Potential",
    "section": "",
    "text": "\\[ V(r) = 4 \\epsilon \\biggl[ \\biggl (\\frac{\\sigma}{r}\\biggr)^{12} -  \\biggl (\\frac{\\sigma}{r}\\biggr)^6 \\biggr] \\]\n\\[ \\sigma = \\frac{r_m}{2^{1/6}} \\]\nHere, epsilon is the energy minimum and r_m is the distance of the energy minimum.\nNote the part of the equation inside the square brackets. Recall that negative energies represent a more favorable interaction. The attractive term (raised to the power of 6) is subtracted from the repulsive term (raised to the power of 12).\nSee the Lennard-Jones (L-J) potential on Wikipedia for more details. Also, L-J models are part of a wider set of models of Interatomic potentials."
  },
  {
    "objectID": "posts/lennard-jones/index.html#make-plot-of-an-example-l-j-potential",
    "href": "posts/lennard-jones/index.html#make-plot-of-an-example-l-j-potential",
    "title": "Python: Lennard-Jones Potential",
    "section": "",
    "text": "In the calculations and plot that follows, all distance units are in meters and energy units are in kJ/mol. I chose kJ/mol as the energy unit arbitrarily. I chose meters as the distance unit because the orders of magnitude at the atomic scale are the easiest to plot (note that the x-scale of the plot is marked in nanometers.)\n\n\nrmin and epsilon define shape of the potential and rmax is the maximum distance at which I calculate and plot the potential. My selection of values for rmin and epsilon are arbitrary and I selected them for ease of interpretation in the plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrmin = 0.5e-9   # 2 angstroms\nrmax = 1.0e-9  # 10 angstroms\nepsilon = 1  # kJ/mol\n\n\n\n\nrs contains the radii that are in plot and vs contains the corresponding potentials. I calculate vs by NumPy array broadcasting which eliminates the need for a loop.\n\nrs = np.linspace(rmin - 0.75e-10, rmax, 200)\nsigma = rmin / 2**(1/6)\nvs = 4 * epsilon * ((sigma/rs)**12 - (sigma/rs)**6)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,7))\nax.set_xlim(min(rs), max(rs))\nax.set_ylim(min(vs)*1.1, max(vs))\nax.axhline(y=0, color='r', linestyle='dotted')\nax.plot(rs, vs)\nax.axvline(rs[vs.argmin()], color='orange')\nax.set_title(f'L-J Potential', size=20)\nax.set_xlabel('r (m)', size=15)\nax.set_ylabel('V (kJ/mol)', size=15)\n\nText(0, 0.5, 'V (kJ/mol)')\n\n\n\n\n\n\n\n\n\n\n\n\nI noted V(r) = 0 kJ/mol with a horizontal red dashed line and the position of the minimum potential with a vertical orange line.\nRecall that the equation contains a repulsive term and an attractive term. The most favorable distance for this interaction is at 0.5 nm (5 angstroms) because that has the minimum energy. The repulsive term lies to the left of the orange line and rapidly increases as the radius approaches zero. This represents repulsion between atoms if they get too close. The attractive part of the potential lies to the right of the orange line and asymptotically approaches 0 as the atoms drift apart."
  },
  {
    "objectID": "posts/particle-in-box/index.html",
    "href": "posts/particle-in-box/index.html",
    "title": "Python: Quantum mechanics: particle in a box",
    "section": "",
    "text": "This post explores the simplest quatnum mechanical system: the particle in a one-dimensioanl infinite potential well, also known as the one-dimensional particle in a box.\n\\[ \\psi_{n}(x) = \\sqrt{\\frac{2}{L}} \\sin\\Bigl(\\frac{n \\pi x}{L}\\Bigr) \\]\n\\[ E_n = U_0 + \\frac{n^2 h^2}{8 m L^2} \\]\n\\[ n = 1, 2, 3, \\dots \\]"
  },
  {
    "objectID": "posts/particle-in-box/index.html#particle-in-a-one-dimensional-box",
    "href": "posts/particle-in-box/index.html#particle-in-a-one-dimensional-box",
    "title": "Python: Quantum mechanics: particle in a box",
    "section": "Particle in a one-dimensional box",
    "text": "Particle in a one-dimensional box\nThis Python code on this page explores Section 11.3.1 in Molecular Modeling for Beginners, 2nd Ed. by Alan Hinchliffe. In this Section, the author described the particle in a one-dimensional infinite well.\nIt has the following equations to describe its behavior:\n\\[ \\psi_{n}(x) = \\sqrt{\\frac{2}{L}} \\sin\\Bigl(\\frac{n \\pi x}{L}\\Bigr) \\]\n\\[ E_n = U_0 + \\frac{n^2 h^2}{8 m L^2} \\]\n\\[ n = 1, 2, 3, \\dots \\]\n\\[ 0 \\leqslant x \\leqslant L \\]\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nfrom math import sin, sqrt, pi\n\nclass ParticleInABox1d:\n    \"\"\"\n    This class models a one-dimensional particle in a box.\n    \"\"\"\n    \n    def __init__(self, mass, length):\n        \"\"\"\n        Units for mass and length can be in any units you wish,\n        as long as they are consistent.\n        \n        Parameters\n        ----------\n        mass: float\n            The mass of the particle.\n            \n        length: float\n            Length of the one-dimensional box.\n        \"\"\"\n        self.mass = mass\n        self.length = length\n        \n    def wavefunction(self, n, x):\n        \"\"\"\n        Returns the value of the wavefunction with quantum number n\n        at position x.\n        \n        Parameters\n        ----------\n        n: int\n            Quantum number of the box.\n        \n        x: float\n            Postion in the box to evaluate. 0 &lt;= x &lt;= self.length\n            \n        Returns\n        -------\n        float\n            The value of the wavefunction at that point.\n        \"\"\"\n        return sqrt(2 / self.length) * sin(n * pi * x / self.length)\n    \n    def prob_density(self, n, points=100):\n        \"\"\"\n        Returns the probability density at the given number of points\n        along the length of the box.\n        \n        Parameters\n        ----------\n        n: int\n            The quantum number.\n        \n        points: int\n            The number of points to sample along the length of the\n            box.\n            \n        Returns\n        -------\n        np.array, list\n            The first array are the x coordinates along the box, and the second\n            array is the probability density at that point.\n        \"\"\"\n        xs = np.linspace(0, self.length, points)\n        ys = [self.wavefunction(n, x) ** 2 for x in xs]\n        return xs, ys\n\n\nbox = ParticleInABox1d(1, 1)\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 15), sharex=True, sharey=True)\n\nxs, ys = box.prob_density(n=1)\naxs[0, 0].set_title('n = 1', size=20, color='r')\naxs[0, 0].set_ylabel('Ψ1(x)^2', size=15, color='b')\naxs[0, 0].set_xlabel('x', size=15)\naxs[0, 0].set_xlim(0, box.length)\naxs[0, 0].set_ylim(0, max(ys) * 1.05)\naxs[0, 0].plot(xs, ys)\n\nxs, ys = box.prob_density(n=2)\naxs[0, 1].set_title('n = 2', size=20, color='r')\naxs[0, 1].set_ylabel('Ψ2(x)^2', size=15, color='b')\naxs[0, 1].set_xlabel('x', size=15)\naxs[0, 1].set_xlim(0, box.length)\naxs[0, 1].set_ylim(0, max(ys) * 1.05)\naxs[0, 1].plot(xs, ys)\n\nxs, ys = box.prob_density(n=3)\naxs[1, 0].set_title('n = 3', size=20, color='r')\naxs[1, 0].set_ylabel('Ψ3(x)^2', size=15, color='b')\naxs[1, 0].set_xlabel('x', size=15)\naxs[1, 0].set_xlim(0, box.length)\naxs[1, 0].set_ylim(0, max(ys) * 1.05)\naxs[1, 0].plot(xs, ys)\n\nxs, ys = box.prob_density(n=4)\naxs[1, 1].set_title('n = 4', size=20, color='r')\naxs[1, 1].set_ylabel('Ψ4(x)^2', size=15, color='b')\naxs[1, 1].set_xlabel('x', size=15)\naxs[1, 1].set_xlim(0, box.length)\naxs[1, 1].set_ylim(0, max(ys) * 1.05)\naxs[1, 1].plot(xs, ys)\n\nxs, ys = box.prob_density(n=8)\naxs[2, 0].set_title('n = 8', size=20, color='r')\naxs[2, 0].set_ylabel('Ψ8(x)^2', size=15, color='b')\naxs[2, 0].set_xlabel('x', size=15)\naxs[2, 0].set_xlim(0, box.length)\naxs[2, 0].set_ylim(0, max(ys) * 1.05)\naxs[2, 0].plot(xs, ys)\n\nxs, ys = box.prob_density(n=16)\naxs[2, 1].set_title('n = 16', size=20, color='r')\naxs[2, 1].set_ylabel('Ψ16(x)^2', size=15, color='b')\naxs[2, 1].set_xlabel('x', size=15)\naxs[2, 1].set_xlim(0, box.length)\naxs[2, 1].set_ylim(0, max(ys) * 1.05)\naxs[2, 1].plot(xs, ys)\n\n\n\n\n\n\n\n\nThe plots in Figure 1 show the probability density for increasing values of the quantum number n. The greater the curve’s height, the higher the probability of finding the particle in that region of the box. At the lowest quantum number n=1, the particle is most likely to be found in an area surrounding the middle of the box. As n increases, the particle is more likely to be found in more positions of the box. The curve at n=16 demonstrates the important correspondence principle. The correspondence principle says that as quantum number increases, the particle position predicted by quantum mechanics approaches the position indicated by classical mechanics."
  },
  {
    "objectID": "posts/particle-in-box/index.html#particle-in-a-two-dimensional-box",
    "href": "posts/particle-in-box/index.html#particle-in-a-two-dimensional-box",
    "title": "Python: Quantum mechanics: particle in a box",
    "section": "Particle in a two-dimensional box",
    "text": "Particle in a two-dimensional box\nNote: The two-dimensional particle in a box should not be confused with two non-interacting particles in a one-dimensional box. They are completely different concepts.\nHere, I follow Hinchliffe Section 11.5\nThe particle in a two-dimensional box extends the particle in a box concept to two dimensions. It still contains a single particle; however, this particle lies on a plane surrounded by walls of infinite potential. It uses quantum numbers n and k for the levels along the x-axis and y-axis, respectively (some authors use “nx” and “ny” for these numbers).\nI will simplfy the equations by assuming the side of the box are both equal to length L\n\\[ E_{n,k} = U_0 + \\frac{n^2 h^2}{8 m L^2} + \\frac{k^2 h^2}{8 m L^2} \\]\n\\[ \\psi_{n,k}(x, y) = \\frac{2}{L} \\sin\\Bigl(\\frac{n \\pi x}{L}\\Bigr) \\sin\\Bigl(\\frac{k \\pi y}{L}\\Bigr) \\]\n\\[ n, k = 1, 2, 3, \\dots \\]\n\\[ 0 \\leqslant x \\leqslant L \\]\n\\[ 0 \\leqslant y \\leqslant L \\]\n\nclass ParticleInABox2d:\n    def __init__(self, mass=1, length=1):\n        \"\"\"\n        Parameters\n        ----------\n        mass: float\n            The mass of the particle.\n        \n        length:\n            Legnth of each side of the box in x and y dimensions.\n        \"\"\"\n        self.mass = mass\n        self.length = length\n        \n    def wavefunction(self, n, k, x, y):\n        \"\"\"\n        Parameters\n        ----------\n        n: int\n            Quantum number in the x dimension\n            \n        k: int\n            Quantum number in the y dimension\n            \n        x: float\n            x position in the box. Range from zero to self.length\n            \n        y: float\n            y position in the box. Range from zero to self.length\n            \n        Returns\n        -------\n        float\n            The value of the wavefunction n, k at x, y\n        \"\"\"\n        return (2 / self.length) * sin(n * pi * x / self.length) * sin(k * pi * y / self.length)\n    \n    def prob_density(self, n, k, points=100):\n        \"\"\"\n        Parameters\n        ----------\n        n: int\n            Quantum number in the x dimension\n        \n        k: int\n            Quantum number in the y dimension\n            \n        Returns\n        -------\n        np.array\n            Float 2d numpy array of squares of wavefunctions at the various\n            points.\n        \"\"\"\n        xs = np.linspace(0.0, self.length, points)\n        ys = np.linspace(0.0, self.length, points)\n        zs = np.zeros((points, points), np.float64)\n        for ix, x in enumerate(xs):\n            for iy, y in enumerate(ys):\n                zs[ix, iy] = self.wavefunction(n, k, x, y) ** 2\n        return xs, ys, zs\n\n\nbox_2d = ParticleInABox2d(mass=1.0, length=1.0)\nns = [1, 2, 3]\nks = [1, 2, 3]\nfig, axs = plt.subplots(nrows=len(ns), ncols=len(ks), figsize=(15, 15), subplot_kw={\"projection\": \"3d\"})\n\nfor idx_n, n in enumerate(ns):\n    for idx_k, k in enumerate(ks):\n        xs, ys, zs = box_2d.prob_density(n, k)\n        xs, ys = np.meshgrid(xs, ys)\n        ax = axs[idx_n, idx_k]\n        ax.set_title(f'n={n}, k={k}', size=20, color='r')\n        ax.set_zlabel(f'Ψ{n},{k}(x, y)^2', size=15, color='b')\n        ax.set_xlabel('x', size=15)\n        ax.set_ylabel('y', size=15)\n        ax.plot_surface(ys, xs, zs, cmap=cm.RdYlBu, linewidth=0, antialiased=True)\n\n\n\n\n\n\n\n\n\nbox_2d = ParticleInABox2d(mass=1.0, length=1.0)\nns = [1, 2, 3]\nks = [1, 2, 3]\nfig, axs = plt.subplots(nrows=len(ns), ncols=len(ks), figsize=(15, 15), sharex=True, sharey=True)\n\nfor idx_n, n in enumerate(ns):\n    for idx_k, k in enumerate(ks):\n        xs, ys, zs = box_2d.prob_density(n, k)\n        xs, ys = np.meshgrid(xs, ys)\n        ax = axs[idx_n, idx_k]\n        ax.set_title(f'Ψ{n},{k}(x, y)^2', size=15, color='b')\n        ax.set_xlabel('x', size=15)\n        ax.set_ylabel('y', size=15)\n        cs = ax.contour(ys, xs, zs, cmap=cm.RdYlBu, antialiased=True)\n        ax.clabel(cs, inline=True, fontsize=10)\n\n\n\n\n\n\n\n\nIn Figures 2 and 3, the height of the curves, or the higher values of the innermost contour line, show a greater probability that the particle will live in the corresponding region of the box. As in the one-dimensional case, we see that for higher values of n and k, there are more peaks in the corresponding axes of the two-dimensional box."
  }
]