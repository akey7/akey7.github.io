[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Julia: Strogatz Ch. 05 Figures\n\n\n\njulia\n\nmath\n\n\n\n\n\n\n\n\n\nApr 26, 2025\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPyTorch: Deep Learning Organic Chemistry\n\n\n\npytorch\n\npython\n\nchemistry\n\n\n\n\n\n\n\n\n\nMar 12, 2025\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nJulia: Multivariate Normal Distribution\n\n\n\njulia\n\nmath\n\n\n\n\n\n\n\n\n\nOct 5, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nJulia: Metropolis Algorithm\n\n\n\njulia\n\nmath\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nJulia: Michaelis-Menten Kinetics\n\n\n\njulia\n\nchemistry\n\n\n\n\n\n\n\n\n\nOct 3, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nJulia: Connected Reversible Linear Reaction Network Solver\n\n\n\njulia\n\nchemistry\n\n\n\n\n\n\n\n\n\nSep 29, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nTableau, SQL: Cafe Rewards Analysis\n\n\n\nanalysis\n\nsql\n\ntableau\n\n\n\n\n\n\n\n\n\nAug 19, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPowerBI, SQL: Pizza Place\n\n\n\nanalysis\n\nsql\n\npowerbi\n\n\n\n\n\n\n\n\n\nJul 20, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPowerBI: Hospital Visits\n\n\n\nanalysis\n\npowerbi\n\n\n\n\n\n\n\n\n\nJul 4, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nJulia: Molecular Dynamics of HCl\n\n\n\njulia\n\nphysics\n\nchemistry\n\n\n\n\n\n\n\n\n\nMar 23, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nJulia: Positive and Negative Definite Matrices\n\n\n\njulia\n\nmath\n\n\n\n\n\n\n\n\n\nMar 19, 2024\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Simple Gradient Descent\n\n\n\npython\n\nmath\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nR: FFT with fftpipe\n\n\n\nr\n\nmath\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Lennard-Jones Potential\n\n\n\npython\n\nphysics\n\nchemistry\n\n\n\n\n\n\n\n\n\nSep 10, 2022\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Hydrogen Atom\n\n\n\npython\n\nphysics\n\nchemistry\n\n\n\n\n\n\n\n\n\nJul 16, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Rydberg Equation and Hydrogen Spectra\n\n\n\npython\n\nphysics\n\nchemistry\n\n\n\n\n\n\n\n\n\nJul 15, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Quantum harmonic oscillator\n\n\n\npython\n\nphysics\n\nchemistry\n\n\n\n\n\n\n\n\n\nJul 13, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Quantum mechanics: two non-interacting particles\n\n\n\npython\n\nphysics\n\nchemistry\n\n\n\n\n\n\n\n\n\nJul 11, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nPython: Quantum mechanics: particle in a box\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nJul 5, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nR: Solubility Clustering\n\n\n\nchemistry\n\nr\n\nsolubility\n\n\n\n\n\n\n\n\n\nApr 25, 2021\n\n\nAlicia\n\n\n\n\n\n\n\n\n\n\n\n\nR: Solubility Regression with Linear and Random Forest Models\n\n\n\nchemistry\n\nr\n\nsolubility\n\n\n\n\n\n\n\n\n\nJan 18, 2021\n\n\nAlicia\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I am Alicia Michelle Farley Key, a PhD student in Structural Biology, Biochemistry, and Biophysics. This is a site where I put fun side projects I do from time to time."
  },
  {
    "objectID": "posts/solubility-regression/index.html",
    "href": "posts/solubility-regression/index.html",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "",
    "text": "Aqueous solubility (ability to dissolve in water) is an essential property of a chemical compound important in the laboratory. Can the solubility of a compound be predicted based on a chemical structure alone? John Delaney posed this predictions question in 2004 (Delaney 2004) and wrote a paper with numerous citations in the chemistry literature. This study will take a dataset similar to that study and use linear and random forest regression to predict the compounds’ solubilities.\nThe random forest model is a much better predictor of solubilities.\nThis code for this study is implemented in R and is available in its entirety on GitHub.\nA number of compounds in this dataset are well-known, even outside the chemistry community. Here is a sample of what lies inside the dataset:"
  },
  {
    "objectID": "posts/solubility-regression/index.html#dataset-description",
    "href": "posts/solubility-regression/index.html#dataset-description",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe original report published a dataset of compounds represented as SMILES strings. SMILES strings are a compact and text-based method of specifying chemical structures. This study will use a preprocessed dataset mentioned on moleculenet.ai and distributed by deepchem.io, which contains features parsed from these SMILES strings. You can browse the file on GitHub.This study uses a subset of these preprocessed features, which are listed in Table 2.\n\nTable 2: Features of each compound used in the regression\n\n\n\n\n\n\n\n\nFeature name\nUnits\nDescription\n\n\n\n\nmw\ng/mol\nThe molecular weight of the compound.\n\n\nsolubility\nlog(mol/L)\nThe log solubility, in mol/L. Solubility is the response variable of this study.\n\n\npsa\nÅ2\nThe polar surface area of a molecule.\n\n\nh_bond_donors\nunitless\nThe number of hydrogen bond donors on a molecule.1\n\n\nrotatable_bonds\nunitless\nThe number of rotatable bonds within a molecule.2"
  },
  {
    "objectID": "posts/solubility-regression/index.html#exploratory-visualization",
    "href": "posts/solubility-regression/index.html#exploratory-visualization",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Exploratory visualization",
    "text": "Exploratory visualization\n\ndf &lt;- as_tibble(read.csv(\"data/delaney-processed.csv\")) %&gt;%\n  select(\n    compound = Compound.ID, \n    mw = Molecular.Weight, \n    h_bond_donors = Number.of.H.Bond.Donors, \n    rings = Number.of.Rings, \n    rotatable_bonds = Number.of.Rotatable.Bonds, \n    psa = Polar.Surface.Area, \n    solubility = measured.log.solubility.in.mols.per.litre\n)\n\nBefore I dive into the machine learning model, let’s examine exploratory plots to get a feel for the data distribution. Figure 1 has histograms (for continuous variables) and bar plots (for discrete variables) to demonstrate the dataset’s values’ distributions. Figure 1a, 1b, 1d, 1e, and 1f show distributions of values favoring their respective range’s low ends. Solubility, our response variable, has a broader spread above and below its mean of -3.05.\n\np1 &lt;- ggplot(df, aes(x = mw)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(a)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df, aes(x = psa)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(b)\") +\n  theme_minimal()\n\np3 &lt;- ggplot(df, aes(x = solubility)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(c)\") +\n  theme_minimal()\n\np4 &lt;- ggplot(df, aes(x = h_bond_donors)) +\n  geom_bar() +\n  labs(title = \"(d)\") +\n  theme_minimal()\n\np5 &lt;- ggplot(df, aes(x = rings)) +\n  geom_bar() +\n  labs(title = \"(e)\") +\n  theme_minimal()\n\np6 &lt;- ggplot(df, aes(x = rotatable_bonds)) +\n  geom_bar() +\n  labs(title = \"(f)\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, p3, p4, p5, p6, nrow = 3)\n\n\n\n\nHistograms and bar plots of variables\n\n\n\n\nFigure 1a, 1b, 1d, 1e, and 1f show distributions of values favoring their respective range’s low ends. Solubility, in Figure 1c, has a broader spread above and below its mean of -3.05.\nA number of the features of the molecules require lots of atoms. For example, a five-ring molecule will likely have a higher molecular weight than a three-ring molecule. Molecular weight has a special relationship with all other variables that denote each molecule’s increasing structural complexity. Figure 2 plots molecular weight against all other variables, with a trend line for each relationship, as shown in the jittered scatter plots below.\n\nalpha = 0.1\np1 &lt;- ggplot(df, aes(x = psa, y = mw)) +\n  geom_jitter(alpha = alpha) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(a)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df, aes(x = solubility, y = mw)) +\n  geom_jitter(alpha = alpha) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(b)\") +\n  theme_minimal()\n\np3 &lt;- ggplot(df, aes(x = h_bond_donors, y = mw)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(c)\") +\n  theme_minimal()\n\np4 &lt;- ggplot(df, aes(x = rings, y = mw)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(d)\") +\n  theme_minimal()\n\np5 &lt;- ggplot(df, aes(x = rotatable_bonds, y = mw)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(e)\") +\n  theme_minimal()\n\ngrid.arrange(p1, p2, p3, p4, p5, nrow = 3)\n\n\n\n\nRelationship of molecular weight to other variables\n\n\n\n\nFigures 2a, 2c, 2d, 2e all exhibit increasing molecular weight with increased structural complexity. Figure 2b stands out: in general, as molecular weight increases, solubility decreases."
  },
  {
    "objectID": "posts/solubility-regression/index.html#traintest-split",
    "href": "posts/solubility-regression/index.html#traintest-split",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Train/test split",
    "text": "Train/test split\nAll models use the same randomized train/test split. First, I shuffled the rows of the original dataset. Then, I selected the first 846 rows for the training set and the last 282 rows for the test dataset. I seeded the random number generator with a constant to ensure the same shuffles between model runs.\n\nsolubilityTrainTestSplit &lt;- function(all_data, split_fraction = 0.75, random_seed = 13) {\n  # Shuffle based on random seed\n  set.seed(random_seed)\n  sample_indecies &lt;- sample(nrow(all_data), nrow(all_data))\n  shuffled &lt;- all_data[sample_indecies, ]\n  \n  # Train test split\n  train_row &lt;- round(nrow(shuffled) * split_fraction)\n  test_row &lt;- train_row + 1\n  train &lt;- shuffled[1:train_row, ]\n  test &lt;- shuffled[test_row:nrow(shuffled), ]\n  \n  # Now create the final list that is returned\n  list(\n    train = train,\n    test = test\n  )\n}\n\nsplit &lt;- solubilityTrainTestSplit(df)\ntest &lt;- split$test\ntrain &lt;- split$train\n\nTable 3 is the first few rows of the train dataset:\n\nTable 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompound\nmw\nh_bond_donors\nrings\nrotatable_bonds\npsa\nsolubility\n\n\n\n\n2-Ethyl-2-hexanal\n126.199\n0\n0\n4\n17.07\n-2.460\n\n\n3-Butanoyloxymethylphenytoin\n352.390\n1\n3\n6\n75.71\n-5.071\n\n\nTrichloromethane\n119.378\n0\n0\n0\n0.00\n-1.170\n\n\nIndole\n117.151\n1\n2\n0\n15.79\n-1.520\n\n\n1,2,3,4-Tetrahydronapthalene\n132.206\n0\n2\n0\n0.00\n-4.370\n\n\n2,2’,3,4,5-PCB\n326.437\n0\n2\n1\n0.00\n-7.210\n\n\n\n\n\nTable 4 is the first few rows of the test dataset:\n\n\nTable 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompound\nmw\nh_bond_donors\nrings\nrotatable_bonds\npsa\nsolubility\n\n\n\n\nXipamide\n354.815\n3\n2\n3\n109.49\n-3.790\n\n\nDiallate\n270.225\n0\n0\n4\n20.31\n-4.286\n\n\nm-Fluorobromobenzene\n175.000\n0\n1\n0\n0.00\n-2.670\n\n\n3,5-Dimethylphenol\n122.167\n1\n1\n0\n20.23\n-1.400\n\n\nparabanic acid\n114.060\n2\n1\n0\n75.27\n-0.400\n\n\np-Nitroaniline\n138.126\n1\n1\n1\n69.16\n-2.370"
  },
  {
    "objectID": "posts/solubility-regression/index.html#formula",
    "href": "posts/solubility-regression/index.html#formula",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Formula",
    "text": "Formula\nAll models use the same formula based on all the predictors.\n\nformula &lt;- solubility ~ mw + h_bond_donors + rings + rotatable_bonds + psa"
  },
  {
    "objectID": "posts/solubility-regression/index.html#linear-model",
    "href": "posts/solubility-regression/index.html#linear-model",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Linear Model",
    "text": "Linear Model\nOur first stop is the linear model. Table 3 shows the performance of the linear model after training when I use it to predict the solubilties in the test dataset.\n\nTable 3: linear model performance on test data\n\nanalyzeSolubilityLinearModel &lt;- function(model, test_data) {\n  # Run the prediction\n  predicted_solbilities &lt;- predict(model, test_data)\n  \n  # Assemble the compounds and predictions back onto the\n  # train features\n  \n  test_results &lt;- test_data %&gt;%\n    mutate(prediction = predicted_solbilities) %&gt;%\n    mutate(residual = solubility - prediction)\n  \n  # Calculate the standard deviation and RMSE\n  sd_solubilities &lt;- sd(test_results$solubility)\n  rmse &lt;- sqrt(mean(test_results$residual ^ 2))\n  \n  # calculate r_squared\n  rss &lt;- sum(test_results$residual ^ 2)\n  total_error &lt;- test_results$solubility - mean(test_results$solubility)\n  tss &lt;- sum(total_error ^ 2)\n  r_squared &lt;- 1 - (rss / tss)\n  \n  # Create a list to return to the caller\n  list(\n    sd_solubilities = sd_solubilities,\n    rmse = rmse,\n    test_results = test_results,\n    r_squared = r_squared\n  )\n}\n\nlm_model &lt;- lm(solubility ~ mw + h_bond_donors + rings + rotatable_bonds + psa, train)\nlm_result &lt;- analyzeSolubilityLinearModel(lm_model, test)\nlm_result_df &lt;- tibble(\n  metric = c(\"RMSE\", \"R^2\"),\n  value = c(lm_result$rmse, lm_result$r_squared)\n)\nknitr::kable(lm_result_df)\n\n\n\n\nmetric\nvalue\n\n\n\n\nRMSE\n1.1955559\n\n\nR^2\n0.6721196\n\n\n\n\n\nThe RMSE of 1.196 is less than the test solubility’s standard deviation of 2.092, but the R^2 of 0.672 shows the linear model does not explain much of the variance of underlying solubility."
  },
  {
    "objectID": "posts/solubility-regression/index.html#random-forest-regression",
    "href": "posts/solubility-regression/index.html#random-forest-regression",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Random Forest Regression",
    "text": "Random Forest Regression\nThe second model is the random forest model. For the random forest, I use the ranger package. The model uses 500 trees and is trained and tested with the same train and test data as the original linear model. Table 4 shows the random forest’s prediction performance on the test data after training.\n\nTable 4: Random forest test data performance\n\nanalyzeSolubilityRandomForestModel &lt;- function(model, test_data) {\n  # Run the prediction\n  predicted_solbilities &lt;- predict(model, test_data)$predictions\n  \n  # Assemble the compounds and predictions back onto the\n  # train features\n  \n  test_results &lt;- test_data %&gt;%\n    mutate(prediction = predicted_solbilities) %&gt;%\n    mutate(residual = solubility - prediction)\n  \n  # Calculate the standard deviation and RMSE\n  sd_solubilities &lt;- sd(test_results$solubility)\n  rmse &lt;- sqrt(mean(test_results$residual ^ 2))\n  \n  # calculate r_squared\n  rss &lt;- sum(test_results$residual ^ 2)\n  total_error &lt;- test_results$solubility - mean(test_results$solubility)\n  tss &lt;- sum(total_error ^ 2)\n  r_squared &lt;- 1 - (rss / tss)\n  \n  # Create a list to return to the caller\n  list(\n    sd_solubilities = sd_solubilities,\n    rmse = rmse,\n    test_results = test_results,\n    r_squared = r_squared\n  )\n}\n\nrf_model &lt;- ranger(\n  formula,\n  train,\n  num.trees = 500,\n  respect.unordered.factors = \"order\"\n)\n\nrf_result &lt;- analyzeSolubilityRandomForestModel(rf_model, test)\nrf_result_df = tibble(\n  metric = c(\"RMSE\", \"R^2\"),\n  value = c(rf_result$rmse, rf_result$r_squared)\n)\n\nknitr::kable(rf_result_df)\n\n\n\n\nmetric\nvalue\n\n\n\n\nRMSE\n0.8665462\n\n\nR^2\n0.8277501"
  },
  {
    "objectID": "posts/solubility-regression/index.html#comparing-linear-and-random-forest-models",
    "href": "posts/solubility-regression/index.html#comparing-linear-and-random-forest-models",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Comparing linear and random forest models",
    "text": "Comparing linear and random forest models\nTable 5 shows the performance metrics of the linear and random forest models.\n\nTable 5\n\nmodel_comparison_df &lt;- tibble(\n  model = c(\"Linear\", \"Random forest\"),\n  rmse = c(lm_result$rmse, rf_result$rmse),\n  r_squared = c(lm_result$r_squared, rf_result$r_squared)\n)\n\nknitr::kable(model_comparison_df)\n\n\n\n\nmodel\nrmse\nr_squared\n\n\n\n\nLinear\n1.1955559\n0.6721196\n\n\nRandom forest\n0.8665462\n0.8277501\n\n\n\n\n\nExamining Table 5 shows reveals two comparisons between the models that are immediately apparent. The first comparison is their respective RMSE values when evaluated on the training dataset. Lower RMSE values are better. The standard deviation of log solubility in the test dataset is 2.092. This value compares nicely with the random forest’s RMSE of 0.867, but it does not compare well with the linear model’s RMSE of 1.196. The clear winner here, unsurprisingly, the random forest. The second comparison is their respective R^2 values. R^2 values closer to 1.0 are better. The R^2 value of the linear model is a lowly 0.672, while the R^2 value for the random forest is much better at 0.828.\nFigure 3, which plots actual versus predicted solubilities, is my favorite comparison between the linear and random forest models. Recall that all solubilities are log solubilities, hence the negative values on the axes.\n\nalpha &lt;- 0.5\n\nlinear_result_plot &lt;- ggplot(lm_result$test_results, aes(x = prediction, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(a) Linear\") +\n  theme_minimal()\n\nrf_result_plot &lt;- ggplot(rf_result$test_results, aes(x = prediction, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  stat_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(b) Random forest\") +\n  theme_minimal()\n\ngrid.arrange(linear_result_plot, rf_result_plot, nrow = 1)\n\n\n\n\nactual vs. predicted values for both models\n\n\n\n\nIn Figure 3a, the linear model fit is lacking in the area of high predicted solubility. In this area, the actual solubilities take a wide range, from relatively insoluble to very soluble. At the edge of low predicted solubility, we find a range of actual solubilities.\nIn Figure 3b, the random forest model trends slightly more reliable. The random forest predicts high solubilities where there are actual high solubilities. Near the low end of the actual solubilities, there are numerous under and over predictions from the random forest."
  },
  {
    "objectID": "posts/solubility-regression/index.html#conclusion",
    "href": "posts/solubility-regression/index.html#conclusion",
    "title": "R: Solubility Regression with Linear and Random Forest Models",
    "section": "Conclusion",
    "text": "Conclusion\nThe random forest performs better than the linear model but still has deficiencies around molecular features causing over or under predicted solubilities."
  },
  {
    "objectID": "posts/fftpipe/index.html",
    "href": "posts/fftpipe/index.html",
    "title": "R: FFT with fftpipe",
    "section": "",
    "text": "fftpipe is a package of functions that wrap around the base R fft() function. The fftpipe package enables workflows around the fft() function that use the pipe (%&gt;%) operator. I took inspiration for the interface to fftpipe from the Tidyverse and tidymodels packages.\nSpecifically, fftpipe offers the following functionality:\n\nWaveform generation,\nFFT and inverse FFT transformation,\nPlotting of these waveforms and FFTs."
  },
  {
    "objectID": "posts/fftpipe/index.html#what-is-fftpipe",
    "href": "posts/fftpipe/index.html#what-is-fftpipe",
    "title": "R: FFT with fftpipe",
    "section": "",
    "text": "fftpipe is a package of functions that wrap around the base R fft() function. The fftpipe package enables workflows around the fft() function that use the pipe (%&gt;%) operator. I took inspiration for the interface to fftpipe from the Tidyverse and tidymodels packages.\nSpecifically, fftpipe offers the following functionality:\n\nWaveform generation,\nFFT and inverse FFT transformation,\nPlotting of these waveforms and FFTs."
  },
  {
    "objectID": "posts/fftpipe/index.html#installation",
    "href": "posts/fftpipe/index.html#installation",
    "title": "R: FFT with fftpipe",
    "section": "Installation",
    "text": "Installation\nInstall fftpipe from GitHub with devtools. If you don’t have devtools installed already, install it with install.packages(\"devtools\"). Once devtools is installed, you can then install fftpipe by typing the following command into the R console:\ndevtools::install_github(\"akey7/fftpipe\")"
  },
  {
    "objectID": "posts/fftpipe/index.html#quick-start",
    "href": "posts/fftpipe/index.html#quick-start",
    "title": "R: FFT with fftpipe",
    "section": "Quick Start",
    "text": "Quick Start\nMore detail about each of these steps is described below in this document. But this will get you started!\n\nLoad the necessary packages\nWhile you can use fftpipe on its own, it is designed to work within the Tidyverse ecosystem, so I recommend you load the tidyverse as well. Also, I will set a theme for ggplot().\n\nsuppressPackageStartupMessages(library(fftpipe))\nsuppressPackageStartupMessages(library(tidyverse))\ntheme_set(theme_linedraw(base_size = 15))\n\n\n\nGenerate a waveform\nWaveforms can either be built from external data or synthesized from within the fftpipe package. For this quick start demo, let’s compose a waveform to feed into the FFT with the following properties:\n\nIts duration will be 1 second with a sample rate of 200 samples/second.\nIt will be the sum of 2 cosines, with the second cosine being 3x the frequency and 50% the amplitude of the first cosine.\nIts total amplitude will decrease with an exponential decay function, with an exponential time constant (tau) of 0.5.\nThe final result will be normalized by dividing the vector of values by the length of that vector to prepare the waveform for FFT.\n\nWe can compose this waveform using functions from fftpipe as shown in this code block:\n\nwv &lt;- waveform(duration_s = 1.0, sr = 100) %&gt;%\n  cos_sum(freqs = c(2.0, 6.0), amplitudes = c(1.0, 0.9)) %&gt;%\n  length_norm()\n\n\n\nPlot the input waveform\nWe can streamline plotting the input waveform with the following function call. This plot can be refined in ways I’ll describe later in the document.\n\nwaveform_plot(wv)\n\n\n\n\n\n\n\n\n\n\nPerform the FFT\nWe can perform an FFT on the waveform we just made. To do that, run the following line:\n\nwv_fft &lt;- wv %&gt;%\n  compute_fft()\n\nWe can then use this FFT data.frame to plot the FFT and reconstruct the original waveform in the next two steps.\n\n\nPlot the FFT\nfft_plot() plots the FFT as shown below. By default, the plot only shows the frequency components that are at or below the Nyquist frequency (half the sample rate). Here, our sample rate is 100 Hz, so the maximum frequency is 50 Hz. This plot can be further customized, which will be shown later in the document.\nNote how our original component cosines we summed together appear here as peaks at 2 Hz and 6 Hz. The first peak is taller than the second peak, which corresponds to our 6 Hz component having less amplitude than the 2 Hz component. Hence, our FFT captured the frequencies of our original waveform!\n\nwv_fft %&gt;%\n  fft_plot()\n\n\n\n\n\n\n\n\n\n\nReconstruct the original waveform from the FFT\nFinally, we can reconstruct the original waveform from the FFT. First, we need to make a waveform that defines the sample rate and values for the waveform to be reconstructed. This should match the waveform() call above. Then we need to feed both the new waveform and FFT into the inverse_FFT(). Then, we can pass the reconstruction through a length_norm() call and plot the reconstructed waveform.\n\nwaveform(duration_s = 1.0, sr = 100) %&gt;%\n  inverse_fft(wv_fft) %&gt;%\n  length_norm() %&gt;%\n  waveform_plot()\n\n\n\n\n\n\n\n\nBy using length_norm() before and after the FFT, the amplitudes of our waveform are the same before and after the FFT."
  },
  {
    "objectID": "posts/fftpipe/index.html#in-depth",
    "href": "posts/fftpipe/index.html#in-depth",
    "title": "R: FFT with fftpipe",
    "section": "In Depth",
    "text": "In Depth\nIn this section, I will go over the capabilities of the fftpipe package in more detail. This section will cover:\n\nfftpipe is built around dataframes.\nHow do you customize plots made with waveform_plot() and fft_plot()?\nHow do you compose a waveform with waveform(), cos_sum(), exp_decay(), white_noise(), and length_norm()?\nHow do you do FFT operations with compute_fft() and inverse_fft(). What options are available for fft_plot()?\nHow do you do signal denoising with denoise_fft()?\n\nYou can read the source code for all the functions on GitHub\n\nfftpipe Is Built Around Dataframes\nfftpipe functions pass data frames to each other with the pipe (%&gt;%) operator. The columns differ depending on if a dataframe represents an FFT or a waveform. A waveform is what you pass into an FFT or get out of an inverse FFT.\nWaveform dataframes contain the following columns:\n\n\n\nColumn\nPurpose\n\n\n\n\n.sample\nAn index of the sample in waveform the row represents.\n\n\n.sec\nt: Time of that row in the waveform.\n\n\n.value\nf(t): Value of the waveform at that time.\n\n\n\nFFT dataframes contain the following columns:\n\n\n\n\n\n\n\nColumn\nPurpose\n\n\n\n\n.idx\nIndex of the coefficient. Used to compute the frequency of each coefficient.\n\n\n.value\nThe complex value of the Fourier coefficient.\n\n\n.psd\nThe power spectral density of that Fourier coefficient.\n\n\n\nBy passing these values around in dataframes, the functions of fftpipe can share data amongst themselves without needing the same information specified repeatedly in code.\n\n\nHow Do You Customize Plots?\nThe plots above showed some information but they were not great. The axes didn’t have descriptive labels and the plots lacked titles. Also, the theme was a bit drab. Let’s fix that!\nThe plots created by waveform_plot() and fft_plot() are simply ggplot objects that can be displayed or saved as you wish. Both waveform_plot() and fft_plot() accept ... arguments which are passed directly to the labs() function. Let’s generate a new waveform and then customize its plot\n(You will see some new functions when we create the waveform; these will be explained later.)\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  exp_decay(decay_tau = 0.5) %&gt;%\n  length_norm()\n\nwv_to_customize %&gt;%\n  waveform_plot()\n\n\n\n\n\n\n\n\nSince waveform_plot() and fft_plot() accept arguments to pass to label(), we can add labels to this plot. In your plots, you don’t need to specify all the labels, but I’ve put them all in here to demonstrate what you can do.\n\nwv_to_customize %&gt;%\n  waveform_plot(\n    title = \"My Favorite Waveform!\",\n    subtitle = \"Let's Customize This Plot.\",\n    caption = \"Generated completely with fftpipe\",\n    x = \"t (s)\",\n    y = \"f(t)\"\n  )\n\n\n\n\n\n\n\n\nSince this is just a ggplot object, we can add themeing:\n\nwv_to_customize %&gt;%\n  waveform_plot(\n    title = \"My Favorite Waveform!\",\n    subtitle = \"Let's Customize This Plot.\",\n    caption = \"Generated completely with fftpipe\",\n    x = \"t (s)\",\n    y = \"f(t)\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.caption = element_text(colour = \"blue\", face = \"bold\")\n  )\n\n\n\n\n\n\n\n\nFor theming, the sky is the limit here. Go for it!\n\n\nHow Do You Compose a Waveform for Testing?\nHow do the functions waveform(), cos_sum(), exp_decay(), white_noise(), and length_norm() work together to make waveforms for testing? Let’s illustrate this with an example where we build operations up to make the example waveform shown in the plots above.\nwaveform() creates the primary attributes of a waveform: a duration in seconds and a sample rate. It also fills the values of the waveform with zero to enable further addition operations. Here, I define a waveform that is one second long with a sample rate of 200 Hz.\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200)\n\nwv_to_customize %&gt;%\n  waveform_plot()\n\n\n\n\n\n\n\n\nNow the waveform needs something. That is what cos_sum() is for. cos_sum() takes vectors of frequencies, amplitudes, and phases of cos() functions to sum into a final waveform. It sums them up and adds them to the incoming waveform which is specified by the first argument to the function:\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  )\n\nwv_to_customize %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nNow we have a waveform we can do something with. But there is still more that we can do!\nWe can add noise to the signal with white_noise(). Speicifcally, this function adds random numbers drawn from a normal distribution into the incoming waveform. It takes two arguments, both of which are optional: mean and sd. Adjusting sd is like adjusting the amplitude of the noise (lower values are less noise).\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  white_noise(mean = 0, sd = 1e-5)\n\nwv_to_customize %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nWe can multiply by an exponential decay to create a waveform whose amplitude diminishes over time with exp_decay(). In addition to an incoming dataframe which specifies a waveform, exp_decay() accepts one other parameter decay_tau which specifies the rate of decay (lower values mean faster decay rate).\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  white_noise(mean = 0, sd = 1e-5) %&gt;%\n  exp_decay(decay_tau = 0.5)\n\nwv_to_customize %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nFinally, it’s good to normalize the waveform after creating it before applying a Fourier transform. A common technique I have seen is to divide each element of the signal’s vector by the length of the vector. This functionality is accomplished by the length_norm() function. Note how the amplitude drops in the following plot compared to the plots above.\n\nwv_to_customize &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  white_noise(mean = 0, sd = 1e-5) %&gt;%\n  exp_decay(decay_tau = 0.5) %&gt;%\n  length_norm()\n\nwv_to_customize %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nAll the steps except the waveform() creation are optional. Mix and match to your heart’s content!\n\n\nHow Do You Do FFT Operations?\ncompute_fft() and inverse_fft() perform fast fouriere transform (FFT) operations and the results can be plotted with fft_plot(). Fourier transforms, Fourier series, and Fourier analysis are HUGE topics that span many areas of math and physics. I won’t go into those applications here. But I will show you how to use these functions as fundamental building blocks as a starting point for more advanced FFT applications.\nLet’s revisit our most recent waveform we generated:\n\nwv_for_fft &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  ) %&gt;%\n  white_noise(mean = 0, sd = 1e-5) %&gt;%\n  exp_decay(decay_tau = 0.5) %&gt;%\n  length_norm()\n\nwv_for_fft %&gt;%\n  waveform_plot(x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\nWe can take and plot the FFT of this waveform simply with compute_fft() (remember to length_norm() before you run an FFT). Here we will take our noisy signal, do an FFT, and make a PSD plpot of the first 100 Fourier coefficients:\n\nour_first_fft &lt;- wv_for_fft %&gt;%\n  compute_fft()\n\nour_first_fft %&gt;%\n  fft_plot(show = \"psd\", x = \"Hz\", y = \"PSD\")\n\n\n\n\n\n\n\n\n(Read below to find out how to remove noise with FFT denoising!)\nSince there is more than just spectral density, we can also plot everything out of the FFT. What we will see is the real, imaginary, modulus of every Fourier coefficient as well as every PSD value.\n\nour_first_fft %&gt;%\n  fft_plot(show = \"everything\")\n\n\n\n\n\n\n\n\nNow let’s go backwards with the inverse FFT to regenerate a waveform from FFT data. We do this with the inverse_fft() function. The inverse_fft() function is somewhat tricky: It takes a data.frame of an incoming FFT as the second argument, and a data.frame of an incoming waveform as the first argument. The waveform is necessary so that the inverse operation has somewhere to put the data; it must be a waveform compatible with the original waveform. Here waveform(duration_s = 1.0, sr = 200) is the same call we started the original waveform with. And, as usual, we will call length_norm() as the last step in producing a waveform.\n\nwv_recreated &lt;- waveform(duration_s = 1.0, sr = 200) %&gt;%\n  inverse_fft(our_first_fft) %&gt;%\n  length_norm()\n\nwv_recreated %&gt;%\n  waveform_plot(title = \"Recreated Waveform\", x = \"t (s)\", y = \"f(t)\")\n\n\n\n\n\n\n\n\n\n\nSignal Denoising\nSignal denoising is a simple way to remove noise from a signal through the following steps:1\n\nFFT the noisy signal\nInspect the power spectral density (PSD) for each Fourier coefficient\nObserve that the coefficients encoding the signal have a higher PSD than the noise coefficients.\nZero the coefficients that do not meet a PSD threshold.\nRun and inverse FFT to reconstruct a waveform with reduced noise.\n\nAll of these steps can be accomplished with functions in fftpipe.\n\nCreate Clean and Noisy Signals\nLet’s make the sample rate higher than we have been using so far so that it can capture the high frequency white noise better. First, let’s make a clean signal and inspect its waveform and PSD plot.\n\nwv &lt;- waveform(duration_s = 1.0, sr = 500) \n\nwv_clean &lt;- wv %&gt;%\n  cos_sum(\n    freqs = c(5, 10, 20), \n    amplitudes = c(1.0, 0.875, 0.75), \n    phases = c(3*pi/2, 0, pi/2)\n  )\n\nwv_clean_fft &lt;- wv_clean %&gt;%\n  length_norm() %&gt;%\n  compute_fft()\n\nCreate the plots of the clean signal:\n\nwv_clean %&gt;%\n  length_norm() %&gt;%\n  waveform_plot(\n    title = \"Clean Signal\",\n    x = \"t (s)\", \n    y = \"f(t)\"\n  )\n\n\n\n\n\n\n\n\n\nwv_clean_fft %&gt;%\n  fft_plot(title = \"PSD of Fourier Coefficients, Clean Signal\", x = \"Hz\", y = \"PSD\")\n\n\n\n\n\n\n\n\nNow let’s add some noise!\nThe white_noise() function adds noise from a Gaussian distribution to the signal.\n\nset.seed(123)\n\nwv_noisy &lt;- wv_clean %&gt;%\n  white_noise(mean = 0, sd = 1e-3) %&gt;%\n    length_norm()\n\nwv_noisy_fft &lt;- wv_noisy %&gt;%\n  compute_fft()\n\nAs before, let’s plot the waveform and the PSD of the first half of Fourier coefficients.\n\nwv_noisy %&gt;%\n  waveform_plot(\n    title = \"Noisy Signal\",\n    x = \"t (s)\", \n    y = \"f(t)\"\n  )\n\n\n\n\n\n\n\n\n\nwv_noisy_fft %&gt;%\n  fft_plot(title = \"PSD of Fourier Coefficients, Noisy Signal\", x = \"Hz\", y = \"PSD\")\n\n\n\n\n\n\n\n\nNote that a bunch of Fourier coefficients have lit up with very small PSD values. When we drop these coefficients to zero, we will recover clean Fourier coefficients. Then, when we do an inverse Fourier transform on the cleaned coefficients, we will recover the signal from the noise.\n\nwv_noisy_but_cleaned &lt;- wv_noisy_fft %&gt;%\n  denoise_fft(psd_thresh = 5e-2) %&gt;%\n  inverse_fft(wv, .) %&gt;%\n  length_norm()\n\nwv_noisy_but_cleaned %&gt;%\n  waveform_plot(\n    title = \"Waveform With Noise Removed\", \n    x = \"t (s)\", \n    y = \"f(t)\"\n  )"
  },
  {
    "objectID": "posts/fftpipe/index.html#additional-resources",
    "href": "posts/fftpipe/index.html#additional-resources",
    "title": "R: FFT with fftpipe",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nBrunton, Steve. Denoising Data with FFT [Python] https://youtu.be/s2K1JfNR7Sc\nNeto, João, Fourier Transform: A R Tutorial http://www.di.fc.ul.pt/~jpn/r/fourier/fourier.html"
  },
  {
    "objectID": "posts/cafe-rewards/index.html",
    "href": "posts/cafe-rewards/index.html",
    "title": "Tableau, SQL: Cafe Rewards Analysis",
    "section": "",
    "text": "Data that simulates the behavior of Cafe Rewards members over a 30-day period, including their transactions and responses to promotional offers.\nCustomers receive offers once every few days and have a limited time to redeem them. These can be informational offers (simple advertisement of a product), discount offers, or “buy one, get one” (BOGO) offers. Each customer receives a different mix of offers, attempting to maximize their probability of making a purchase.\nEvery customer purchase during the period is marked as a transaction. For a transaction to be attributed to an offer, it must occur at the same time as when the offer was “completed” by the customer."
  },
  {
    "objectID": "posts/cafe-rewards/index.html#cafe-rewards-offers",
    "href": "posts/cafe-rewards/index.html#cafe-rewards-offers",
    "title": "Tableau, SQL: Cafe Rewards Analysis",
    "section": "",
    "text": "Data that simulates the behavior of Cafe Rewards members over a 30-day period, including their transactions and responses to promotional offers.\nCustomers receive offers once every few days and have a limited time to redeem them. These can be informational offers (simple advertisement of a product), discount offers, or “buy one, get one” (BOGO) offers. Each customer receives a different mix of offers, attempting to maximize their probability of making a purchase.\nEvery customer purchase during the period is marked as a transaction. For a transaction to be attributed to an offer, it must occur at the same time as when the offer was “completed” by the customer."
  },
  {
    "objectID": "posts/cafe-rewards/index.html#tableau-dashboards",
    "href": "posts/cafe-rewards/index.html#tableau-dashboards",
    "title": "Tableau, SQL: Cafe Rewards Analysis",
    "section": "Tableau Dashboards",
    "text": "Tableau Dashboards\n\nCustomer Dashboard\n\n\n\nRewards Dashboard"
  },
  {
    "objectID": "posts/cafe-rewards/index.html#data-source",
    "href": "posts/cafe-rewards/index.html#data-source",
    "title": "Tableau, SQL: Cafe Rewards Analysis",
    "section": "Data Source",
    "text": "Data Source\nMaven Rewards Challenge"
  },
  {
    "objectID": "posts/cafe-rewards/index.html#description",
    "href": "posts/cafe-rewards/index.html#description",
    "title": "Tableau, SQL: Cafe Rewards Analysis",
    "section": "Description",
    "text": "Description\n\nBusiness need\n\nThe business uses promotions to draw customers to cafes and needs to quantify how well these promotions working by looking at the revenue they drive and the customers they attract.\n\n\n\nGoals\n\nFind the most successful promotions so that similar offerings can be made in the future.\n\n\n\nInsights\n\nThe rewards program is very successful! There were 141.9K transactions, $1.8M in monthly revenue, with 37.3% of that revenue coming from transactions in which a reward was redeemed.\nThe most popular offer was $5 discount on a $20 purchase that could be redeemed up to 10 days after viewing by the customer. The offer was sent via web and email channels. There were 1.4 completions of this reward per customer, indicating that many customers redeemed the offer multiple times.\nAcross BOGO and discount offers, the average age of the customer completing the offer was 56 (compared to an average of 54 across all customers) with an income of $69K-$70K (compared to an average of 64K for all customers).\nTransaction counts appear to spike when rewards are announced throughout the month.\nThe dataset contains a total of 17,000 customers, with an average age of 54 years and average income of $65K.\nMost customers fall within the 50 to 69 year age bracket and $40K-$74K income range. 57.2% of the customers identify as male, 41.3% as female, and 1.4% as other.\nCustomers in the dataset enrolled as part of the rewards program starting in 2013 and ending in 2018.\nMost of the customers in the month spent $5 or less or $20 or more and were seen within the last week.\n\n\n\nRecommendation\n\nTarget customers not seen in 2 weeks with offers of a $5 discount redeemable on purchases of $20 or more.\n\n\n\nA note about “Completions per Viewing Customer”\nUnlike a conversion rate, where an offer can only have one completion, the offers in this dataset could be completed more than once per customer. Certain offers were completed up to 4 times by one customer! So I needed another metric to compare offer performance. “Completions per customer” is the total number of offer completions divided by the count of the distinct customers that viewed the offer. A value of 1 would mean that every customer that viewed the offer completed it once. A value greater than 1 means that some customers completed the offer multiple times, and a value less than 1 means that not all viewing customers completed the offer."
  },
  {
    "objectID": "posts/cafe-rewards/index.html#tools-used",
    "href": "posts/cafe-rewards/index.html#tools-used",
    "title": "Tableau, SQL: Cafe Rewards Analysis",
    "section": "Tools Used",
    "text": "Tools Used\n\nPostgreSQL\nTableau"
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html",
    "href": "posts/quantum-harmonic-oscillator/index.html",
    "title": "Python: Quantum harmonic oscillator",
    "section": "",
    "text": "In this post, I will define Python code that models the quantum harmonic oscillator. This page follows page 290 to 297 in Physical Chemistry, 8th Ed. by Peter Atkins and Julio de Paula for the math to create and examples to test the code in this post.\nThe following equations describe its energy levels:\n\\[ \\omega = \\sqrt{\\frac{k}{m}} \\]\n\\[ E_{\\nu} = \\Bigl(\\nu + \\frac{1}{2}\\Bigr) \\hbar \\omega \\]\n\\[ \\nu = 0, 1, 2, \\dots \\]\nThe following equations describe its wavefunction:\n\\[ \\alpha = \\Biggl(\\frac{\\hbar^2}{mk}\\Biggr)^{1/4} \\]\n\\[ \\gamma = \\frac{x}{\\alpha} \\]\n\\[ \\psi_{\\nu}(x) = N_{\\nu} H_{\\nu}(\\gamma) e^{-\\gamma^{2}/2} \\]\nwhere \\(H\\) is a Hermite polynomial. The form of the first six Hermite polynomials are on page 293 of the text and are also implemented in the QuantumHarmonicOscillator class shown in the Python source code section below. Here is a sampling of the first three:\n\\[ H_0(\\gamma) = 1 \\]\n\\[ H_1(\\gamma) = 2 \\gamma \\]\n\\[ H_2(\\gamma) = 4 \\gamma^2 - 2 \\]\nExample 9.3 on page 294 demonstrates the derivation of the normalization constant \\(N_{}\\), which results in the following expression for the normalization constant:\n\\[ N_{\\nu} = \\sqrt{\\frac{1}{\\alpha \\pi^{1/2} 2^{\\nu} \\nu!}} \\]"
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html#introduction",
    "href": "posts/quantum-harmonic-oscillator/index.html#introduction",
    "title": "Python: Quantum harmonic oscillator",
    "section": "",
    "text": "In this post, I will define Python code that models the quantum harmonic oscillator. This page follows page 290 to 297 in Physical Chemistry, 8th Ed. by Peter Atkins and Julio de Paula for the math to create and examples to test the code in this post.\nThe following equations describe its energy levels:\n\\[ \\omega = \\sqrt{\\frac{k}{m}} \\]\n\\[ E_{\\nu} = \\Bigl(\\nu + \\frac{1}{2}\\Bigr) \\hbar \\omega \\]\n\\[ \\nu = 0, 1, 2, \\dots \\]\nThe following equations describe its wavefunction:\n\\[ \\alpha = \\Biggl(\\frac{\\hbar^2}{mk}\\Biggr)^{1/4} \\]\n\\[ \\gamma = \\frac{x}{\\alpha} \\]\n\\[ \\psi_{\\nu}(x) = N_{\\nu} H_{\\nu}(\\gamma) e^{-\\gamma^{2}/2} \\]\nwhere \\(H\\) is a Hermite polynomial. The form of the first six Hermite polynomials are on page 293 of the text and are also implemented in the QuantumHarmonicOscillator class shown in the Python source code section below. Here is a sampling of the first three:\n\\[ H_0(\\gamma) = 1 \\]\n\\[ H_1(\\gamma) = 2 \\gamma \\]\n\\[ H_2(\\gamma) = 4 \\gamma^2 - 2 \\]\nExample 9.3 on page 294 demonstrates the derivation of the normalization constant \\(N_{}\\), which results in the following expression for the normalization constant:\n\\[ N_{\\nu} = \\sqrt{\\frac{1}{\\alpha \\pi^{1/2} 2^{\\nu} \\nu!}} \\]"
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html#the-python-class-for-the-harmonic-oscillator",
    "href": "posts/quantum-harmonic-oscillator/index.html#the-python-class-for-the-harmonic-oscillator",
    "title": "Python: Quantum harmonic oscillator",
    "section": "The Python class for the harmonic oscillator",
    "text": "The Python class for the harmonic oscillator\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.integrate import quad\nfrom math import pi, sqrt, exp, factorial\n\n\nclass QuantumHarmonicOscillator:\n    \"\"\"\n    This models a harmonic oscillator. Parameters used throughout the methods\n    are:\n\n    v: The quantum of the harmonic oscillator\n    k: The force constant\n    mass_r: The reduced mass of the system. If you need to calculate this for a \n            diatomic molecule, see the diatomic_reduced_mass() method below.\n\n    The nomenclature of variable names follows Atkins and de Paula Physical\n    Chemistry 8th ed, pp. 290-297.\n\n    Atkins, P. W. & De Paula, J. Atkins’ Physical chemistry. (W.H. Freeman, 2006).\n    \"\"\"\n\n    def __init__(self, mass_r, k):\n        \"\"\"\n        Setup the values used by all methods in this model.\n\n        Parameters\n        ----------\n        mass: float\n            The reduced mass of the system, in kg\n\n        k: float\n            The force constant, in N/m\n        \"\"\"\n        self.hbar = 1.054571817e-34\n        self.k = k\n        self.mass_r = mass_r\n        self.omega = sqrt(k / mass_r)\n        self.max_nu = 6\n\n    @staticmethod\n    def diatomic_reduced_mass(mass_1, mass_2):\n        \"\"\"\n        Computes the reduced mass of a diatomic system. In order to provide\n        meaningful results, please use the exact isotopic mass for your molecule.\n\n        Parameters\n        ----------\n        mass_1: float\n            The mass of the first atom in kg.\n\n        mass_2: float\n            The mass of the second atom in kg.\n        \"\"\"\n        return mass_1*mass_2/(mass_1+mass_2)\n\n    def hermite(self, v, gamma):\n        \"\"\"\n        Returns the value of the v-th (nth) Hermite polynomial evaluated on gamma\n\n        The v and gamma notation follows Atkins' Physical Chemistry 8th ed.\n\n        Parameters\n        ----------\n        v: int\n            The v-th (nth) Hermite polynomial\n\n        gamma: float\n            The value to calculate with the Hermite polynomial\n\n        Returns\n        -------\n        float\n            The value of the v-th Hermite polynomial evaluated with gamma.\n\n        Raises\n        ------\n        Exception\n            Raises an exception if the nth Hermite polynomial is not\n            supported.\n        \"\"\"\n        if v == 0:\n            return 1\n        elif v == 1:\n            return 2 * gamma\n        elif v == 2:\n            return 4 * gamma**2 - 2\n        elif v == 3:\n            return 8 * gamma**3 - 12 * gamma\n        elif v == 4:\n            return 16 * gamma**4 - 48 * gamma**2 + 12\n        elif v == 5:\n            return 32 * gamma**5 - 160 * gamma**3 + 120 * gamma\n        elif v == 6:\n            return 64 * gamma**6 - 480 * gamma**4 + 720 * gamma**2 - 120\n        else:\n            raise Exception(f\"Hermite polynomial {v} is not supported\")\n\n    def max_v(self):\n        \"\"\"\n        Returns\n        -------\n        int\n            The maximum v value for the instance.\n        \"\"\"\n        return 6\n\n    def energy(self, v):\n        \"\"\"\n        Calculate the energy at the given level v of the system\n\n        Parameters\n        ----------\n        v: int\n            The quantum vmber v for the energy level of this system\n\n        Returns\n        -------\n        float\n            Energy of the system in Joules.\n        \"\"\"\n        return (v + 0.5) * self.hbar * self.omega\n\n    def energy_separation(self):\n        \"\"\"\n        Returns\n        -------\n        float\n            The energy difference between adjacent energy levels in Joules.\n        \"\"\"\n        return self.hbar * self.omega\n\n    def wavefunction(self, v, x):\n        \"\"\"\n        Returns the value of the wavefunction at energy level v\n        at coordinate x.\n\n        Parameters\n        ----------\n        v: float\n            Energy level of the system.\n\n        x: float\n            x coordinate of the particle in m.\n\n        Returns\n        -------\n        float\n            Value of the wavefunction v at x.\n        \"\"\"\n        alpha = (self.hbar**2 / self.mass_r / self.k) ** 0.25\n        gamma = x / alpha\n        normalization = sqrt(1 / (alpha * sqrt(pi) * 2**v * factorial(v)))\n        gaussian = exp((-(gamma**2)) / 2)\n        hermite = self.hermite(v, gamma)\n        return normalization * hermite * gaussian\n\n    def wavefunction_across_range(self, v, x_min, x_max, points=100):\n        \"\"\"\n        Calculates the wavefunction across a range.\n\n        Parameters\n        ----------\n        v: int\n            The quantum vmber of the system.\n\n        x_min: float\n            The minimum x value to calculate.\n\n        x_max: float\n            The maximum x value to calculate.\n\n        points: int\n            The vmber of points across the range\n\n        Returns\n        -------\n        np.array, list\n            The first array are the x coordinates, the second list are the\n            float values of the wavefunction.\n        \"\"\"\n        xs = np.linspace(x_min, x_max, points)\n        ys = [self.wavefunction(v, x) for x in xs]\n        return xs, ys\n\n    def prob_density(self, v, x_min, x_max, points=100):\n        \"\"\"\n        Returns the probability density between x_min and x_max for a given\n        vmber of points at energy level v.\n\n        Parameters\n        ----------\n        v: int\n            Quantum vmber of the system.\n\n        x_min: float\n            Minimum of length being calculated. Probably negative. Units are\n            meters.\n\n        x_max: float\n            Maximum of length being calculated. Probably positive. Units are\n            meters.\n\n        points: int\n            The vmber of points to compute the probability density for\n\n        Returns\n        -------\n        np.array, list\n            The first array is the list of x coordinates. The list are the\n            corresponding values of the probability density.\n        \"\"\"\n        xs = np.linspace(x_min, x_max, points)\n        ys = [self.wavefunction(v, x) ** 2 for x in xs]\n        return xs, ys\n\n    def integrate_prob_density_between_limits(self, v, x_min, x_max):\n        \"\"\"\n        As a way of testing the methods in this class, provide a way to\n        integrate across the wavefunction squared between limits.\n\n        Parameters\n        ----------\n        v: int\n            Quantum vmber of the system.\n\n        x_min: float\n            lower bound of integration\n\n        x_max: float\n            upper bound of integration\n        \"\"\"\n\n        def integrand(x):\n            return self.wavefunction(v, x) ** 2\n\n        result, _ = quad(integrand, x_min, x_max)\n        return result"
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html#test-the-class-with-1h35cl-properties",
    "href": "posts/quantum-harmonic-oscillator/index.html#test-the-class-with-1h35cl-properties",
    "title": "Python: Quantum harmonic oscillator",
    "section": "Test the class with 1H35Cl Properties",
    "text": "Test the class with 1H35Cl Properties\nThe 1H35Cl molecule has a bond with the following physical properties (the mass is of the proton in Hydrogen).\n\\[ k = 516.3 N/m \\]\n\\[ m = 1.7 \\times 10^{-27} kg \\]\n\nk = 516.3\nmass = 1.7e-27\nx_min = -0.5e-10\nx_max = 0.5e-10\npoints = 1000\nqho = QuantumHarmonicOscillator(k=k, mass_r=mass)\nnus = range(qho.max_nu + 1)\nfig, axs = plt.subplots(nrows=len(nus), ncols=2, figsize=(10, len(nus) * 5), sharex=True)\nfor idx, nu in enumerate(nus):\n    xs_wavefunction, ys_wavefunction = qho.wavefunction_across_range(nu, x_min, x_max, points)\n    xs_prob_density, ys_prob_density = qho.prob_density(nu, x_min, x_max, points)\n    integrated = qho.integrate_prob_density_between_limits(nu, x_min, x_max)\n    axs[nu, 0].set_title(f'ν={nu}', size=15, color='r')\n    axs[nu, 0].set_ylabel(f'Ψ{nu}(x)', size=15, color='b')\n    axs[nu, 0].set_xlabel('x', size=15)\n    axs[nu, 0].set_xlim(x_min, x_max)\n    axs[nu, 1].set_title(f'ν={nu}, ∫Ψ{nu}^2={round(integrated)}', size=15, color='r')\n    axs[nu, 1].set_ylabel(f'Ψ{nu}(x)^2', size=15, color='b')\n    axs[nu, 1].set_xlabel('x', size=15)\n    axs[nu, 1].set_xlim(x_min, x_max)\n    axs[nu, 0].plot(xs_wavefunction, ys_wavefunction)\n    axs[nu, 1].plot(xs_prob_density, ys_prob_density, color='g')\n\nplt.show()\n\n\n\n\n\n\n\n\nIn the plots of Figure 1, there are two columns. The left column is a plot of wavefunctions at different nu levels, each with a title indicating the level of the plot. The right column has plots of the squares of the wavefunctions. Like the left column plots, the titles of these plots contain the nu value that created them. In addition, the titles include an integral in their titles. These integrals all have a value of one, meaning each square of the wavefunction displays the probability throughout the entire space of wavefunction."
  },
  {
    "objectID": "posts/quantum-harmonic-oscillator/index.html#testing-the-energy-level-calculation-from-illustration-9.3",
    "href": "posts/quantum-harmonic-oscillator/index.html#testing-the-energy-level-calculation-from-illustration-9.3",
    "title": "Python: Quantum harmonic oscillator",
    "section": "Testing the energy level calculation from Illustration 9.3",
    "text": "Testing the energy level calculation from Illustration 9.3\nUsing the 1H35Cl specifications from above, I will calculate the zero point energy and energy separation following Atkins and de Paula, Illustration 9.3. The 1H35Cl specifications are:\n\\[ k = 516.3 N/m \\]\n\\[ m = 1.7 \\times 10^{-27} kg \\]\nHowever, the book uses k=500 N/m and I will make that assumption to maintain consistency.\n\nk = 500\nmass = 1.7e-27\nmol = 6.022e23\nqho = QuantumHarmonicOscillator(k=k, mass_r=mass)\nenergy_sep_kj_mol = qho.energy_separation() * mol / 1000  # Convert to kJ/mol\nprint(f'energy sepration between ν and ν+1, {energy_sep_kj_mol} kJ/mol')\nzero_point_kj_mol = qho.energy(v=0) * mol / 1000\nprint(f'zero-point energy {zero_point_kj_mol} kJ/mol')\nfirst_excitation_thz = (qho.energy(v=1) - qho.energy(v=0)) / 6.626e-34 / 1e12\nprint(f'First excitation energy {first_excitation_thz} THz')\n\nenergy sepration between ν and ν+1, 34.44113487055476 kJ/mol\nzero-point energy 17.22056743527738 kJ/mol\nFirst excitation energy 86.31480043180733 THz\n\n\nFrom the answers given in the book, this gives my model the following differences.\n\ndelta_energy_sep = (round(energy_sep_kj_mol) / 34.0 - 1) * 100\nprint(f'delta energy separation {delta_energy_sep}%')\ndelta_zero_point = (round(zero_point_kj_mol) / 15.0 - 1) * 100\nprint(f'delta zero point energy {delta_zero_point}%')\ndelta_first_excitation = (round(first_excitation_thz) / 86.0 - 1) * 100\nprint(f'delta first excitation {delta_first_excitation}%')\n\ndelta energy separation 0.0%\ndelta zero point energy 13.33333333333333%\ndelta first excitation 0.0%"
  },
  {
    "objectID": "posts/metropolis_basic/index.html",
    "href": "posts/metropolis_basic/index.html",
    "title": "Julia: Metropolis Algorithm",
    "section": "",
    "text": "Draw random samples from a distribution with the classic Metropolis algorithm."
  },
  {
    "objectID": "posts/metropolis_basic/index.html#define-px-and-fx",
    "href": "posts/metropolis_basic/index.html#define-px-and-fx",
    "title": "Julia: Metropolis Algorithm",
    "section": "Define p(x) and f(x)",
    "text": "Define p(x) and f(x)\nAssuming:\n\\[ p(x) = {f(x) \\over NC} \\]\nWhere NC is a normalizing constant that is unknown.\n\nDefine the function\n\nfunction f(xs)\n    μ1 = 1\n    σ1 = 1\n    dist1 = Normal(μ1, σ1)\n\n    μ2 = -2.0\n    σ2 = 0.5\n    dist2 = Normal(μ2, σ2)\n\n    μ3 = -5.0\n    σ3 = 1.5\n    dist3 = Normal(μ3, σ3)\n\n    pdf.(dist1, xs) + pdf.(dist2, xs).*0.25 + pdf.(dist3, xs)\nend;  # ; suppresses output in the Quarto document.\n\n\n\nSet domain of p(x) for subsequent code\n\ndomain_min = -11\ndomain_max = 5\n\n5\n\n\n\n\nPlot f(x)\n\nxs = range(start=domain_min, stop=domain_max, length=200)\nxticks = range(start=domain_min, stop=domain_max, length=5)\nx_tick_labels = [@sprintf(\"%.2f\", x) for x in xticks]\nys = f(xs)\n\nplot(\n    xs, \n    ys, \n    xticks=(xticks, x_tick_labels), \n    xlims=(domain_min, domain_max*1.1), \n    linewidth=3, \n    xlabel=\"x\", \n    ylabel=\"f(x)\", \n    title=\"f(x)\", \n    legend=false,\n    titlefont=font(18)\n)"
  },
  {
    "objectID": "posts/metropolis_basic/index.html#sample-fx",
    "href": "posts/metropolis_basic/index.html#sample-fx",
    "title": "Julia: Metropolis Algorithm",
    "section": "Sample f(x)",
    "text": "Sample f(x)\n\nDraw samples with Metropolis algorithm\n\nnum_steps = 1000000\nσ_step = 1.0\n\nsamples = zeros(Float64, num_steps)\nsamples[1] = rand(Uniform(domain_min, domain_max))\n\nfor i ∈ 2:num_steps\n    next_proposal = rand(Normal(samples[i-1], σ_step))\n    samples[i] = min(1, f(next_proposal) / f(samples[i-1])) &gt; rand() ? next_proposal : samples[i-1]\nend\n\n\n\nPlot histogram of the samples\n\nhistogram(\n    samples,\n    bins=200,\n    normalize=true,\n    xlabel=\"x\", \n    ylabel=\"Probability Density\", \n    title=\"Samples From f(x)\", \n    legend=false,\n    titlefont=font(18),\n    linecolor=:white,\n    fillcolor=\"#EFCB68\"\n)"
  },
  {
    "objectID": "posts/positive-and-negative-definite-matrices/index.html",
    "href": "posts/positive-and-negative-definite-matrices/index.html",
    "title": "Julia: Positive and Negative Definite Matrices",
    "section": "",
    "text": "These functions implement the expression, which can calculate scalar quantities when given x1 and x2 components of a 2-element vector.\n\\[ f(\\mathbf x) = \\mathbf x^T \\mathbf A \\mathbf x \\]\nIn this case, I take the matrix A to positive definite or negative definite. Then, I pass the same vectors into the function to see the behavior of positive or negative definite matrices.\n\n\nThe 2x2 identity matrix is positive definite:\n\\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{bmatrix}\n\\]\n\nfunction pos_definite(x1::Float64, x2::Float64)\n    A = [1.0 0.0; 0.0 1.0]\n    x = [x1; x2]\n    x' * A * x\nend\n\npos_definite (generic function with 1 method)\n\n\n\n\n\nThe following 2x2 matrix is negative definite:\n\\[\n\\begin{bmatrix}\n-1 & 0 \\\\\n0 & -2 \\\\\n\\end{bmatrix}\n\\]\n\nfunction neg_definite(x1::Float64, x2::Float64)\n    A = [-1.0 0.0; 0.0 -2.0]\n    x = [x1; x2]\n    x' * A * x\nend\n\nneg_definite (generic function with 1 method)"
  },
  {
    "objectID": "posts/positive-and-negative-definite-matrices/index.html#functions-to-create-surfaces",
    "href": "posts/positive-and-negative-definite-matrices/index.html#functions-to-create-surfaces",
    "title": "Julia: Positive and Negative Definite Matrices",
    "section": "",
    "text": "These functions implement the expression, which can calculate scalar quantities when given x1 and x2 components of a 2-element vector.\n\\[ f(\\mathbf x) = \\mathbf x^T \\mathbf A \\mathbf x \\]\nIn this case, I take the matrix A to positive definite or negative definite. Then, I pass the same vectors into the function to see the behavior of positive or negative definite matrices.\n\n\nThe 2x2 identity matrix is positive definite:\n\\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{bmatrix}\n\\]\n\nfunction pos_definite(x1::Float64, x2::Float64)\n    A = [1.0 0.0; 0.0 1.0]\n    x = [x1; x2]\n    x' * A * x\nend\n\npos_definite (generic function with 1 method)\n\n\n\n\n\nThe following 2x2 matrix is negative definite:\n\\[\n\\begin{bmatrix}\n-1 & 0 \\\\\n0 & -2 \\\\\n\\end{bmatrix}\n\\]\n\nfunction neg_definite(x1::Float64, x2::Float64)\n    A = [-1.0 0.0; 0.0 -2.0]\n    x = [x1; x2]\n    x' * A * x\nend\n\nneg_definite (generic function with 1 method)"
  },
  {
    "objectID": "posts/positive-and-negative-definite-matrices/index.html#surface-plots",
    "href": "posts/positive-and-negative-definite-matrices/index.html#surface-plots",
    "title": "Julia: Positive and Negative Definite Matrices",
    "section": "Surface plots",
    "text": "Surface plots\n\nPositive definite plot\n\nxs = -5:0.1:5\nys = -5:0.1:5\n\nzs = [pos_definite(x, y) for x in xs, y in ys]\n\ndisplay(surface(xs, ys, zs, title=\"Positive Definite Matrix Transform\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\", cmap=:cool))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegative definite plot\n\nxs = -5:0.1:5\nys = -5:0.1:5\n\nzs = [neg_definite(x, y) for x in xs, y in ys]\n\ndisplay(surface(xs, ys, zs, title=\"Negative Definite Matrix Transform\", xlabel=\"x\", ylabel=\"y\", zlabel=\"z\", cmap=:cool))"
  },
  {
    "objectID": "posts/particle-in-box/index.html",
    "href": "posts/particle-in-box/index.html",
    "title": "Python: Quantum mechanics: particle in a box",
    "section": "",
    "text": "This post explores the simplest quatnum mechanical system: the particle in a one-dimensioanl infinite potential well, also known as the one-dimensional particle in a box.\n\\[ \\psi_{n}(x) = \\sqrt{\\frac{2}{L}} \\sin\\Bigl(\\frac{n \\pi x}{L}\\Bigr) \\]\n\\[ E_n = U_0 + \\frac{n^2 h^2}{8 m L^2} \\]\n\\[ n = 1, 2, 3, \\dots \\]"
  },
  {
    "objectID": "posts/particle-in-box/index.html#particle-in-a-one-dimensional-box",
    "href": "posts/particle-in-box/index.html#particle-in-a-one-dimensional-box",
    "title": "Python: Quantum mechanics: particle in a box",
    "section": "Particle in a one-dimensional box",
    "text": "Particle in a one-dimensional box\nThis Python code on this page explores Section 11.3.1 in Molecular Modeling for Beginners, 2nd Ed. by Alan Hinchliffe. In this Section, the author described the particle in a one-dimensional infinite well.\nIt has the following equations to describe its behavior:\n\\[ \\psi_{n}(x) = \\sqrt{\\frac{2}{L}} \\sin\\Bigl(\\frac{n \\pi x}{L}\\Bigr) \\]\n\\[ E_n = U_0 + \\frac{n^2 h^2}{8 m L^2} \\]\n\\[ n = 1, 2, 3, \\dots \\]\n\\[ 0 \\leqslant x \\leqslant L \\]\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nfrom math import sin, sqrt, pi\n\nclass ParticleInABox1d:\n    \"\"\"\n    This class models a one-dimensional particle in a box.\n    \"\"\"\n    \n    def __init__(self, mass, length):\n        \"\"\"\n        Units for mass and length can be in any units you wish,\n        as long as they are consistent.\n        \n        Parameters\n        ----------\n        mass: float\n            The mass of the particle.\n            \n        length: float\n            Length of the one-dimensional box.\n        \"\"\"\n        self.mass = mass\n        self.length = length\n        \n    def wavefunction(self, n, x):\n        \"\"\"\n        Returns the value of the wavefunction with quantum number n\n        at position x.\n        \n        Parameters\n        ----------\n        n: int\n            Quantum number of the box.\n        \n        x: float\n            Postion in the box to evaluate. 0 &lt;= x &lt;= self.length\n            \n        Returns\n        -------\n        float\n            The value of the wavefunction at that point.\n        \"\"\"\n        return sqrt(2 / self.length) * sin(n * pi * x / self.length)\n    \n    def prob_density(self, n, points=100):\n        \"\"\"\n        Returns the probability density at the given number of points\n        along the length of the box.\n        \n        Parameters\n        ----------\n        n: int\n            The quantum number.\n        \n        points: int\n            The number of points to sample along the length of the\n            box.\n            \n        Returns\n        -------\n        np.array, list\n            The first array are the x coordinates along the box, and the second\n            array is the probability density at that point.\n        \"\"\"\n        xs = np.linspace(0, self.length, points)\n        ys = [self.wavefunction(n, x) ** 2 for x in xs]\n        return xs, ys\n\n\nbox = ParticleInABox1d(1, 1)\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(10, 15), sharex=True, sharey=True)\n\nxs, ys = box.prob_density(n=1)\naxs[0, 0].set_title('n = 1', size=20, color='r')\naxs[0, 0].set_ylabel('Ψ1(x)^2', size=15, color='b')\naxs[0, 0].set_xlabel('x', size=15)\naxs[0, 0].set_xlim(0, box.length)\naxs[0, 0].set_ylim(0, max(ys) * 1.05)\naxs[0, 0].plot(xs, ys)\n\nxs, ys = box.prob_density(n=2)\naxs[0, 1].set_title('n = 2', size=20, color='r')\naxs[0, 1].set_ylabel('Ψ2(x)^2', size=15, color='b')\naxs[0, 1].set_xlabel('x', size=15)\naxs[0, 1].set_xlim(0, box.length)\naxs[0, 1].set_ylim(0, max(ys) * 1.05)\naxs[0, 1].plot(xs, ys)\n\nxs, ys = box.prob_density(n=3)\naxs[1, 0].set_title('n = 3', size=20, color='r')\naxs[1, 0].set_ylabel('Ψ3(x)^2', size=15, color='b')\naxs[1, 0].set_xlabel('x', size=15)\naxs[1, 0].set_xlim(0, box.length)\naxs[1, 0].set_ylim(0, max(ys) * 1.05)\naxs[1, 0].plot(xs, ys)\n\nxs, ys = box.prob_density(n=4)\naxs[1, 1].set_title('n = 4', size=20, color='r')\naxs[1, 1].set_ylabel('Ψ4(x)^2', size=15, color='b')\naxs[1, 1].set_xlabel('x', size=15)\naxs[1, 1].set_xlim(0, box.length)\naxs[1, 1].set_ylim(0, max(ys) * 1.05)\naxs[1, 1].plot(xs, ys)\n\nxs, ys = box.prob_density(n=8)\naxs[2, 0].set_title('n = 8', size=20, color='r')\naxs[2, 0].set_ylabel('Ψ8(x)^2', size=15, color='b')\naxs[2, 0].set_xlabel('x', size=15)\naxs[2, 0].set_xlim(0, box.length)\naxs[2, 0].set_ylim(0, max(ys) * 1.05)\naxs[2, 0].plot(xs, ys)\n\nxs, ys = box.prob_density(n=16)\naxs[2, 1].set_title('n = 16', size=20, color='r')\naxs[2, 1].set_ylabel('Ψ16(x)^2', size=15, color='b')\naxs[2, 1].set_xlabel('x', size=15)\naxs[2, 1].set_xlim(0, box.length)\naxs[2, 1].set_ylim(0, max(ys) * 1.05)\naxs[2, 1].plot(xs, ys)\n\n\n\n\n\n\n\n\nThe plots in Figure 1 show the probability density for increasing values of the quantum number n. The greater the curve’s height, the higher the probability of finding the particle in that region of the box. At the lowest quantum number n=1, the particle is most likely to be found in an area surrounding the middle of the box. As n increases, the particle is more likely to be found in more positions of the box. The curve at n=16 demonstrates the important correspondence principle. The correspondence principle says that as quantum number increases, the particle position predicted by quantum mechanics approaches the position indicated by classical mechanics."
  },
  {
    "objectID": "posts/particle-in-box/index.html#particle-in-a-two-dimensional-box",
    "href": "posts/particle-in-box/index.html#particle-in-a-two-dimensional-box",
    "title": "Python: Quantum mechanics: particle in a box",
    "section": "Particle in a two-dimensional box",
    "text": "Particle in a two-dimensional box\nNote: The two-dimensional particle in a box should not be confused with two non-interacting particles in a one-dimensional box. They are completely different concepts.\nHere, I follow Hinchliffe Section 11.5\nThe particle in a two-dimensional box extends the particle in a box concept to two dimensions. It still contains a single particle; however, this particle lies on a plane surrounded by walls of infinite potential. It uses quantum numbers n and k for the levels along the x-axis and y-axis, respectively (some authors use “nx” and “ny” for these numbers).\nI will simplfy the equations by assuming the side of the box are both equal to length L\n\\[ E_{n,k} = U_0 + \\frac{n^2 h^2}{8 m L^2} + \\frac{k^2 h^2}{8 m L^2} \\]\n\\[ \\psi_{n,k}(x, y) = \\frac{2}{L} \\sin\\Bigl(\\frac{n \\pi x}{L}\\Bigr) \\sin\\Bigl(\\frac{k \\pi y}{L}\\Bigr) \\]\n\\[ n, k = 1, 2, 3, \\dots \\]\n\\[ 0 \\leqslant x \\leqslant L \\]\n\\[ 0 \\leqslant y \\leqslant L \\]\n\nclass ParticleInABox2d:\n    def __init__(self, mass=1, length=1):\n        \"\"\"\n        Parameters\n        ----------\n        mass: float\n            The mass of the particle.\n        \n        length:\n            Legnth of each side of the box in x and y dimensions.\n        \"\"\"\n        self.mass = mass\n        self.length = length\n        \n    def wavefunction(self, n, k, x, y):\n        \"\"\"\n        Parameters\n        ----------\n        n: int\n            Quantum number in the x dimension\n            \n        k: int\n            Quantum number in the y dimension\n            \n        x: float\n            x position in the box. Range from zero to self.length\n            \n        y: float\n            y position in the box. Range from zero to self.length\n            \n        Returns\n        -------\n        float\n            The value of the wavefunction n, k at x, y\n        \"\"\"\n        return (2 / self.length) * sin(n * pi * x / self.length) * sin(k * pi * y / self.length)\n    \n    def prob_density(self, n, k, points=100):\n        \"\"\"\n        Parameters\n        ----------\n        n: int\n            Quantum number in the x dimension\n        \n        k: int\n            Quantum number in the y dimension\n            \n        Returns\n        -------\n        np.array\n            Float 2d numpy array of squares of wavefunctions at the various\n            points.\n        \"\"\"\n        xs = np.linspace(0.0, self.length, points)\n        ys = np.linspace(0.0, self.length, points)\n        zs = np.zeros((points, points), np.float64)\n        for ix, x in enumerate(xs):\n            for iy, y in enumerate(ys):\n                zs[ix, iy] = self.wavefunction(n, k, x, y) ** 2\n        return xs, ys, zs\n\n\nbox_2d = ParticleInABox2d(mass=1.0, length=1.0)\nns = [1, 2, 3]\nks = [1, 2, 3]\nfig, axs = plt.subplots(nrows=len(ns), ncols=len(ks), figsize=(15, 15), subplot_kw={\"projection\": \"3d\"})\n\nfor idx_n, n in enumerate(ns):\n    for idx_k, k in enumerate(ks):\n        xs, ys, zs = box_2d.prob_density(n, k)\n        xs, ys = np.meshgrid(xs, ys)\n        ax = axs[idx_n, idx_k]\n        ax.set_title(f'n={n}, k={k}', size=20, color='r')\n        ax.set_zlabel(f'Ψ{n},{k}(x, y)^2', size=15, color='b')\n        ax.set_xlabel('x', size=15)\n        ax.set_ylabel('y', size=15)\n        ax.plot_surface(ys, xs, zs, cmap=cm.RdYlBu, linewidth=0, antialiased=True)\n\n\n\n\n\n\n\n\n\nbox_2d = ParticleInABox2d(mass=1.0, length=1.0)\nns = [1, 2, 3]\nks = [1, 2, 3]\nfig, axs = plt.subplots(nrows=len(ns), ncols=len(ks), figsize=(15, 15), sharex=True, sharey=True)\n\nfor idx_n, n in enumerate(ns):\n    for idx_k, k in enumerate(ks):\n        xs, ys, zs = box_2d.prob_density(n, k)\n        xs, ys = np.meshgrid(xs, ys)\n        ax = axs[idx_n, idx_k]\n        ax.set_title(f'Ψ{n},{k}(x, y)^2', size=15, color='b')\n        ax.set_xlabel('x', size=15)\n        ax.set_ylabel('y', size=15)\n        cs = ax.contour(ys, xs, zs, cmap=cm.RdYlBu, antialiased=True)\n        ax.clabel(cs, inline=True, fontsize=10)\n\n\n\n\n\n\n\n\nIn Figures 2 and 3, the height of the curves, or the higher values of the innermost contour line, show a greater probability that the particle will live in the corresponding region of the box. As in the one-dimensional case, we see that for higher values of n and k, there are more peaks in the corresponding axes of the two-dimensional box."
  },
  {
    "objectID": "posts/connected_reversible_linear/index.html",
    "href": "posts/connected_reversible_linear/index.html",
    "title": "Connected Reversible Linear Reaction Solver",
    "section": "",
    "text": "This system is taken from 4.4. Connected Reversible Linear Reactions in [1]. I have rewritten the chemical equation to simplify the notation of the fluxes over the reactions.\n\\[ x_1 \\underset{v_2}{\\stackrel{v_1}{\\rightleftharpoons}} x_2 \\stackrel{v_3} \\rightarrow x_3 \\underset{v_5}{\\stackrel{v_4}{\\rightleftharpoons}} x_4 \\]\n\\[ \\mathbf S = \\begin{pmatrix} {-1} & {1} & {0} & {0} & {0}  \\\\ {1} & {-1} & {-1} & {0} & {0}  \\\\ {0} & {0} & {1} & {-1} & {1}  \\\\  {0} & {0} & {0} & {1} & {-1}  \\\\ \\end{pmatrix} \\]\n\\[ \\mathbf v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ v_4 \\\\ v_5 \\end{pmatrix} \\]\n\\[ \\mathbf{Sv} = \\mathbf 0 \\]"
  },
  {
    "objectID": "posts/connected_reversible_linear/index.html#what-is-being-modeled",
    "href": "posts/connected_reversible_linear/index.html#what-is-being-modeled",
    "title": "Connected Reversible Linear Reaction Solver",
    "section": "",
    "text": "This system is taken from 4.4. Connected Reversible Linear Reactions in [1]. I have rewritten the chemical equation to simplify the notation of the fluxes over the reactions.\n\\[ x_1 \\underset{v_2}{\\stackrel{v_1}{\\rightleftharpoons}} x_2 \\stackrel{v_3} \\rightarrow x_3 \\underset{v_5}{\\stackrel{v_4}{\\rightleftharpoons}} x_4 \\]\n\\[ \\mathbf S = \\begin{pmatrix} {-1} & {1} & {0} & {0} & {0}  \\\\ {1} & {-1} & {-1} & {0} & {0}  \\\\ {0} & {0} & {1} & {-1} & {1}  \\\\  {0} & {0} & {0} & {1} & {-1}  \\\\ \\end{pmatrix} \\]\n\\[ \\mathbf v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\\\ v_4 \\\\ v_5 \\end{pmatrix} \\]\n\\[ \\mathbf{Sv} = \\mathbf 0 \\]"
  },
  {
    "objectID": "posts/connected_reversible_linear/index.html#steady-state-fluxes",
    "href": "posts/connected_reversible_linear/index.html#steady-state-fluxes",
    "title": "Connected Reversible Linear Reaction Solver",
    "section": "Steady state fluxes",
    "text": "Steady state fluxes\nThis demo will find steady state fluxes using JuMP.jl [2] with a GLPK solver.\n\nCreate the model\n\n# Use JuMP.jl with GLPK\nmodel = Model(GLPK.Optimizer)\n\n# Define flux variables and their constraints\n@variable(model, 0.0 &lt;= v1 &lt;= 1.0)\n@variable(model, 0.0 &lt;= v2 &lt;= 1.0)\n@variable(model, 0.0 &lt;= v3 &lt;= 1.0)\n@variable(model, 0.0 &lt;= v4 &lt;= 1.0)\n@variable(model, 0.0 &lt;= v5 &lt;= 1.0)\n\n# Steady state constraints, Sv=0\n@constraint(model, -v1 + v2 == 0)\n@constraint(model, v1 - v2 - v3 == 0)\n@constraint(model, v3 - v4 + v5 == 0)\n@constraint(model, v4 - v5 == 0)\n\n# Set objective function to maximize\n@objective(model, Max, v4)\n\n# Solve the model\noptimize!(model)\n\n\n\nResults of optimization\nPrint diagnostic information.\n\nprintln(\"Was solution feasible? \", is_solved_and_feasible(model))\n\nWas solution feasible? true\n\n\nWhat was the solution?\n\nprintln(\"Optimal v1: \", value(v1))\nprintln(\"Optimal v2: \", value(v2))\nprintln(\"Optimal v3: \", value(v3))\nprintln(\"Optimal v4: \", value(v4))\nprintln(\"Optimal v5: \", value(v5))\nprintln(\"Optimal objective value: \", objective_value(model))\n\nOptimal v1: 0.0\nOptimal v2: 0.0\nOptimal v3: 0.0\nOptimal v4: 1.0\nOptimal v5: 1.0\nOptimal objective value: 1.0"
  },
  {
    "objectID": "posts/connected_reversible_linear/index.html#dynamic-model",
    "href": "posts/connected_reversible_linear/index.html#dynamic-model",
    "title": "Connected Reversible Linear Reaction Solver",
    "section": "Dynamic model",
    "text": "Dynamic model\nThis plot reproduces Fig. 4.3 in [1]. It is the time course of concentrations given the equations below, and starting with the given initial conditions.\nI am writing the chemical equation to explcitly show the rate constants used in the vector below:\n\\[ x_1 \\underset{k_{-1}}{\\stackrel{k_1}{\\rightleftharpoons}} x_2 \\stackrel{k_2} \\rightarrow x_3 \\underset{k_{-3}}{\\stackrel{k_3}{\\rightleftharpoons}} x_4 \\]\nThe stoichiometric is the same as above. I am writing the flux vector differently than above to show the concentrations and rate constants explicitly.\n\\[ \\mathbf S = \\begin{pmatrix} {-1} & {1} & {0} & {0} & {0}  \\\\ {1} & {-1} & {-1} & {0} & {0}  \\\\ {0} & {0} & {1} & {-1} & {1}  \\\\  {0} & {0} & {0} & {1} & {-1}  \\\\ \\end{pmatrix}, \\mathbf v(\\mathbf x) = \\begin{pmatrix} k_1 x_1 \\\\ k_{-1} x_2 \\\\ k_2 x_2 \\\\ k_3 x_3 \\\\ k_{-3}x_4 \\end{pmatrix} \\]\nThis gives the following differential equations for concentrations over time:\n\\[ {dx_1 \\over dt} = -k_1x_1+k_{-1}x_2 \\] \\[ {dx_2 \\over dt} = k_1x_1-k_{-1}x_2-k_2x_2 \\] \\[ {dx_3 \\over dt} = k_2x_2-k_3x_3+k_{-3}x_4 \\] \\[ {dx_4 \\over dt} =  k_3x_3-k_{-3}x_4 \\]\n\nCompute concentrations over time\nSet initial conditions and compute concentrations over time.\n\n# Rate constants\nk_fwd_1 = 1.0\nk_rev_1 = 1.0\nk_fwd_2 = 1.0\nk_fwd_3 = 1.0\nk_rev_3 = 1.0\n\n# Initialize matrix that will store trajectory\nx = zeros(Float64, 1000, 4)\n\n# Set initial concentration conditions\nx[1, 1] = 1.0\nx[1, 2] = 0.0\nx[1, 3] = 0.0\nx[1, 4] = 0.0\n\n# 10 arbitrary units of time. Set x axis for plotting.\nt = 10\nts = range(start=0, stop=t, length=length(x[:, 1]))\n\n# Step size\nh = t / length(x[:, 1])\n\n# Comptue concentration trajectories\nfor i ∈ 2:length(x[:, 1])\n    x1 = x[i-1, 1]\n    x2 = x[i-1, 2]\n    x3 = x[i-1, 3]\n    x4 = x[i-1, 4]\n    x[i, 1] = x1 + h*(-k_fwd_1*x1 + k_rev_1*x2)\n    x[i, 2] = x2 + h*(k_fwd_1*x1 - k_rev_1*x2 - k_fwd_2*x2)\n    x[i, 3] = x3 + h*(k_fwd_2*x2 - k_fwd_3*x3 + k_rev_3*x4)\n    x[i, 4] = x4 + h*(k_fwd_3*x3 - k_rev_3*x4)\nend\n\n\n\nPlot concentrations over time\n\nplot(\n    ts,\n    x, \n    labels=[\"x1\" \"x2\" \"x3\" \"x4\"],\n    linewidth=3, \n    xlabel=\"Time\",\n    ylabel=\"Concentration\"\n)"
  },
  {
    "objectID": "posts/connected_reversible_linear/index.html#citation",
    "href": "posts/connected_reversible_linear/index.html#citation",
    "title": "Connected Reversible Linear Reaction Solver",
    "section": "Citation",
    "text": "Citation\n\nBernhard Ø. Palsson. Systems Biology: Simulation of Dynamic Network States. Cambridge University Press, 2011. doi:10.1017/CBO9780511736179.\nLubin, M. et al. JuMP 1.0: recent improvements to a modeling language for mathematical optimization. Math. Prog. Comp. 15, 581–589 (2023)."
  },
  {
    "objectID": "posts/pizza-place/index.html",
    "href": "posts/pizza-place/index.html",
    "title": "PowerBI, SQL: Pizza Place",
    "section": "",
    "text": "The goal of the project was to look at trends in traffic and revenue in the store and find menu items for potential removal to increase efficiency."
  },
  {
    "objectID": "posts/pizza-place/index.html#dashboards-and-charts",
    "href": "posts/pizza-place/index.html#dashboards-and-charts",
    "title": "PowerBI, SQL: Pizza Place",
    "section": "Dashboards and Charts",
    "text": "Dashboards and Charts\n\nMonthly and daily revenue\n\n\n\nAverage daily revenue by month\n\n\n\nRevenue by pizza type\n\n\n\nInsights\n\nThe report spans the entire year of 2015.\nRevenue per order was $10 at a minimum, $38 on average, and $444 at the maximum.\nPlease consult the plot of revenue from sales of different pizza types to see which pizzas performed above and below the median in terms of revenue.\nA total of $818K was earned in 2015.\nSeptember and October were unusually slow months, with July being the busiest.\nThursday to Saturday are the busiest days.\nI recommend a promotion to encourage traffic in the store from Sunday to Wednesday, typically the slowest days and, if some low revenue pizzas have high costs to produce, that those pizzas be discontinued."
  },
  {
    "objectID": "posts/pizza-place/index.html#data-source",
    "href": "posts/pizza-place/index.html#data-source",
    "title": "PowerBI, SQL: Pizza Place",
    "section": "Data source",
    "text": "Data source\n\nMaven Analytics Pizza Place"
  },
  {
    "objectID": "posts/pizza-place/index.html#technology-used",
    "href": "posts/pizza-place/index.html#technology-used",
    "title": "PowerBI, SQL: Pizza Place",
    "section": "Technology used",
    "text": "Technology used\n\nPower BI\nPostgreSQL"
  },
  {
    "objectID": "posts/strogatz-ch-05/index.html",
    "href": "posts/strogatz-ch-05/index.html",
    "title": "Julia: Strogatz Ch. 05 Figures",
    "section": "",
    "text": "Chapter 5 in Strogatz, 3rd edition has many great phase portraits! In this post, I seek to make similar phase portraits in Julia to enhance y understanding of the material.\n\nIn function definitions below, the trailing ; prevents extraneous output in this document."
  },
  {
    "objectID": "posts/strogatz-ch-05/index.html#phase-portrait-inspiration",
    "href": "posts/strogatz-ch-05/index.html#phase-portrait-inspiration",
    "title": "Julia: Strogatz Ch. 05 Figures",
    "section": "",
    "text": "Chapter 5 in Strogatz, 3rd edition has many great phase portraits! In this post, I seek to make similar phase portraits in Julia to enhance y understanding of the material.\n\nIn function definitions below, the trailing ; prevents extraneous output in this document."
  },
  {
    "objectID": "posts/strogatz-ch-05/index.html#fig.-5.1.5-inspired",
    "href": "posts/strogatz-ch-05/index.html#fig.-5.1.5-inspired",
    "title": "Julia: Strogatz Ch. 05 Figures",
    "section": "Fig. 5.1.5-Inspired",
    "text": "Fig. 5.1.5-Inspired\nThere are three phase portraits from the discussion of Figure 5.1.5 that I wanted to display. I set initial conditions at cartesian-coordinate conversions of points around the unit circle to create these plots. As shown in the text, the system being plotted is:\n\\[ x(t) = x_0 e^{at} \\] \\[ y(t) = y_0 e^{-t} \\]\nWhich are conveniently expressed in Julia code:\n\nx_eq(t, x0, a) = x0 * exp(a * t)\ny_eq(t, y0) = y0 * exp(-t);\n\nThe function to draw a selection of the phase portraits found in Figure 5.1.5 is the following:\n\nfunction portrait(a::Float64, r::Float64, width::Int64 = 500, height::Int64 = 500)\n    traces::Vector{GenericTrace} = []\n    angles = [0.0, π/4, π/2, π, 3π/4, 5π/4, 3π/2, 7π/4]\n    x0s = [r * cos(θ) for θ ∈ angles]\n    y0s = [r * sin(θ) for θ ∈ angles]\n    ts = range(0.0, 2.0, length = 10)\n    for (i, (x0, y0)) ∈ enumerate(zip(x0s, y0s))\n        xs = x_eq.(ts, x0, a)\n        ys = y_eq.(ts, y0)\n        showlegend = i == 1\n        trace_start = scatter(\n            x = [x0],\n            y = [y0],\n            mode = \"markers\",\n            marker = attr(color = \"blue\", size = 10),\n            name = \"start\",\n            showlegend = showlegend,\n        )\n        trace_line = scatter(\n            x = xs,\n            y = ys,\n            mode = \"lines\",\n            line = attr(color = \"black\"),\n            name = \"path\",\n            showlegend = showlegend,\n        )\n        trace_end = scatter(\n            x = [xs[end]],\n            y = [ys[end]],\n            mode = \"markers\",\n            marker = attr(color = \"red\", size = 10),\n            name = \"end\",\n            showlegend = showlegend,\n        )\n        push!(traces, trace_start)\n        push!(traces, trace_line)\n        push!(traces, trace_end)\n    end\n    title = \"&lt;b&gt;a = $a&lt;/b&gt;\"\n    plot_bgcolor = \"white\"\n    paper_bgcolor = \"white\"\n    border_width = 1\n    gridwidth = 1\n    border_color = \"black\"\n    gridcolor = \"lightgray\"\n    layout = Layout(\n        plot_bgcolor = plot_bgcolor,\n        paper_bgcolor = paper_bgcolor,\n        title = title,\n        xaxis = attr(\n            showline = true,\n            linewidth = border_width,\n            linecolor = border_color,\n            mirror = true,\n            showgrid = true,\n            gridcolor = gridcolor,\n            gridwidth = gridwidth,\n        ),\n        yaxis = attr(\n            showline = true,\n            linewidth = border_width,\n            linecolor = border_color,\n            mirror = true,\n            showgrid = true,\n            gridcolor = gridcolor,\n            gridwidth = gridwidth,\n        ),\n        width = width,\n        height = height,\n    )\n    plot(traces, layout)\nend;\n\nThe text shows plots for varying values of a, which are reflected in the plots below. The blue dots are at the beginnings of the paths and the red dots are at the end of the paths to give a sense of directionality to the plots.\nThe plots are made with Plotly, so they can be hovered and zoomed, too!\n\nportrait(-2.0, 0.1, 550, 500)\n\n\n\n\n    \n    \n    \n    \n    \n\n\n\n\nportrait(-1.0, 1.0, 550, 500)\n\n\n\n\n    \n    \n    \n    \n    \n\n\n\n\nportrait(1.0, 1.0, 550, 500)"
  },
  {
    "objectID": "posts/strogatz-ch-05/index.html#fig-5.2.2-inspired",
    "href": "posts/strogatz-ch-05/index.html#fig-5.2.2-inspired",
    "title": "Julia: Strogatz Ch. 05 Figures",
    "section": "Fig 5.2.2-Inspired",
    "text": "Fig 5.2.2-Inspired\nLeading up to Figure 5.2.2, we are taken through solving the following:\n\\[\n\\begin{pmatrix}\n\\dot x \\\\\n\\dot y\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & 1 \\\\\n4 & -2\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n\\]\nAs shown in the book, the solution to this system is:\n\\[ x(t) = e^{2t} + e^{-3t} \\] \\[ y(t) = e^{2t} - 4e^{-3t} \\]\nLike the book, I will call the 2x2 matrices in the equation above A. The text’s solution solves for initial conditions (x0, y0) = (2, 3), but in the examples below I solve for different initial conditions to get many different curves.\nThe following functions takes a given A matrix and vector of initial conditions, computes eigensolutions, and solves for c1 and c2 and returns the appropriate x(t) and y(t) functions to plot the phase portraits.\n\nfunction solve_for_ics(A::Matrix{Float64}, ics::Vector{Float64})\n    eig = eigen(A)\n    λ = eig.values\n    v = eig.vectors\n    c = eig.vectors \\ ics\n    x_eq(t::Float64) = c[1]*v[1, 1]*exp(λ[1]*t) + c[2]*v[1, 2]*exp(λ[2]*t)\n    y_eq(t::Float64) = c[1]*v[2, 1]*exp(λ[1]*t) + c[2]*v[2, 2]*exp(λ[2]*t)\n    x_eq, y_eq\nend;\n\nThe following plot is very similar to portrait() above, except it is designed to accomodate the A matrix and call solve_for_ics():\n\nfunction portrait02(\n    A::Matrix{Float64},\n    r::Float64,\n    ts::Vector{Float64},\n    width::Int64 = 500,\n    height::Int64 = 500,\n)\n    angles = [0.0, π/4, π/2, π, 3π/4, 5π/4, 3π/2, 7π/4]\n    x0s = [r * cos(θ) for θ ∈ angles]\n    y0s = [r * sin(θ) for θ ∈ angles]\n    traces::Vector{GenericTrace} = []\n    for (i, (x0, y0)) ∈ enumerate(zip(x0s, y0s))\n        x_eq, y_eq = solve_for_ics(A, [x0, y0])\n        xs = x_eq.(ts)\n        ys = y_eq.(ts)\n        showlegend = i == 1\n        trace_start = scatter(\n            x = [xs[1]],\n            y = [ys[1]],\n            mode = \"markers\",\n            marker = attr(color = \"blue\", size = 10),\n            name = \"start\",\n            showlegend = showlegend,\n        )\n        trace_line = scatter(\n            x = xs,\n            y = ys,\n            mode = \"lines\",\n            marker = attr(color = \"black\"),\n            name = \"path\",\n            showlegend = showlegend,\n        )\n        trace_end = scatter(\n            x = [xs[end]],\n            y = [ys[end]],\n            mode = \"markers\",\n            marker = attr(color = \"red\", size = 10),\n            name = \"stop\",\n            showlegend = showlegend,\n        )\n        push!(traces, trace_start)\n        push!(traces, trace_line)\n        push!(traces, trace_end)\n    end\n    title = \"&lt;b&gt;A = $(string(A))&lt;/b&gt;\"\n    plot_bgcolor = \"white\"\n    paper_bgcolor = \"white\"\n    border_width = 1\n    gridwidth = 1\n    border_color = \"black\"\n    gridcolor = \"lightgray\"\n    layout = Layout(\n        plot_bgcolor = plot_bgcolor,\n        paper_bgcolor = paper_bgcolor,\n        title = title,\n        xaxis = attr(\n            showline = true,\n            linewidth = border_width,\n            linecolor = border_color,\n            mirror = true,\n            showgrid = true,\n            gridcolor = gridcolor,\n            gridwidth = gridwidth,\n        ),\n        yaxis = attr(\n            showline = true,\n            linewidth = border_width,\n            linecolor = border_color,\n            mirror = true,\n            showgrid = true,\n            gridcolor = gridcolor,\n            gridwidth = gridwidth,\n        ),\n        width = width,\n        height = height,\n    )\n    plot(traces, layout)\nend;\n\nAs before, the blue dots show beginings of paths and the red dots show the end.\n\nportrait02([1.0 1.0; 4.0 -2.0], 1.0, collect(range(-0.75, 0.75, 100)), 550, 500)\n\n\n\n\n    \n    \n    \n    \n    \n\n\n\n\nportrait02([2.0 2.0; 3.0 -3.0], 1.0, collect(range(-0.5, 0.5, 100)), 550, 500)"
  },
  {
    "objectID": "posts/strogatz-ch-05/index.html#problem-and-solution-5.2.2-inspired",
    "href": "posts/strogatz-ch-05/index.html#problem-and-solution-5.2.2-inspired",
    "title": "Julia: Strogatz Ch. 05 Figures",
    "section": "Problem and Solution 5.2.2-Inspired",
    "text": "Problem and Solution 5.2.2-Inspired\nProblem 5.2.2 seeks to solve\n\\[\n\\begin{pmatrix}\n\\dot x \\\\\n\\dot y\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n\\]\nWhat makes this interesting is that it results in complex eigenvalues. In the back of the book, the solution is given as:\n\\[\n\\mathbf x(t) = c_1 e^t\n\\begin{pmatrix}\n\\cos t \\\\\n\\sin t\n\\end{pmatrix}\n+ c_2e^t\n\\begin{pmatrix}\n-\\sin t \\\\\n\\cos t\n\\end{pmatrix}\n\\]\nHowever, no further guidance was given on how to find c1 and c2, so in orer to make the nifty spiral plots promised by such a system, I needed to find another approach. Enter DifferentialEquations.jl, the Julia massive Julia package for solving DEs. With this package in hand, the following code made quick work of the problem:\n\nfunction solve_system(\n    A::Matrix{Float64},\n    x0::Vector{Float64},\n    tspan::Tuple{Float64,Float64},\n)\n    f(x, p, t) = A * x\n    prob = ODEProblem(f, x0, tspan)\n    solve(prob, Tsit5())\nend\n\nsolve_system (generic function with 1 method)\n\n\nThe following function, very similar to those above, can plot such a system.\n\nfunction portrait03(\n    A::Matrix{Float64},\n    tspan::Tuple{Float64,Float64},\n    rs::Vector{Float64},\n    angles::Vector{Float64},\n    width::Int64,\n    height::Int64,\n)\n    x0s = [[r * cos(θ), r * sin(θ)] for θ ∈ angles, r ∈ rs]\n    traces::Vector{GenericTrace} = []\n    for (i, x0) ∈ enumerate(x0s)\n        sol = solve_system(A, x0, tspan)\n        xs = sol[1, :]\n        ys = sol[2, :]\n        showlegend = i == 1\n        trace_path = scatter(\n            x = xs,\n            y = ys,\n            mode = \"lines\",\n            line = attr(color = \"black\"),\n            name = \"path\",\n            showlegend = showlegend,\n        )\n        trace_start = scatter(\n            x = [xs[1]],\n            y = [ys[1]],\n            mode = \"markers\",\n            line = attr(color = \"blue\"),\n            name = \"start\",\n            showlegend = showlegend,\n        )\n        trace_end = scatter(\n            x = [xs[end]],\n            y = [ys[end]],\n            mode = \"markers\",\n            line = attr(color = \"red\"),\n            name = \"end\",\n            showlegend = showlegend,\n        )\n        push!(traces, trace_path)\n        push!(traces, trace_start)\n        push!(traces, trace_end)\n    end\n    title = \"&lt;b&gt;A = $A&lt;/b&gt;\"\n    plot_bgcolor = \"white\"\n    paper_bgcolor = \"white\"\n    border_width = 1\n    gridwidth = 1\n    border_color = \"black\"\n    gridcolor = \"lightgray\"\n    layout = Layout(\n        title = title,\n        plot_bgcolor = plot_bgcolor,\n        paper_bgcolor = paper_bgcolor,\n        width = width,\n        height = height,\n        xaxis = attr(\n            showline = true,\n            linewidth = border_width,\n            linecolor = border_color,\n            mirror = true,\n            showgrid = true,\n            gridcolor = gridcolor,\n            gridwidth = gridwidth,\n        ),\n        yaxis = attr(\n            showline = true,\n            linewidth = border_width,\n            linecolor = border_color,\n            mirror = true,\n            showgrid = true,\n            gridcolor = gridcolor,\n            gridwidth = gridwidth,\n        ),\n    )\n    plot(traces, layout)\nend\n\nportrait03 (generic function with 1 method)\n\n\nHere is the plot of a solution to Problem 5.2.2\n\nportrait03(\n    [1.0 -1.0; 1.0 1.0],\n    (0.0, 2.0),\n    [0.5],\n    [0.0, π/4, π/2, π, 3π/4, 5π/4, 3π/2, 7π/4],\n    550,\n    500,\n)\n\n\n\n\n    \n    \n    \n    \n    \n\n\n\nAnd a a plot of the solution to following system:\n\\[\n\\begin{pmatrix}\n\\dot x \\\\\n\\dot y\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 & -3 \\\\\n0 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n\\]\n\nportrait03(\n    [3.0 -3.0; 2.0 2.0],\n    (0.0, 2.0),\n    [0.5],\n    [0.0, π/4, π/2, π, 3π/4, 5π/4, 3π/2, 7π/4],\n    550,\n    500,\n)"
  },
  {
    "objectID": "posts/strogatz-ch-05/index.html#figure-5.3.2-inspired",
    "href": "posts/strogatz-ch-05/index.html#figure-5.3.2-inspired",
    "title": "Julia: Strogatz Ch. 05 Figures",
    "section": "Figure 5.3.2-inspired",
    "text": "Figure 5.3.2-inspired\nFigure 5.3.2 represents the following system:\n\\[\n\\begin{pmatrix}\n\\dot x \\\\\n\\dot y\n\\end{pmatrix}\n=\n\\begin{pmatrix}\na & b \\\\\nb & a\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix}\n\\]\nAssuming:\n\\[ a &lt; 0, b &gt; 0 \\]\nTo solve the systems, I use the solve_for_ics() function I use earlier since the eigenvalues are real-valued. To plot these phase portraits I use the following function, similar to those above, but with a few tweaks to make plotting these phase portraits easier:\n\nfunction portrait(\n    A::Matrix{Float64},\n    angles::Vector{Float64},\n    rs::Vector{Float64},\n    ts::Vector{Float64},\n    width::Int64 = 550,\n    height::Int64 = 500,\n)\n    x0s = [r * cos(θ) for θ ∈ angles, r ∈ rs]\n    y0s = [r * sin(θ) for θ ∈ angles, r ∈ rs]\n    traces::Vector{GenericTrace} = []\n    # for (i, (x0, y0)) ∈ enumerate(Base.product(x0s, y0s))\n    for (i, (x0, y0)) ∈ enumerate(zip(x0s, y0s))\n        x_eq, y_eq = solve_for_ics(A, [x0, y0])\n        xs = x_eq.(ts)\n        ys = y_eq.(ts)\n        showlegend = i == 1\n        trace_start = scatter(\n            x = [xs[1]],\n            y = [ys[1]],\n            mode = \"markers\",\n            marker = attr(color = \"blue\", size = 10),\n            name = \"start\",\n            showlegend = showlegend,\n        )\n        trace_line = scatter(\n            x = xs,\n            y = ys,\n            mode = \"lines\",\n            marker = attr(color = \"black\"),\n            name = \"path\",\n            showlegend = showlegend,\n        )\n        trace_end = scatter(\n            x = [xs[end]],\n            y = [ys[end]],\n            mode = \"markers\",\n            marker = attr(color = \"red\", size = 10),\n            name = \"stop\",\n            showlegend = showlegend,\n        )\n        push!(traces, trace_start)\n        push!(traces, trace_line)\n        push!(traces, trace_end)\n    end\n    title = \"&lt;b&gt;A = $(string(A))&lt;/b&gt;\"\n    plot_bgcolor = \"white\"\n    paper_bgcolor = \"white\"\n    border_width = 1\n    gridwidth = 1\n    border_color = \"black\"\n    gridcolor = \"lightgray\"\n    layout = Layout(\n        plot_bgcolor = plot_bgcolor,\n        paper_bgcolor = paper_bgcolor,\n        title = title,\n        xaxis = attr(\n            showline = true,\n            linewidth = border_width,\n            linecolor = border_color,\n            mirror = true,\n            showgrid = true,\n            gridcolor = gridcolor,\n            gridwidth = gridwidth,\n        ),\n        yaxis = attr(\n            showline = true,\n            linewidth = border_width,\n            linecolor = border_color,\n            mirror = true,\n            showgrid = true,\n            gridcolor = gridcolor,\n            gridwidth = gridwidth,\n        ),\n        width = width,\n        height = height,\n    )\n    plot(traces, layout)\nend;\n\n(Actually, in the book this represents a Romeo and Juliet story, but I am switching back to x and y for the purposes of this demo.)\nThe first phase portrait has a = -2 and b = 1 to match the condition a2 &gt; b2:\n\nportrait(\n    [-2.0 1.0; 1.0 -2.0],\n    [0.0, π/2, π/4, 3π/4, π, 5π/4, 3π/2, 7π/4],\n    [1.0],\n    collect(range(-1.0, 1.0, 100)),\n)\n\n\n\n\n    \n    \n    \n    \n    \n\n\n\nThe second phase portrait has a = -1 and b = 2 to match the condition a2 &lt; b2:\n\nportrait(\n    [-1.0 2.0; 2.0 -1.0],\n    [0.0, π/4, π/3, 2π/3, 3π/4, 5π/6, 7π/6, 5π/4, 4π/3, 5π/3, 7π/4, 11π/6],\n    [1.0],\n    collect(range(-0.5, 0.5, 100)),\n)"
  },
  {
    "objectID": "posts/lennard-jones/index.html",
    "href": "posts/lennard-jones/index.html",
    "title": "Python: Lennard-Jones Potential",
    "section": "",
    "text": "\\[ V(r) = 4 \\epsilon \\biggl[ \\biggl (\\frac{\\sigma}{r}\\biggr)^{12} -  \\biggl (\\frac{\\sigma}{r}\\biggr)^6 \\biggr] \\]\n\\[ \\sigma = \\frac{r_m}{2^{1/6}} \\]\nHere, epsilon is the energy minimum and r_m is the distance of the energy minimum.\nNote the part of the equation inside the square brackets. Recall that negative energies represent a more favorable interaction. The attractive term (raised to the power of 6) is subtracted from the repulsive term (raised to the power of 12).\nSee the Lennard-Jones (L-J) potential on Wikipedia for more details. Also, L-J models are part of a wider set of models of Interatomic potentials.\n\n\n\nIn the calculations and plot that follows, all distance units are in meters and energy units are in kJ/mol. I chose kJ/mol as the energy unit arbitrarily. I chose meters as the distance unit because the orders of magnitude at the atomic scale are the easiest to plot (note that the x-scale of the plot is marked in nanometers.)\n\n\nrmin and epsilon define shape of the potential and rmax is the maximum distance at which I calculate and plot the potential. My selection of values for rmin and epsilon are arbitrary and I selected them for ease of interpretation in the plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrmin = 0.5e-9   # 2 angstroms\nrmax = 1.0e-9  # 10 angstroms\nepsilon = 1  # kJ/mol\n\n\n\n\nrs contains the radii that are in plot and vs contains the corresponding potentials. I calculate vs by NumPy array broadcasting which eliminates the need for a loop.\n\nrs = np.linspace(rmin - 0.75e-10, rmax, 200)\nsigma = rmin / 2**(1/6)\nvs = 4 * epsilon * ((sigma/rs)**12 - (sigma/rs)**6)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,7))\nax.set_xlim(min(rs), max(rs))\nax.set_ylim(min(vs)*1.1, max(vs))\nax.axhline(y=0, color='r', linestyle='dotted')\nax.plot(rs, vs)\nax.axvline(rs[vs.argmin()], color='orange')\nax.set_title(f'L-J Potential', size=20)\nax.set_xlabel('r (m)', size=15)\nax.set_ylabel('V (kJ/mol)', size=15)\n\nText(0, 0.5, 'V (kJ/mol)')\n\n\n\n\n\n\n\n\n\n\n\n\nI noted V(r) = 0 kJ/mol with a horizontal red dashed line and the position of the minimum potential with a vertical orange line.\nRecall that the equation contains a repulsive term and an attractive term. The most favorable distance for this interaction is at 0.5 nm (5 angstroms) because that has the minimum energy. The repulsive term lies to the left of the orange line and rapidly increases as the radius approaches zero. This represents repulsion between atoms if they get too close. The attractive part of the potential lies to the right of the orange line and asymptotically approaches 0 as the atoms drift apart."
  },
  {
    "objectID": "posts/lennard-jones/index.html#equation",
    "href": "posts/lennard-jones/index.html#equation",
    "title": "Python: Lennard-Jones Potential",
    "section": "",
    "text": "\\[ V(r) = 4 \\epsilon \\biggl[ \\biggl (\\frac{\\sigma}{r}\\biggr)^{12} -  \\biggl (\\frac{\\sigma}{r}\\biggr)^6 \\biggr] \\]\n\\[ \\sigma = \\frac{r_m}{2^{1/6}} \\]\nHere, epsilon is the energy minimum and r_m is the distance of the energy minimum.\nNote the part of the equation inside the square brackets. Recall that negative energies represent a more favorable interaction. The attractive term (raised to the power of 6) is subtracted from the repulsive term (raised to the power of 12).\nSee the Lennard-Jones (L-J) potential on Wikipedia for more details. Also, L-J models are part of a wider set of models of Interatomic potentials."
  },
  {
    "objectID": "posts/lennard-jones/index.html#make-plot-of-an-example-l-j-potential",
    "href": "posts/lennard-jones/index.html#make-plot-of-an-example-l-j-potential",
    "title": "Python: Lennard-Jones Potential",
    "section": "",
    "text": "In the calculations and plot that follows, all distance units are in meters and energy units are in kJ/mol. I chose kJ/mol as the energy unit arbitrarily. I chose meters as the distance unit because the orders of magnitude at the atomic scale are the easiest to plot (note that the x-scale of the plot is marked in nanometers.)\n\n\nrmin and epsilon define shape of the potential and rmax is the maximum distance at which I calculate and plot the potential. My selection of values for rmin and epsilon are arbitrary and I selected them for ease of interpretation in the plot\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrmin = 0.5e-9   # 2 angstroms\nrmax = 1.0e-9  # 10 angstroms\nepsilon = 1  # kJ/mol\n\n\n\n\nrs contains the radii that are in plot and vs contains the corresponding potentials. I calculate vs by NumPy array broadcasting which eliminates the need for a loop.\n\nrs = np.linspace(rmin - 0.75e-10, rmax, 200)\nsigma = rmin / 2**(1/6)\nvs = 4 * epsilon * ((sigma/rs)**12 - (sigma/rs)**6)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,7))\nax.set_xlim(min(rs), max(rs))\nax.set_ylim(min(vs)*1.1, max(vs))\nax.axhline(y=0, color='r', linestyle='dotted')\nax.plot(rs, vs)\nax.axvline(rs[vs.argmin()], color='orange')\nax.set_title(f'L-J Potential', size=20)\nax.set_xlabel('r (m)', size=15)\nax.set_ylabel('V (kJ/mol)', size=15)\n\nText(0, 0.5, 'V (kJ/mol)')\n\n\n\n\n\n\n\n\n\n\n\n\nI noted V(r) = 0 kJ/mol with a horizontal red dashed line and the position of the minimum potential with a vertical orange line.\nRecall that the equation contains a repulsive term and an attractive term. The most favorable distance for this interaction is at 0.5 nm (5 angstroms) because that has the minimum energy. The repulsive term lies to the left of the orange line and rapidly increases as the radius approaches zero. This represents repulsion between atoms if they get too close. The attractive part of the potential lies to the right of the orange line and asymptotically approaches 0 as the atoms drift apart."
  },
  {
    "objectID": "data-analysis.html",
    "href": "data-analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "Fictitious business data analyses using SQL, Tableau, and PowerBI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTableau, SQL: Cafe Rewards Analysis\n\n\n\n\n\n\nAlicia\n\n\nAug 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPowerBI, SQL: Pizza Place\n\n\n\n\n\n\nAlicia\n\n\nJul 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPowerBI: Hospital Visits\n\n\n\n\n\n\nAlicia\n\n\nJul 4, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "The Rydberg formula is used to predict emission spectrum lines from hydrogen. The significance of the Rydberg formula is that it was one of the first studies of quantum effects of energy transitions in atoms. Furthermore, it demonstrates that energy emitted is in specific wavelengths, corresponding to a particular energy level transition.\n\\[ \\tilde\\nu = R_H\\Biggl(\\frac{1}{n_{1}^2} + \\frac{1}{n_{2}^2}\\Biggr)\\]\nWhere \\(\\) is the wavenumber in \\(cm^{-1}\\) and \\(R_H = 109,677 cm^{-1}\\). I transform these wavenumbers to nanometers with the equation \\(= \\).\nIn this post, I create spectrum lines using the Rydberg equation and create plots of the series named after the physicists that discovered them.\nPython code to generate the plots is spread throughout the post.\n\nimport matplotlib.pyplot as plt\n\ndef rydberg_nm(n1, n2):\n    \"\"\"\n    Calculates the Rydberg wavenumber between n1 and n2.\n\n    Parameters\n    ----------\n    n1: int\n        The n1 level\n\n    n2: int\n        The n2 level\n\n    Returns\n    -------\n    float\n        Nanometer wavelength of the transition\n    \"\"\"\n    rh = 109677  # cm^-1, Rydberg constant \n    t1 = 1 / n1 ** 2\n    t2 = 1 / n2 ** 2\n    wavenumber = rh * (t1 - t2)\n    return 1 / wavenumber * 1e7\n\n\n\nhttps://en.wikipedia.org/wiki/Lyman_series\nFigure 1 is the Lyman series of hydrogen spectrum lines, as calculated by the Rydberg formula.\nn1=1, n2=2, nm=121.56909227398026\nn1=1, n2=3, nm=102.57392160617086\nn1=1, n2=4, nm=97.25527381918421\nn1=1, n2=5, nm=94.97585333904709\nn1=1, n2=6, nm=93.78187118278477\nn1=1, n2=7, nm=93.07633627226615\nn1=1, n2=8, nm=92.62407030398496\nn1=1, n2=9, nm=92.31652944555377\nn1=1, n2=10, nm=92.09779717725777\nn1=1, n2=11, nm=91.93662603219758\n\nlyman_n2s = range (2, 12)\nlyman = [rydberg_nm(1, n2) for n2 in lyman_n2s]\nfor nm, n2 in zip(lyman, lyman_n2s):\n    print(f'n1=1, n2={n2}, nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Lyman Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(90, 130)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in lyman:\n    ax.axvline(nm, color='r')\n\nn1=1, n2=2, nm=121.56909227398026\nn1=1, n2=3, nm=102.57392160617086\nn1=1, n2=4, nm=97.25527381918421\nn1=1, n2=5, nm=94.97585333904709\nn1=1, n2=6, nm=93.78187118278477\nn1=1, n2=7, nm=93.07633627226615\nn1=1, n2=8, nm=92.62407030398496\nn1=1, n2=9, nm=92.31652944555377\nn1=1, n2=10, nm=92.09779717725777\nn1=1, n2=11, nm=91.93662603219758\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Balmer_series\nFigure 2 is the Balmer series of hydrogen spectral lines, as calculated by the Rydberg formula.\nn1=2, n2=3, nm=656.4730982794933\nn1=2, n2=4, nm=486.27636909592104\nn1=2, n2=5, nm=434.1753295499296\nn1=2, n2=6, nm=410.2956864246834\nn1=2, n2=7, nm=397.12570142833556\nn1=2, n2=8, nm=389.02109527673684\nn1=2, n2=9, nm=383.6531093841195\n\nbalmer_n2s = range(3, 10)\nbalmer = [rydberg_nm(2, n2) for n2 in balmer_n2s]\nfor nm, n2 in zip(balmer, balmer_n2s):\n    print(f'n1=2, n2={n2}, nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Balmer Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(350, 700)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in balmer:\n    ax.axvline(nm, color='r')\n\nn1=2, n2=3, nm=656.4730982794933\nn1=2, n2=4, nm=486.27636909592104\nn1=2, n2=5, nm=434.1753295499296\nn1=2, n2=6, nm=410.2956864246834\nn1=2, n2=7, nm=397.12570142833556\nn1=2, n2=8, nm=389.02109527673684\nn1=2, n2=9, nm=383.6531093841195\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://en.wikipedia.org/wiki/Hydrogen_spectral_series#Paschen_series_(Bohr_series,n′=_3)\nFigure 3 is the Paschen series of spectral lines as calculated by the Rydberg formula.\nn1=2, n2=4 nm=1875.6374236556956\nn1=2, n2=5 nm=1282.1740200771358\nn1=2, n2=6 nm=1094.1218304658223\nn1=2, n2=7 nm=1005.2244317404744\nn1=2, n2=8 nm=954.8699611338087\n\npaschen_n2s = range(4, 9)\npaschen = [rydberg_nm(3, n2) for n2 in paschen_n2s]\nfor nm, n2 in zip(paschen, paschen_n2s):\n    print(f'n1=2, n2={n2} nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Paschen Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(800, 1900)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in paschen:\n    ax.axvline(nm, color='r')\n\nn1=2, n2=4 nm=1875.6374236556956\nn1=2, n2=5 nm=1282.1740200771358\nn1=2, n2=6 nm=1094.1218304658223\nn1=2, n2=7 nm=1005.2244317404744\nn1=2, n2=8 nm=954.8699611338087\n\n\n\n\n\n\n\n\n\n\n\n\nSee Physical Chemistry, 8th ed by Atkins and de Paula, page 320 for the Rydberg equation in wavenumbers. On https://www.powertechnology.com/calculators/ I found that you could convert wavenumbers in inverse centimeters to nanometers with \\(= \\), with wavelength in nanometers (search for the keyword “nanometer”.) Wikipedia contains an extensive article about the Rydberg formula and its history back to the 1880s https://en.wikipedia.org/wiki/Rydberg_formula"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html#lyman-series",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html#lyman-series",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Lyman_series\nFigure 1 is the Lyman series of hydrogen spectrum lines, as calculated by the Rydberg formula.\nn1=1, n2=2, nm=121.56909227398026\nn1=1, n2=3, nm=102.57392160617086\nn1=1, n2=4, nm=97.25527381918421\nn1=1, n2=5, nm=94.97585333904709\nn1=1, n2=6, nm=93.78187118278477\nn1=1, n2=7, nm=93.07633627226615\nn1=1, n2=8, nm=92.62407030398496\nn1=1, n2=9, nm=92.31652944555377\nn1=1, n2=10, nm=92.09779717725777\nn1=1, n2=11, nm=91.93662603219758\n\nlyman_n2s = range (2, 12)\nlyman = [rydberg_nm(1, n2) for n2 in lyman_n2s]\nfor nm, n2 in zip(lyman, lyman_n2s):\n    print(f'n1=1, n2={n2}, nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Lyman Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(90, 130)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in lyman:\n    ax.axvline(nm, color='r')\n\nn1=1, n2=2, nm=121.56909227398026\nn1=1, n2=3, nm=102.57392160617086\nn1=1, n2=4, nm=97.25527381918421\nn1=1, n2=5, nm=94.97585333904709\nn1=1, n2=6, nm=93.78187118278477\nn1=1, n2=7, nm=93.07633627226615\nn1=1, n2=8, nm=92.62407030398496\nn1=1, n2=9, nm=92.31652944555377\nn1=1, n2=10, nm=92.09779717725777\nn1=1, n2=11, nm=91.93662603219758"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html#balmer-series",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html#balmer-series",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Balmer_series\nFigure 2 is the Balmer series of hydrogen spectral lines, as calculated by the Rydberg formula.\nn1=2, n2=3, nm=656.4730982794933\nn1=2, n2=4, nm=486.27636909592104\nn1=2, n2=5, nm=434.1753295499296\nn1=2, n2=6, nm=410.2956864246834\nn1=2, n2=7, nm=397.12570142833556\nn1=2, n2=8, nm=389.02109527673684\nn1=2, n2=9, nm=383.6531093841195\n\nbalmer_n2s = range(3, 10)\nbalmer = [rydberg_nm(2, n2) for n2 in balmer_n2s]\nfor nm, n2 in zip(balmer, balmer_n2s):\n    print(f'n1=2, n2={n2}, nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Balmer Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(350, 700)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in balmer:\n    ax.axvline(nm, color='r')\n\nn1=2, n2=3, nm=656.4730982794933\nn1=2, n2=4, nm=486.27636909592104\nn1=2, n2=5, nm=434.1753295499296\nn1=2, n2=6, nm=410.2956864246834\nn1=2, n2=7, nm=397.12570142833556\nn1=2, n2=8, nm=389.02109527673684\nn1=2, n2=9, nm=383.6531093841195"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html#paschen-series",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html#paschen-series",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "See https://en.wikipedia.org/wiki/Hydrogen_spectral_series#Paschen_series_(Bohr_series,n′=_3)\nFigure 3 is the Paschen series of spectral lines as calculated by the Rydberg formula.\nn1=2, n2=4 nm=1875.6374236556956\nn1=2, n2=5 nm=1282.1740200771358\nn1=2, n2=6 nm=1094.1218304658223\nn1=2, n2=7 nm=1005.2244317404744\nn1=2, n2=8 nm=954.8699611338087\n\npaschen_n2s = range(4, 9)\npaschen = [rydberg_nm(3, n2) for n2 in paschen_n2s]\nfor nm, n2 in zip(paschen, paschen_n2s):\n    print(f'n1=2, n2={n2} nm={nm}')\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 2))\nax.set_title('Paschen Series', color='b', size=20)\nax.set_yticks([])\nax.set_xlim(800, 1900)\nax.set_xlabel(\"nm\", size=20, color='b')\nfor nm in paschen:\n    ax.axvline(nm, color='r')\n\nn1=2, n2=4 nm=1875.6374236556956\nn1=2, n2=5 nm=1282.1740200771358\nn1=2, n2=6 nm=1094.1218304658223\nn1=2, n2=7 nm=1005.2244317404744\nn1=2, n2=8 nm=954.8699611338087"
  },
  {
    "objectID": "posts/rydberg-equation-and-hydrogen-spectra/index.html#references",
    "href": "posts/rydberg-equation-and-hydrogen-spectra/index.html#references",
    "title": "Python: Rydberg Equation and Hydrogen Spectra",
    "section": "",
    "text": "See Physical Chemistry, 8th ed by Atkins and de Paula, page 320 for the Rydberg equation in wavenumbers. On https://www.powertechnology.com/calculators/ I found that you could convert wavenumbers in inverse centimeters to nanometers with \\(= \\), with wavelength in nanometers (search for the keyword “nanometer”.) Wikipedia contains an extensive article about the Rydberg formula and its history back to the 1880s https://en.wikipedia.org/wiki/Rydberg_formula"
  },
  {
    "objectID": "posts/hydrogen-atom/index.html",
    "href": "posts/hydrogen-atom/index.html",
    "title": "Python: Hydrogen Atom",
    "section": "",
    "text": "In this post, I define a class to model the behavior of a hydrogen atom. In the process, I get to solve integrals like the following numerically to test my code:\n\\[ \\int_0^{\\pi} \\int_0^{2\\pi} \\lvert Y_{l, m_l} \\rvert ^2 \\sin \\theta d \\theta d \\phi = 1 \\]\nThis post consists of a arge block of Python code up front, and then explanations and plots below. Let’s get started!"
  },
  {
    "objectID": "posts/hydrogen-atom/index.html#define-a-class-for-the-hydrogenic-atom",
    "href": "posts/hydrogen-atom/index.html#define-a-class-for-the-hydrogenic-atom",
    "title": "Python: Hydrogen Atom",
    "section": "Define a class for the hydrogenic atom",
    "text": "Define a class for the hydrogenic atom\nThis Python class has wavefunctions and energies. It is based on Chapter 10 of Physical Chemistry, 8th Ed by Atkins and De Paula. The goal of the Python class is to reproduce some of the plots and reproduce results with numeric integration found in Chapter 10.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom math import exp, sqrt, pi, cos, sin\nfrom scipy.integrate import dblquad, tplquad, quad\nimport cmath\n\nclass HydrogenicAtom:\n    \"\"\"\n    This class models the wavefunctions and energy levels of a hydrogenic atom.\n    \n    It assumes an infinitely heavy nucleus, so the mass is that of the\n    electron.\n    \n    Masses are in kg, distances are in m\n    \n    This whole class uses complex numbers and math functions.\n    \"\"\"\n    \n    def __init__(self, n=1, l=0, ml=0, z=1):\n        \"\"\"\n        This sets some instance attributes and a few constant used throughout the\n        class.\n        \n        Leave z at 1 for hydrogen.\n        \n        Parameters\n        ----------    \n        n: int\n            Principal quantum number.\n            \n        l: int\n            Orbital angular momentum quantum number.\n            \n        ml: int\n            Magnetic quantum number.\n            \n        z: int\n            Number of protons in the nucleus. Defaults to 1.\n        \"\"\"\n        self.a0 = 5.29e-11       # m\n        self.me = 9.10938356e-31 # kg\n        self.e0 = 8.85418782e-12 # Permitivity of free space\n        self.e_charge = 1.60217662e-19 # Coulombs, charge of electron\n        self.hbar = 1.054571817e-34  # Reduced Planck's constant\n        self.n = n\n        self.l = l\n        self.ml = ml\n        self.z = z\n        \n    def rho(self, r):\n        \"\"\"\n        Calculates the rho part of the radial function. It assumes an infinitely\n        heavy nucleus.\n        \n        From Atkins and de Paula, Table 10.1, page 324\n        \n        Parameters\n        ----------\n        r: float\n            The distance, in meters being calculated\n            \n        Returns\n        -------\n        float\n            The value of rho.\n        \"\"\"\n        return (2 * self.z / self.n / self.a0) * r\n    \n    def radial(self, r):\n        \"\"\"\n        The radial part of the wavefunction\n        \n        Parameters\n        ----------\n        r: float\n            Radius, in meters\n            \n        Returns\n        -------\n        complex\n            The value of the radial wavefunction, which only uses the real\n            part of the complex value.\n            \n        Raises\n        ------\n        Exception\n            Raises an exception for invalid n and l\n        \"\"\"\n        za32 = (self.z / self.a0) ** (3/2)\n        rho = self.rho(r)\n        exp_rho = exp(-rho / 2.0)\n        \n        if self.n == 1 and self.l == 0:  # 1s orbital\n            return 2 * za32 * exp_rho\n        elif self.n == 2 and self.l == 0:  # 2s orbital\n            return (1.0 / sqrt(8)) * za32 * (2.0 - rho) * exp_rho\n        elif self.n == 2 and self.l == 1:  # 2p orbital\n            return (1.0 / sqrt(24)) * za32 * rho * exp_rho\n        elif self.n == 3 and self.l == 0:\n            return (1.0 / sqrt(243)) * za32 * (6.0 - 6 * rho + rho ** 2) * exp_rho\n        elif self.n == 3 and self.l == 1:\n            return (1.0 / sqrt(486)) * za32 * (4.0 - rho) * rho * exp_rho\n        elif self.n == 3 and self.l == 2:\n            return (1.0 / sqrt(2430)) * za32 * rho ** 2 * exp_rho\n        else:\n            raise Exception(f'No radial function for {self.n} and {self.l}')\n            \n    def spherical_harmonic(self, theta, phi):\n        \"\"\"\n        Find the value of the spherical harmonic given an quantum numbers\n        l, ml and coordinates theta, phi.\n        \n        From Atkins and de Paula, Table 9.3, page 302\n        \n        Parameters\n        ----------    \n        theta: float\n            Theta coordinate, from 0 to pi\n\n        phi: float\n            Phi coordinate, from 0 to 2*pi\n            \n        Returns\n        -------\n        complex\n            The value of the spherical harmonic, which is a complex value\n            \n        Raises\n        ------\n        Exception\n            Raises an Exception for an invalid combination of l and ml\n        \"\"\"\n        if self.l == 0 and self.ml == 0:\n            return sqrt(1 / 4.0 / pi)\n        \n        elif self.l == 1 and self.ml == 0:\n            return sqrt(3.0 / 4.0 / pi) * cos(theta)\n        elif self.l == 1 and self.ml == 1:\n            return -sqrt(3.0 / 8.0 / pi) * sin(theta) * cmath.exp(1j * phi)\n        elif self.l == 1 and self.ml == -1:\n            return -sqrt(3.0 / 8.0 / pi) * sin(theta) * cmath.exp(-1j * phi)\n        \n        elif self.l == 2 and self.ml == 0:\n            return sqrt(5.0 / 16.0 / pi) * (3 * cos(theta)** 2 - 1)\n        elif self.l == 2 and self.ml == 1:\n            return -sqrt(15.0 / 8.0 / pi) * cos(theta) * sin(theta) * cmath.exp(1j * phi)\n        elif self.l == 2 and self.ml == -1:\n            return sqrt(15.0 / 8.0 / pi) * cos(theta) * sin(theta) * cmath.exp(-1j * phi)\n        elif self.l == 2 and self.ml == 2:\n            return sqrt(15.0 / 32.0 / pi) * sin(theta) ** 2 * cmath.exp(2j * phi)\n        elif self.l == 2 and self.ml == -2:\n            return sqrt(15.0 / 32.0 / pi) * sin(theta) ** 2 * cmath.exp(-2j * phi)\n        \n        elif self.l == 3 and self.ml == 0:\n            return sqrt(7.0 / 16.0 / pi) * (5 * cos(theta) ** 3 - 3 * cos(theta))\n        elif self.l == 3 and self.ml == 1:\n            return -sqrt(21.0 / 64.0 / pi) * (5 * cos(theta) ** 2 - 1) * sin(theta) * cmath.exp(1j * phi)\n        elif self.l == 3 and self.ml == -1:\n            return sqrt(21.0 / 64.0 / pi) * (5 * cos(theta) ** 2 - 1) * sin(theta) * cmath.exp(-1j * phi)\n        elif self.l == 3 and self.ml == 2:\n            return sqrt(105.0 / 32.0 / pi) * sin(theta) ** 2 * cos(theta) * cmath.exp(2j * phi)\n        elif self.l == 3 and self.ml == -2:\n            return sqrt(105.0 / 32.0 / pi) * sin(theta) ** 2 * cos(theta) * cmath.exp(-2j * phi)\n        elif self.l == 3 and self.ml == 3:\n            return -sqrt(35.0 / 64.0 / pi) * sin(theta) ** 3 * cmath.exp(3j * phi)\n        elif self.l == 3 and self.ml == -3:\n            return sqrt(35.0 / 64.0 / pi) * sin(theta) ** 3 * cmath.exp(-3j * phi)\n        \n        else:\n            raise Exception(f'No equation for l={self.l} and ml={self.ml}')\n            \n    def wavefunction(self, r, theta, phi):\n        \"\"\"\n        Returns the value of the wavefunction at the given location.\n        \n        Parameters\n        ----------\n        r: float\n            The r value, from 0 to infinity\n            \n        theta: float\n            The theta value, from 0 to pi\n            \n        phi: float\n            The phi value, from 0 to 2*pi\n            \n        Returns\n        -------\n        complex\n            The complex value of the wavefunction.\n        \"\"\"\n        return self.radial(r) * self.spherical_harmonic(theta, phi)\n            \n    def energy(self):\n        \"\"\"\n        This calcuates the energy in Joules of the electron at the n level of this\n        atoms configuration.\n        \n        It makes the infinite mas approximation for the nucleus, so the mass in\n        the expression is that of the electron.\n        \n        From page 324 of Atkins and de Paula\n        \n        Returns\n        -------\n        float, float\n            First float is energy level in joules, second float is energy level in eV\n        \"\"\"\n        ev_per_joule = 6.242e+18\n        numerator = self.z**2 * self.me * self.e_charge**4\n        denominator = 32 * pi**2 * self.e0**2 * self.hbar**2 * self.n**2\n        value = numerator / denominator\n        joules = -value\n        ev = joules * ev_per_joule\n        return joules, ev\n    \n    def mean_orbital_radius(self):\n        \"\"\"\n        Returns the mean orbital radius of the given state of this atom.\n        It does this by integration.\n        \n        Returns\n        -------\n        float\n            The mean orbital radius\n        \"\"\"\n        second_point_guess = 10 * self.n * self.a0\n        def integrand(r):\n            return r**3 * self.radial(r)**2\n        radius, _ = quad(integrand, 0, 1, points=[0, second_point_guess])\n        return radius"
  },
  {
    "objectID": "posts/hydrogen-atom/index.html#check-the-spherical-harmonics",
    "href": "posts/hydrogen-atom/index.html#check-the-spherical-harmonics",
    "title": "Python: Hydrogen Atom",
    "section": "Check the spherical harmonics",
    "text": "Check the spherical harmonics\nMake sure all the spherical harmonics are normalized. That means each of the the spherical harmonic functions must stasify the following expression:\n\\[ \\int_0^{\\pi} \\int_0^{2\\pi} \\lvert Y_{l, m_l} \\rvert ^2 \\sin \\theta d \\theta d \\phi = 1 \\]\nThe next block of code takes the spherical harmonic for each parameter set of l and ml and numerically integrates it to ensure that the result is 1.0, within floating point precision. A report of the parameters and the results follows, and they all integrate to approximately 1.0! Success!\n\nparameters = [\n    { 'n': 3, 'l': 0, 'ml': 0 },\n    { 'n': 3, 'l': 1, 'ml': 0 },\n    { 'n': 3, 'l': 1, 'ml': 1 },\n    { 'n': 3, 'l': 1, 'ml': -1 },\n    { 'n': 3, 'l': 2, 'ml': 0 },\n    { 'n': 3, 'l': 2, 'ml': 1 },\n    { 'n': 3, 'l': 2, 'ml': -1 },\n    { 'n': 3, 'l': 2, 'ml': 2 },\n    { 'n': 3, 'l': 2, 'ml': -2 },\n    { 'n': 3, 'l': 3, 'ml': 0 },\n    { 'n': 3, 'l': 3, 'ml': 1 },\n    { 'n': 3, 'l': 3, 'ml': -1 },\n    { 'n': 3, 'l': 3, 'ml': 2 },\n    { 'n': 3, 'l': 3, 'ml': -2 },\n    { 'n': 3, 'l': 3, 'ml': 3 },\n    { 'n': 3, 'l': 3, 'ml': -3 }\n]\n\nfor p in parameters:\n    ha = HydrogenicAtom(n=p['n'], l=p['l'], ml=p['ml'])\n    def integrand(phi,theta):\n        value = ha.spherical_harmonic(theta, phi)\n        return abs(value) ** 2 * sin(theta).real\n    p['result'], _ = dblquad(integrand, 0, pi, 0, 2 * pi)\n    \nfor p in parameters:\n    print(p)\n\n{'n': 3, 'l': 0, 'ml': 0, 'result': 0.9999999999999999}\n{'n': 3, 'l': 1, 'ml': 0, 'result': 1.0}\n{'n': 3, 'l': 1, 'ml': 1, 'result': 1.0000000000000002}\n{'n': 3, 'l': 1, 'ml': -1, 'result': 1.0000000000000002}\n{'n': 3, 'l': 2, 'ml': 0, 'result': 1.0000000000000002}\n{'n': 3, 'l': 2, 'ml': 1, 'result': 0.9999999999999998}\n{'n': 3, 'l': 2, 'ml': -1, 'result': 0.9999999999999998}\n{'n': 3, 'l': 2, 'ml': 2, 'result': 1.0}\n{'n': 3, 'l': 2, 'ml': -2, 'result': 1.0}\n{'n': 3, 'l': 3, 'ml': 0, 'result': 1.0}\n{'n': 3, 'l': 3, 'ml': 1, 'result': 1.0}\n{'n': 3, 'l': 3, 'ml': -1, 'result': 1.0}\n{'n': 3, 'l': 3, 'ml': 2, 'result': 0.9999999999999998}\n{'n': 3, 'l': 3, 'ml': -2, 'result': 0.9999999999999998}\n{'n': 3, 'l': 3, 'ml': 3, 'result': 1.0000000000000002}\n{'n': 3, 'l': 3, 'ml': -3, 'result': 1.0000000000000002}"
  },
  {
    "objectID": "posts/hydrogen-atom/index.html#plots-of-the-radial-functions",
    "href": "posts/hydrogen-atom/index.html#plots-of-the-radial-functions",
    "title": "Python: Hydrogen Atom",
    "section": "Plots of the radial functions",
    "text": "Plots of the radial functions\nFigure 1 has plots of the radial functions for various combinations of n and l. Some of the subplots are blank because there is no corresponding radial function for their position on the chart.\n\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(12, 15))\n\n# Just so I can access instance variables in an instance to make the dictionary.\nha = HydrogenicAtom()\nyscaler = (ha.z / ha.a0)**(3/2)\n\nparameters = [\n    {'n': 1, 'l': 0, 'x_scaler': 5, 'yscaler':  yscaler },\n    {'n': 2, 'l': 0, 'x_scaler': 15, 'yscaler': yscaler },\n    {'n': 3, 'l': 0, 'x_scaler': 20, 'yscaler': yscaler },\n    {'n': 2, 'l': 1, 'x_scaler': 15, 'yscaler': yscaler },\n    {'n': 3, 'l': 1, 'x_scaler': 30, 'yscaler': yscaler },\n    {'n': 3, 'l': 2, 'x_scaler': 30, 'yscaler': yscaler }\n]\n\nfor p in parameters:\n    row = p['n'] - 1\n    col = p['l']\n    ha = HydrogenicAtom(n=p['n'], l=p['l'])\n    xs = np.linspace(0, ha.a0 * p['x_scaler'], 100)\n    xs_labels = xs / ha.a0  # so that the x axis is labeled in units of a0\n    ys = [ha.radial(r) / p['yscaler'] for r in xs]\n    ax = axs[row, col]\n    ax.set_title(f'n={p[\"n\"]}, l={p[\"l\"]}', color='b')\n    ax.set_xlim(min(xs_labels), max(xs_labels))\n    if col == 0:\n        ax.set_ylabel('r/((Z/a0)**(3/2))', color='b')\n    ax.set_xlabel('r/a0')\n    ax.axhline(0.0, color='r')\n    ax.plot(xs_labels, ys)\n    \naxs[0, 1].set_title('intentionally blank')\naxs[0, 2].set_title('intentionally blank')\naxs[1, 2].set_title('intentionally blank')\n\nText(0.5, 1.0, 'intentionally blank')"
  },
  {
    "objectID": "posts/hydrogen-atom/index.html#hydrogen-energy-levels",
    "href": "posts/hydrogen-atom/index.html#hydrogen-energy-levels",
    "title": "Python: Hydrogen Atom",
    "section": "Hydrogen energy levels",
    "text": "Hydrogen energy levels\nNote how the levels pack closer together at higher energy levels. The lowest energy, -13.6 eV, is the ground state of the hydrogen atom. All the energies are negative, which means they refer to bound states where the nucleus holds the electron.\n\nys = []\nfor n in range(1, 10):\n    ha = HydrogenicAtom(n=n)\n    _, ev = ha.energy()\n    ys.append((n, round(ev, 2)))\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(2, 10))\nax.set_ylim(-14.0, 0.0)\nax.set_xticks([])\nax.set_ylabel('eV', size=20, color='b')\nax.set_title('Hydrogen Energy Levels, n=1 to n=9', size=20, color='b')\nfor y in ys:\n    ax.axhline(y[1], color='r')\n\n\n\n\n\n\n\n\n\n\n\npng"
  },
  {
    "objectID": "posts/hydrogen-atom/index.html#by-numeric-integration-what-is-the-mean-radius-of-1s-orbital",
    "href": "posts/hydrogen-atom/index.html#by-numeric-integration-what-is-the-mean-radius-of-1s-orbital",
    "title": "Python: Hydrogen Atom",
    "section": "By numeric integration, what is the mean radius of 1s orbital?",
    "text": "By numeric integration, what is the mean radius of 1s orbital?\nIn this section, I follow the integral given in Example 10.2 that will find the mean radius of an orbital:\n\\[ \\langle r \\rangle = \\int_0^{\\infty} r^3 R_{n,l}^2 dr \\]\nI integrate it numerically with the quad function from scipy.integrate. The points argument to quad tells the function the r values that the value ies within. To help the quad numeri integration function out, I took a guess that the mean radius is going to be within \\(10 a_0 n\\) radii of the nucleus. Also, I took 1 meter as the “infinity” for integration in this case.\n\nha = HydrogenicAtom(n=1, l=0, ml=0)\n\ndef integrand(r):\n    return r**3 * ha.radial(r)**2\n\nprint(quad(integrand, 0, 1, points=[0, 10 * ha.a0 * ha.n]))\n\n(7.934974578483517e-11, 3.8150348949583136e-14)\n\n\nThe first element of the tuple above is the result of the integration, and the second element is the estimated error of the integration. Below is the solution to the analytical integration solution given by the book. It matches the numeric integration!\n\nprint(3 * ha.a0 / 2)\n\n7.935e-11\n\n\nWhat about the 3s orbital? First numeric integration, then the numeric solution from the book.\n\nha = HydrogenicAtom(n=3, l=0, ml=0)\n\ndef integrand(r):\n    return r**3 * ha.radial(r)**2\n\nprint(quad(integrand, 0, 1, points=[0, 10 * ha.a0 * ha.n]))\n\n(7.118296273936346e-10, 6.083642982151686e-11)\n\n\n\nprint(27 * ha.a0 / 2)\n\n7.1415e-10\n\n\nWhat about the 3p orbital?\n\nha = HydrogenicAtom(n=3, l=1, ml=0)\n\ndef integrand(r):\n    return r**3 * ha.radial(r)**2\n\nprint(quad(integrand, 0, 1, points=[0, 10 * ha.a0 * ha.n]))\n\n(6.598213399404118e-10, 3.765511206963294e-12)\n\n\n\nprint(25 * ha.a0 / 2)\n\n6.6125e-10\n\n\nOverall, the numerical integration and the guess about where the interesting parts of the integration are worked out fairly well for these examples."
  },
  {
    "objectID": "posts/hydrogen-atom/index.html#what-is-the-mean-radius-of-each-orbital",
    "href": "posts/hydrogen-atom/index.html#what-is-the-mean-radius-of-each-orbital",
    "title": "Python: Hydrogen Atom",
    "section": "What is the mean radius of each orbital?",
    "text": "What is the mean radius of each orbital?\nMake some plots of the mean radius of each orbital. Red circles are s orbitals, green squares are p orbitals, blue diamonds are d orbitals. Note that the radii of d &lt; p &lt; s for each level n.\n\nparameters = [\n    {'n': 1, 'l': 0 },\n    {'n': 2, 'l': 0 },\n    {'n': 2, 'l': 1 },\n    {'n': 3, 'l': 0 },\n    {'n': 3, 'l': 1 },\n    {'n': 3, 'l': 2 }\n]\n\nfor p in parameters:\n    ha = HydrogenicAtom(n=p['n'], l=p['l'])\n    p['mean_radius_a0'] = ha.mean_orbital_radius() / ha.a0\n    \nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\nfor p in parameters:\n    x = p['n']\n    y = p['mean_radius_a0']\n    if p['l'] == 0:\n        color = 'r'\n        marker = 'o'\n        label = 's'\n    elif p['l'] == 1:\n        color = 'g'\n        marker = 's'\n        label = 'p'\n    else:\n        color = 'b'\n        marker = 'd'\n        label = 'd'\n    ax.scatter(x, y, marker=marker, color=color, s=200)\nax.set_xticks([1, 2, 3])\nax.set_xlabel('n')\nax.set_ylabel('r / a0')\nax.set_title('Relative mean orbital radii')\n\nText(0.5, 1.0, 'Relative mean orbital radii')"
  },
  {
    "objectID": "posts/solubility-clustering/index.html",
    "href": "posts/solubility-clustering/index.html",
    "title": "R: Solubility Clustering",
    "section": "",
    "text": "In my prior aqueous solubility regression study, I did an exploratory data visualization and found intriguing plots of solubility versus other variables in the study. I didn’t perform any experimental modeling of those relationships in that study. Here, I followup by performing a cluster analysis of solubility relationships to help future regression modeling efforts. My question is: do clusters within each of these relationships explain each feature’s effect on solubility?\nTable 1 is a sample of some of the compounds in the dataset:\ndf &lt;- as_tibble(read.csv(\"data/delaney-processed.csv\")) %&gt;%\n  select(\n    compound = Compound.ID, \n    mw = Molecular.Weight, \n    h_bond_donors = Number.of.H.Bond.Donors, \n    rings = Number.of.Rings, \n    rotatable_bonds = Number.of.Rotatable.Bonds, \n    psa = Polar.Surface.Area, \n    solubility = measured.log.solubility.in.mols.per.litre\n)\nknitr::kable(head(df))\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompound\nmw\nh_bond_donors\nrings\nrotatable_bonds\npsa\nsolubility\n\n\n\n\nAmigdalin\n457.432\n7\n3\n7\n202.32\n-0.77\n\n\nFenfuram\n201.225\n1\n2\n2\n42.24\n-3.30\n\n\ncitral\n152.237\n0\n0\n4\n17.07\n-2.06\n\n\nPicene\n278.354\n0\n5\n0\n0.00\n-7.87\n\n\nThiophene\n84.143\n0\n1\n0\n0.00\n-1.33\n\n\nbenzothiazole\n135.191\n0\n2\n0\n12.89\n-1.50"
  },
  {
    "objectID": "posts/solubility-clustering/index.html#dataset-description",
    "href": "posts/solubility-clustering/index.html#dataset-description",
    "title": "R: Solubility Clustering",
    "section": "Dataset description",
    "text": "Dataset description\nSee the previous post for a description of the dataset and variables I use in this study."
  },
  {
    "objectID": "posts/solubility-clustering/index.html#review-of-prior-figures",
    "href": "posts/solubility-clustering/index.html#review-of-prior-figures",
    "title": "R: Solubility Clustering",
    "section": "Review of prior figures",
    "text": "Review of prior figures\nFigure 1 contains histograms and bar plots showing the distributions of variables in the datasets.\n\np1 &lt;- ggplot(df, aes(x = mw)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(a)\")\n\np2 &lt;- ggplot(df, aes(x = psa)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(b)\")\n\np3 &lt;- ggplot(df, aes(x = solubility)) +\n  geom_histogram(bins = 10) +\n  labs(title = \"(c)\")\n\np4 &lt;- ggplot(df, aes(x = h_bond_donors)) +\n  geom_bar() +\n  labs(title = \"(d)\")\n\np5 &lt;- ggplot(df, aes(x = rings)) +\n  geom_bar() +\n  labs(title = \"(e)\")\n\np6 &lt;- ggplot(df, aes(x = rotatable_bonds)) +\n  geom_bar() +\n  labs(title = \"(f)\")\n\ngrid.arrange(p1, p2, p3, p4, p5, p6, nrow = 3)\n\n\n\n\nHistograms and bar plots of variables\n\n\n\n\nAs shown above, the subplots 1a, 1b, 1d, 1e, and 1f show distributions that favor the low end of the distribution. This low-end favorability is essential when extracting relationships for values greater in these distributions. For example, data about what happens to solubility with large ring counts are relatively sparse.\nFigure 2 contains the plots of solubility versus other variables I want to explore with clustering. Note that the plots have jittered points to prevent overplotting. Also, note that the solubility is in log(mol/L) because solubility in this dataset spans many orders of magnitude.\n\nalpha &lt;- 0.1\n\np1 &lt;- ggplot(df, aes(x = mw, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(a)\")\n\np2 &lt;- ggplot(df, aes(x = psa, y = solubility)) +\n  geom_jitter(alpha = alpha) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(b)\")\n\np3 &lt;- ggplot(df, aes(x = h_bond_donors, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(c)\")\n\np4 &lt;- ggplot(df, aes(x = rings, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(d)\")\n\np5 &lt;- ggplot(df, aes(x = rotatable_bonds, y = solubility)) +\n  geom_jitter(alpha = alpha, width = 0.1) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"(e)\")\n\ngrid.arrange(p1, p2, p3, p4, p5, nrow = 3)\n\n\n\n\nRelationship of molecular weight to other variables\n\n\n\n\nEach variable has a different trend of its effect on solubility, as shown above. Figures 2b (of polar surface area) and 2c (of h-bond donors) show increasing trends of solubility. Figures 2a (molecular weight), 2d (rings), and 2e (rotatable bonds) show decreasing trends in solubility."
  },
  {
    "objectID": "posts/solubility-clustering/index.html#solubility-trends-for-each-variable-by-cluster",
    "href": "posts/solubility-clustering/index.html#solubility-trends-for-each-variable-by-cluster",
    "title": "R: Solubility Clustering",
    "section": "Solubility trends for each variable by cluster",
    "text": "Solubility trends for each variable by cluster\nI will display the clustering information as scatter plots, with each point’s color indicating the group from the hierarchical clustering algorithm. For each variable, I make tests with two, three, and four clusters. Cluster_2, cluster_3, and cluster_4 are the variable names for each of these cluster counts, respectively. On each plot, solubility is on the y axis. I am particularly interested in clusters that separate the compounds by their solubilities, i.e., by dividing groups with a line parallel to the x-axis while minimizing the number of groups needed to make this division.\n\nhierarchical_clusters &lt;- function(features_df) {\n  cluster_labels = list()\n  \n  distances &lt;- features_df %&gt;%\n    as.matrix() %&gt;%\n    scale() %&gt;%\n    dist()\n  \n  hclust_out &lt;- hclust(distances, method = \"complete\")\n  \n  features_df %&gt;%\n    mutate(\n      cluster_2 = as.factor(cutree(hclust_out, k = 2)),\n      cluster_3 = as.factor(cutree(hclust_out, k = 3)),\n      cluster_4 = as.factor(cutree(hclust_out, k = 4))\n    )\n}\n\n\nSolubility versus polar surface area (PSA)\n\nalpha &lt;- 0.3\nsize &lt;- 3\n\npsa_cluster_df &lt;- hierarchical_clusters(select(df, solubility, psa))\n\ncluster_2 &lt;- ggplot(psa_cluster_df, aes(x = psa, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(a)\")\n\ncluster_3 &lt;- ggplot(psa_cluster_df, aes(x = psa, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(b)\")\n\ncluster_4 &lt;- ggplot(psa_cluster_df, aes(x = psa, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(c)\")\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus polar surface area (PSA)\n\n\n\n\nFigure 3a shows that two clusters are not enough to separate the solubility axis into high and low solubilities. Figure 3c reveals that four clusters show the necessary separation, but the fourth only covers five compounds with large polar surface areas and high solubilities. Figure 3b is just right: it displays a clear break of high and low solubilities with only three clusters."
  },
  {
    "objectID": "posts/solubility-clustering/index.html#solubility-versus-h-bond-donors-by-cluster",
    "href": "posts/solubility-clustering/index.html#solubility-versus-h-bond-donors-by-cluster",
    "title": "R: Solubility Clustering",
    "section": "Solubility versus h-bond donors by cluster",
    "text": "Solubility versus h-bond donors by cluster\n\nh_bond_donor_cluster_df &lt;- hierarchical_clusters(select(df, solubility, h_bond_donors))\n\ncluster_2 &lt;- ggplot(h_bond_donor_cluster_df, aes(x = h_bond_donors, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha) +\n  labs(title = \"(a)\") +\n  theme_minimal()\n\ncluster_3 &lt;- ggplot(h_bond_donor_cluster_df, aes(x = h_bond_donors, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha) +\n  labs(title = \"(b)\") +\n  theme_minimal()\n\ncluster_4 &lt;- ggplot(h_bond_donor_cluster_df, aes(x = h_bond_donors, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha) +\n  labs(title = \"(c)\") +\n  theme_minimal()\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus h-bond donors by cluster\n\n\n\n\nSimilar to Figure 3, Figure 4a with two clusters provides no clear break between high and low solubility. Figure 4c shows that the fourth cluster covers a few points a corner of the plot. Once again, Figure 4b shows that three clusters provide a clear break between high and low solubility, with a few points covered by a third cluster. I’ll choose 3 clusters as optimal for h-bond donors.\n\nSolubility versus rings by cluster\n\nrings_cluster_df &lt;- hierarchical_clusters(select(df, solubility, rings))\n\ncluster_2 &lt;- ggplot(rings_cluster_df, aes(x = rings, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(a)\")\n\ncluster_3 &lt;- ggplot(rings_cluster_df, aes(x = rings, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(b)\")\n\ncluster_4 &lt;- ggplot(rings_cluster_df, aes(x = rings, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(c)\")\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus rings\n\n\n\n\nFigure 5a shows two clusters, each of which straddles high and low solubility. Figure 5b, with three clusters, exhibits the same problem, though the third cluster is generally of low solubility. Figure 5c represents the separation best: cluster 1 is mostly high solubility, cluster 4 is primarily low solubility, and cluster 3 is low solubility. Cluster 2 aggregates between -3 and -5, so it to provides a useful solubility range. I’ll choose four as the optimum number of clusters."
  },
  {
    "objectID": "posts/solubility-clustering/index.html#solubility-versus-rotatable_bonds-by-cluster",
    "href": "posts/solubility-clustering/index.html#solubility-versus-rotatable_bonds-by-cluster",
    "title": "R: Solubility Clustering",
    "section": "Solubility versus rotatable_bonds by cluster",
    "text": "Solubility versus rotatable_bonds by cluster\n\nrotatable_bonds_cluster_df &lt;- hierarchical_clusters(select(df, solubility, rotatable_bonds))\n\ncluster_2 &lt;- ggplot(rotatable_bonds_cluster_df, aes(x = rotatable_bonds, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(a)\")\n\ncluster_3 &lt;- ggplot(rotatable_bonds_cluster_df, aes(x = rotatable_bonds, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(b)\")\n\ncluster_4 &lt;- ggplot(rotatable_bonds_cluster_df, aes(x = rotatable_bonds, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(c)\")\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus rotatable_bonds by cluster\n\n\n\n\nFigure 6a shows two clusters of rotatable bond counts that don’t provide a break between high and low solubility. Figure 6c adds the fourth cluster with a wide span of solubilities and, therefore, not much value. Once again, Figure 6b shows three clusters, with reasonably well-defined breaks around solubilities of -3 and -4. I’ll choose three as the optimum number of clusters for rotatable bonds.\n\nSolubility versus molecular weight by cluster\n\nmw_cluster_df &lt;- hierarchical_clusters(select(df, solubility, mw))\n\ncluster_2 &lt;- ggplot(mw_cluster_df, aes(x = mw, y = solubility, col = cluster_2)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(a)\") +\n  theme_minimal()\n\ncluster_3 &lt;- ggplot(mw_cluster_df, aes(x = mw, y = solubility, col = cluster_3)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(b)\") +\n  theme_minimal()\n\ncluster_4 &lt;- ggplot(mw_cluster_df, aes(x = mw, y = solubility, col = cluster_4)) +\n  geom_point(alpha = alpha, size = size) +\n  labs(title = \"(c)\") +\n  theme_minimal()\n\ngrid.arrange(cluster_2, cluster_3, cluster_4, nrow = 2)\n\n\n\n\nSolubility versus molecular weight by cluster\n\n\n\n\n\n\nSummary of optimum number of clusters\n\n\n\nVariable name\nOptimal cluster count\n\n\n\n\npolar surface area (PSA)\n3\n\n\nh-bond donors\n3\n\n\nrings\n4\n\n\nrotatable bonds\n3\n\n\nmolecular weight\n4"
  },
  {
    "objectID": "posts/solubility-clustering/index.html#conclusion",
    "href": "posts/solubility-clustering/index.html#conclusion",
    "title": "R: Solubility Clustering",
    "section": "Conclusion",
    "text": "Conclusion\nWhen I use hierarchiical clustering to group solubilities, each variable in the dataset needs a different number of clusters to adequately specify its relastionship to solubility."
  },
  {
    "objectID": "posts/moldyn/index.html",
    "href": "posts/moldyn/index.html",
    "title": "Julia: Molecular Dynamics of HCl",
    "section": "",
    "text": "Molecular dynamics models the motion of atoms within molecules using classical mechanics calculations. Molecular dynamics envisions atoms as balls on springs and calculates the accelerations of atoms within the system in a potential energy force field that has terms for several types of motions in the molecule. In this post, I will use the letter U to denote potential energy. Typical force fields model the following potentials: \\[ U = U_{stretch} + U_{angle} + U_{dihedral} + U_{improper} \\] For a complete description of the force field terms and motions, I refer the reader to Chapter 3 of the Cramer text referenced at the bottom of this document. In this project, I only model bond stretch energy so I could keep the code simple. The notation I will use in this post and code mostly comes from Chapter 3 of the Cramer text.\n\n\nThe stretch potential energy of a bond between atoms A and i is denoted Ustretch and has the following form: \\[ U_{stretch} = {1 \\over 2}k_{Ai}(r_{Ai}-r_{Ai,eq})^2 \\] Where kAi is the spring constant of the bond, rAi,eq is the equilibrium distance between atoms A and B and rAB is the instantaneous distance between the atoms. Since I am only modeling the stretch energy in this project, I will shorten Ustretch to U from here onward.\nThe first derivative of the stretch energy which is used to calculate the stretch forces acting upon an atom by the other atoms it is bonded to is: \\[ {\\partial U \\over \\partial x_A}=\\sum_{i} {\\partial U \\over \\partial r_{Ai}} {\\partial r_{Ai} \\over \\partial x_A} \\] Where the summation is over the atoms bonded to A and i refers to the other atoms. Similarly to x axis potential, the y and z potentials are: \\[ {\\partial U \\over \\partial y_A}=\\sum_{i} {\\partial U \\over \\partial r_{Ai}} {\\partial r_{Ai} \\over \\partial y_A} \\] \\[ {\\partial U \\over \\partial z_A}=\\sum_{i} {\\partial U \\over \\partial r_{Ai}} {\\partial r_{Ai} \\over \\partial z_A} \\] The partial derivatives of rAi for each x, y, z coordinate are: \\[ {\\partial r_{Ai} \\over \\partial x_A} = {x_A-x_i \\over \\sqrt{(x_A-x_i)^2 + (y_A-y_i)^2 + (z_A-z_i)^2}} \\] \\[ {\\partial r_{Ai} \\over \\partial y_A} = {y_A-y_i \\over \\sqrt{(x_A-x_i)^2 + (y_A-y_i)^2 + (z_A-z_i)^2}} \\] \\[ {\\partial r_{Ai} \\over \\partial z_A} = {z_A-z_i \\over \\sqrt{(x_A-x_i)^2 + (y_A-y_i)^2 + (z_A-z_i)^2}} \\] Finally, the partial derivative of stretch energy with respect to bond distance rAi is: \\[ {\\partial U \\over \\partial r_{Ai}} = k_{Ai}(r_{Ai}-r_{Ai,eq}) \\]\n\n\n\nThe analytical period of a harmonic oscillator that models the interatomic bonds in this simulation is: \\[ {1 \\over \\mu} = {1 \\over m_1} + {1 \\over m_2} \\]\n\\[ T = \\sqrt{\\mu \\over k_b} \\] where mu is the reduced mass, m1 and m2 are the masses of the atoms (in kg) and kb is the spring constant of the bond. T is the period of the oscillation in seconds. While I do not use this equation in the simulation, I do use it to verify the simulation is producing physically valid output.\n\n\n\nTo propagate the trajectory over time, I use the velocity Verlet algorithm. At each time step, I perform the algorithm for each atom. The steps of the algorithm are as follows: \\[ \\mathbf x_{t+1} = \\mathbf x_t + \\mathbf {v}_t \\Delta t + \\mathbf a_t (\\Delta t)^2 \\] \\[ \\mathbf v_{t+{1 \\over 2}} = \\mathbf v_t + {1 \\over 2} \\mathbf a_i \\Delta t \\] \\[ \\mathbf a_{i+1} = -{\\mathbf \\nabla_i \\over m} \\] \\[ \\mathbf v_{i+1} = \\mathbf v_{t+{1 \\over 2}} + {1 \\over 2}\\mathbf a_{i+1} \\Delta t \\]\n\n\n\nAs diagnostic metrics to assess the proper behavior of the simulation, I calculate the kinetic, potential, and total energy of the system at each time step. For the kinetic energy, I just sum over the kinetic energies of all the atoms to arrive at the total kinetic energy Ek: \\[ E_k = \\sum_i^N {1 \\over 2} m_i v_i^2 \\] Since the only energy is stretch energy in this simulation, I sum over the stretch energy over all atoms to find the total potential energy Ep: \\[ E_p = \\sum_i^N U_i \\]"
  },
  {
    "objectID": "posts/moldyn/index.html#background",
    "href": "posts/moldyn/index.html#background",
    "title": "Julia: Molecular Dynamics of HCl",
    "section": "",
    "text": "Molecular dynamics models the motion of atoms within molecules using classical mechanics calculations. Molecular dynamics envisions atoms as balls on springs and calculates the accelerations of atoms within the system in a potential energy force field that has terms for several types of motions in the molecule. In this post, I will use the letter U to denote potential energy. Typical force fields model the following potentials: \\[ U = U_{stretch} + U_{angle} + U_{dihedral} + U_{improper} \\] For a complete description of the force field terms and motions, I refer the reader to Chapter 3 of the Cramer text referenced at the bottom of this document. In this project, I only model bond stretch energy so I could keep the code simple. The notation I will use in this post and code mostly comes from Chapter 3 of the Cramer text.\n\n\nThe stretch potential energy of a bond between atoms A and i is denoted Ustretch and has the following form: \\[ U_{stretch} = {1 \\over 2}k_{Ai}(r_{Ai}-r_{Ai,eq})^2 \\] Where kAi is the spring constant of the bond, rAi,eq is the equilibrium distance between atoms A and B and rAB is the instantaneous distance between the atoms. Since I am only modeling the stretch energy in this project, I will shorten Ustretch to U from here onward.\nThe first derivative of the stretch energy which is used to calculate the stretch forces acting upon an atom by the other atoms it is bonded to is: \\[ {\\partial U \\over \\partial x_A}=\\sum_{i} {\\partial U \\over \\partial r_{Ai}} {\\partial r_{Ai} \\over \\partial x_A} \\] Where the summation is over the atoms bonded to A and i refers to the other atoms. Similarly to x axis potential, the y and z potentials are: \\[ {\\partial U \\over \\partial y_A}=\\sum_{i} {\\partial U \\over \\partial r_{Ai}} {\\partial r_{Ai} \\over \\partial y_A} \\] \\[ {\\partial U \\over \\partial z_A}=\\sum_{i} {\\partial U \\over \\partial r_{Ai}} {\\partial r_{Ai} \\over \\partial z_A} \\] The partial derivatives of rAi for each x, y, z coordinate are: \\[ {\\partial r_{Ai} \\over \\partial x_A} = {x_A-x_i \\over \\sqrt{(x_A-x_i)^2 + (y_A-y_i)^2 + (z_A-z_i)^2}} \\] \\[ {\\partial r_{Ai} \\over \\partial y_A} = {y_A-y_i \\over \\sqrt{(x_A-x_i)^2 + (y_A-y_i)^2 + (z_A-z_i)^2}} \\] \\[ {\\partial r_{Ai} \\over \\partial z_A} = {z_A-z_i \\over \\sqrt{(x_A-x_i)^2 + (y_A-y_i)^2 + (z_A-z_i)^2}} \\] Finally, the partial derivative of stretch energy with respect to bond distance rAi is: \\[ {\\partial U \\over \\partial r_{Ai}} = k_{Ai}(r_{Ai}-r_{Ai,eq}) \\]\n\n\n\nThe analytical period of a harmonic oscillator that models the interatomic bonds in this simulation is: \\[ {1 \\over \\mu} = {1 \\over m_1} + {1 \\over m_2} \\]\n\\[ T = \\sqrt{\\mu \\over k_b} \\] where mu is the reduced mass, m1 and m2 are the masses of the atoms (in kg) and kb is the spring constant of the bond. T is the period of the oscillation in seconds. While I do not use this equation in the simulation, I do use it to verify the simulation is producing physically valid output.\n\n\n\nTo propagate the trajectory over time, I use the velocity Verlet algorithm. At each time step, I perform the algorithm for each atom. The steps of the algorithm are as follows: \\[ \\mathbf x_{t+1} = \\mathbf x_t + \\mathbf {v}_t \\Delta t + \\mathbf a_t (\\Delta t)^2 \\] \\[ \\mathbf v_{t+{1 \\over 2}} = \\mathbf v_t + {1 \\over 2} \\mathbf a_i \\Delta t \\] \\[ \\mathbf a_{i+1} = -{\\mathbf \\nabla_i \\over m} \\] \\[ \\mathbf v_{i+1} = \\mathbf v_{t+{1 \\over 2}} + {1 \\over 2}\\mathbf a_{i+1} \\Delta t \\]\n\n\n\nAs diagnostic metrics to assess the proper behavior of the simulation, I calculate the kinetic, potential, and total energy of the system at each time step. For the kinetic energy, I just sum over the kinetic energies of all the atoms to arrive at the total kinetic energy Ek: \\[ E_k = \\sum_i^N {1 \\over 2} m_i v_i^2 \\] Since the only energy is stretch energy in this simulation, I sum over the stretch energy over all atoms to find the total potential energy Ep: \\[ E_p = \\sum_i^N U_i \\]"
  },
  {
    "objectID": "posts/moldyn/index.html#methods",
    "href": "posts/moldyn/index.html#methods",
    "title": "Julia: Molecular Dynamics of HCl",
    "section": "Methods",
    "text": "Methods\nIn this section I describe the code that I wrote to run the simulation.\n\n1H35Cl Properties\nIn this simulation, I model 1H35Cl, as it is a classic molecule used to learn physical chemistry. The values I use to model the bond motion are:\n\n\n\n\n\n\n\n\n\nQuantity\nVariable\nValue\nUnits\n\n\n\n\nspring constant\nkAi\n516.0\nN/m\n\n\nequilibrium bond distance\nrAi,eq\n1.57 x 10-10\nm\n\n\nmass of hydrogen 1 atom\nm1\n1\namu\n\n\nmass of chlorine 35 atom\nm2\n35\namu\n\n\n\nkAi and rAi,eq are from Atkins and de Paula (below), page 454. Masses for 1H35Cl are converted from atomic mass units to kilograms in the simulation code. The analytical period of oscillation of the 1H35Cl bond is 11 fs, as calcualted below. This value will be used to analyze the period of oscillation in the results and discussion sections.\n\n\nDefine Constants\nkg_per_amu converts amu to kg. num_steps is the total number of time steps to run the simulation (delta t is 1 x 10-18 s, as will be explained below.)\n\n\nCode\nkg_per_amu = 1.661e-27\nnum_steps = 25000;  # Semicolon to suppress code output from final document\n\n\n\n\nVectors and matrices holding simulation information.\n\n\nCode\n# Positions (xs), velocities (vs), and accelerations (accels) arrays:\n# First axis is timestep\n# Second axis are atoms\n# Third axis is x,y,z (meters for position, m/s for velocities)\n\nvs = zeros(Float64, num_steps, 2, 3)\nxs = zeros(Float64, num_steps, 2, 3)\naccels = zeros(Float64, num_steps, 2, 3)\n\n# Masses: The masses of each atom (kg)\nms = zeros(Float64, 2)\n\n# Total kinetic energies: The total kinetic energy of the system\n# at each timestep.\n\ntkes = zeros(Float64, num_steps)\n\n# Total potential energies: Total potential energy of the system\n# at each timestep\n\ntpes = zeros(Float64, num_steps);  # Semicolon to suppress code output from final document\n\n\n\n\nInitialize Simulation\nSet the initial conditions of the simulation, including start positions and velocities of the atoms and their masses. Also set the topology to the 1H35Cl bond here.\n\n\nCode\n# Equilibrium bond length for HCl\nr_ab_eq_hcl = 1.57e-10\n\n# Assume Cl is at 0,0,0 and H lies along the x-axis\n\n# HCl equilibrium bond length\nxs[1, 2, :] = [r_ab_eq_hcl*0.999, 0.0, 0.0]\n\n# Masses, Cl first then H\nms[1] = 35 * kg_per_amu\nms[2] = 1 * kg_per_amu\n\n# 1-2 Bonds\n# Rows are bonds, columns are atoms participating in bond\n# Note: This is specifying edges on a graph, so 1-2 also has 2-1\n\none_two_bonds = [1 2; 2 1]\n\n# 1-2 Bonds, stretch constants\n# Note: There is the same constant for each direction of the bond\n# HCl bond constant 516 N/m according to Atkins and de Paula, pg. 454\n\none_two_bonds_kab = [516.0 516.0]\n\n# 1-2 Bonds, equilibrium distances\n# Note: There is a distance for each direction of the bond\n\none_two_bonds_req = [r_ab_eq_hcl r_ab_eq_hcl];  # Semicolon to suppress code output from final document\n\n\n\n\nFunctions to Run the Simulation\nWhile the code above defines the data for the simulation to operate on, these functions are what perform the calculations on those data.\n\n\nCode\n# Distance from atom a to b\nfunction r_ab(a, b)\n    sqrt(sum((a-b).^2))\nend\n\n# Stretch energy for a 1-2 bond\nfunction stretch_energy(a, b, k_ab, r_ab_eq) \n    0.5*k_ab*(r_ab(a, b)-r_ab_eq)^2\nend\n\n# Kinetic energy for an atom\nfunction kinetic_energy(vs::Array{Float64}, ms::Array{Float64}, timestep, atom)\n    v = vs[timestep, atom, :]\n    m = ms[atom]\n    0.5 * m * sum(v.^2)\nend\n\n# Stretch energy gradient for a single 1-2 bond\nfunction stretch_gradient(a, b, k_ab, r_ab_eq)\n    du_drab = 0.5 * k_ab * (2*r_ab(a, b)-2*r_ab_eq)\n    drab_dxa = (a[1]-b[1])/r_ab(a, b)\n    drab_dya = (a[2]-b[2])/r_ab(a, b)\n    drab_dza = (a[3]-b[3])/r_ab(a, b)\n\n    [drab_dxa, drab_dya, drab_dza] * du_drab\nend\n\n# Propagate 1-2 bond stretch trajectories\nfunction stretch_velocity_verlet(xs, vs, accels, tkes, tpes, one_two_bonds, one_two_bonds_kab, one_two_bonds_req, ms, dt, num_steps)\n    for time_i in 1:num_steps-1\n        for bond_i in [1 2]\n            k_ab = one_two_bonds_kab[bond_i]\n            r_eq = one_two_bonds_req[bond_i]\n            atom_a = one_two_bonds[bond_i, 1]\n            atom_b = one_two_bonds[bond_i, 2]\n            xs[time_i+1, atom_a, :] = xs[time_i, atom_a, :] + vs[time_i, atom_a, :] * dt + accels[time_i, atom_a, :] * dt^2\n            v_mid = vs[time_i, atom_a, :] + 0.5 * accels[time_i, atom_a, :] * dt\n            accels[time_i+1, atom_a, :] = -stretch_gradient(xs[time_i, atom_a, :], xs[time_i, atom_b, :], k_ab, r_eq) / ms[atom_a]\n            vs[time_i+1, atom_a, :] = v_mid + 0.5 * accels[time_i+1, atom_a, :] * dt\n        end\n\n        tkes[time_i] = total_kinetic_energy(vs, ms, time_i)\n        tpes[time_i] = total_stretch_energy(xs, one_two_bonds, one_two_bonds_kab, one_two_bonds_req, time_i)\n    end\n\n    tkes[num_steps] = total_kinetic_energy(vs, ms, num_steps)\n    tpes[num_steps] = total_stretch_energy(xs, one_two_bonds, one_two_bonds_kab, one_two_bonds_req, num_steps)\n\n    return nothing\nend\n\n# Determine the total kinetic energy of the system\nfunction total_kinetic_energy(vs::Array{Float64}, ms::Array{Float64}, timestep::Int)\n    sum([kinetic_energy(vs, ms, timestep, atom) for atom in eachindex(ms)])\nend\n\n# Determine total stretch energy of the system.\nfunction total_stretch_energy(xs::Array{Float64}, one_two_bonds::Array{Int}, one_two_bonds_kab::Array{Float64}, one_two_bonds_req::Array{Float64}, timestep::Int)\n    stretch_energies = zeros(Float64, length(one_two_bonds_kab))\n    \n    for i in eachindex(one_two_bonds_kab)\n        atom_a = one_two_bonds[i, 1]\n        atom_b = one_two_bonds[i, 2]\n        k_ab = one_two_bonds_kab[i]\n        r_eq = one_two_bonds_req[i]\n        pos_a = xs[timestep, atom_a, 1:3]\n        pos_b = xs[timestep, atom_b, 1:3]\n        stretch_energies[i] = stretch_energy(pos_a, pos_b, k_ab, r_eq)\n    end\n\n    # Divide by 2.0 to prevent double counting the 1-2 bonds\n    sum(stretch_energies) / 2.0\nend;  # Semicolon to suppress code output from final document\n\n\n\n\nVelocity Verlet\nNow for the fun part: Run the simulation! The following code block calls the functions to operate on the data above and calculate the trajectory of 1H35Cl.\n\n\nCode\ndt = 1e-18\nstretch_velocity_verlet(xs, vs, accels, tkes, tpes, one_two_bonds, one_two_bonds_kab, one_two_bonds_req, ms, dt, num_steps);  # Semicolon to suppress code output from final document"
  },
  {
    "objectID": "posts/moldyn/index.html#calculate-analytical-period",
    "href": "posts/moldyn/index.html#calculate-analytical-period",
    "title": "Julia: Molecular Dynamics of HCl",
    "section": "Calculate Analytical Period",
    "text": "Calculate Analytical Period\nAs referenced above and as a check of the simulation, calculate the analytical period of the oscillation of the 1H35Cl bond. Units are in seconds.\n\n\nCode\nμ = 1/(1/ms[1]+1/ms[2])\n2π * sqrt(μ/one_two_bonds_kab[1])\n\n\n1.1115336262465321e-14"
  },
  {
    "objectID": "posts/moldyn/index.html#results",
    "href": "posts/moldyn/index.html#results",
    "title": "Julia: Molecular Dynamics of HCl",
    "section": "Results",
    "text": "Results\n\nX Coordinate of Hydrogen\nSince the 1H35Cl molecule lies on the x axis, to find the period of oscillation we can look at plot of the hydrogen atom’s x-coordinate versus time, as shown below:\n\n\nCode\nx_axis = eachindex(xs[:, 2, 1]) / 1000\nh_x_axis_trajectory = xs[:, 2, 1]\nhydrogen_plot = plot(x_axis, h_x_axis_trajectory, xlabel=\"Time (fs)\", ylabel=\"H x position (m)\", legend=false)\ndisplay(hydrogen_plot)\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot shows the period of oscillation is 11 fs, which agrees with the analytical calculation above.\n\n\nKinetic, Potential, and Total Energy\nIdeally, at each time step the total energy of the system should remain constant. To verify this, the following plot displays the kinetic, potential, and total energy in the simulation (please ignore the warning messages about plot ticks):\n\n\nCode\nplot(x_axis, tkes, xlabel=\"Time (fs)\", ylabel=\"J\", label=\"Ek\")\nplot!(x_axis, tpes, label=\"Ep\")\nenergy_plot = plot!(x_axis, tpes + tkes, label=\"total\")\ndisplay(energy_plot)\n\n\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194\n┌ Warning: No strict ticks found\n└ @ PlotUtils ~/.julia/packages/PlotUtils/dVEMd/src/ticks.jl:194"
  },
  {
    "objectID": "posts/moldyn/index.html#discussion",
    "href": "posts/moldyn/index.html#discussion",
    "title": "Julia: Molecular Dynamics of HCl",
    "section": "Discussion",
    "text": "Discussion\nThis was my first project in the Julia programming language, and I am pleased with the results. The analytical period agrees with the simulation period of 1H35Cl. I find that very exciting! However, the total energy of the system increases slightly over the run of the trajectory, which means that energy is not being conserved by the algorithm. To combat this upward energy creep, I made the time step 1 x 10-18 s. This seemed to help the energy creep, but did not ameliorate the situation entirely.\nFuture directions of this work could include:\n\nIncorporating Uangle energy and simulating 1H216O.\nSimulating multiple molecules at once.\n\nThanks for reading!"
  },
  {
    "objectID": "posts/moldyn/index.html#references",
    "href": "posts/moldyn/index.html#references",
    "title": "Julia: Molecular Dynamics of HCl",
    "section": "References",
    "text": "References\nAtkins, P. W. & De Paula, J. Atkins’ Physical Chemistry. (W.H. Freeman, New York, 2006).\nCramer, C. J. Essentials of Computational Chemistry: Theories and Models. (Wiley, Chichester, West Sussex, England ; Hoboken, NJ, 2004)."
  },
  {
    "objectID": "posts/multivariate_normal_distribution/index.html",
    "href": "posts/multivariate_normal_distribution/index.html",
    "title": "Julia: Multivariate Normal Distribution",
    "section": "",
    "text": "Plot of multivariate normal distribution."
  },
  {
    "objectID": "posts/multivariate_normal_distribution/index.html#simple-multivariate-normal-distribution",
    "href": "posts/multivariate_normal_distribution/index.html#simple-multivariate-normal-distribution",
    "title": "Julia: Multivariate Normal Distribution",
    "section": "Simple Multivariate Normal Distribution",
    "text": "Simple Multivariate Normal Distribution\n\nDefine the distribution\n\nμ = [0.0, 0.0]  # Means\nΣ = [1.0 0.0; 0.0 1.0]  # Covariance matrix--off diagonal 0.0, directions uncorrelated.\nmv_gaussian = MvNormal(μ, Σ);  # ; To suppress output from cell\n\n\n\nPlot the PDF as a countour plot\n\n# Create a grid of points for the x and y axes\nxs = range(start=-3, stop=3, length=100)\nys = range(start=-3, stop=3, length=100)\n\n# Compute the PDF values over the grid\nzs = [pdf(mv_gaussian, [x, y]) for x in xs, y in ys]\n\nfig = Figure(resolution = (775, 700))\nax = Axis(fig[1, 1])\ncontour_plot = CairoMakie.contour!(ax, xs, ys, zs, levels=20, colormap=:viridis, linewidth = 3)\nColorbar(fig[1, 2], limits=(0, maximum(zs)), colormap=:viridis, flipaxis=false, size=50)\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\n\nPlot 10,000 random samples as a histogram\n\nsamples = rand(mv_gaussian, 100000)\nx_samples = samples[1, :]\ny_samples = samples[2, :]\n\nhistogram2d(x_samples, y_samples, nbins=(100, 100), colormap=:viridis, normalize=true, size=(700, 600))"
  },
  {
    "objectID": "posts/mm_basic/index.html",
    "href": "posts/mm_basic/index.html",
    "title": "Julia: Michaelis-Menten Kinetics",
    "section": "",
    "text": "Basic Michaelis-Menten enzyme kinetics."
  },
  {
    "objectID": "posts/mm_basic/index.html#chemical-and-rate-equations-and-units",
    "href": "posts/mm_basic/index.html#chemical-and-rate-equations-and-units",
    "title": "Julia: Michaelis-Menten Kinetics",
    "section": "Chemical and Rate Equations and Units",
    "text": "Chemical and Rate Equations and Units\nReaction:\n\\[ E + S \\leftrightharpoons ES \\rightarrow E + P\\]\nReversible rates are k+1 and k-1. The final irreversible step has rate kcat.\nSee Wikipedia for details.\nRate equation: \\[ v = {V_{\\max}[S] \\over K_M + [S]} \\]\n\n\n\n\n\n\n\n\nVariable\nUnits\nMeaning\n\n\n\n\nv\nmin-1\nRate of formation of product\n\n\nVmax\nmin-1\nMaximum possible rate of product formation\n\n\n[S]\nM\nConcentration of substrate\n\n\nKM\nM\nConcentration of substrate at 1/2 Vmax"
  },
  {
    "objectID": "posts/mm_basic/index.html#plot-v-vs.-s",
    "href": "posts/mm_basic/index.html#plot-v-vs.-s",
    "title": "Julia: Michaelis-Menten Kinetics",
    "section": "Plot v vs. [S]",
    "text": "Plot v vs. [S]\nHover over the traces to see the exact coordinates!\n\n\nCode\nsubstrate_concentrations = collect(range(start=0, stop=1e-2, length=100))\n\nfunction mm_curve(vmax, km)\n    function mm(substrate_concentration)\n        vmax * substrate_concentration / (km + substrate_concentration)\n    end\n\n    dP_dts = mm.(substrate_concentrations)\n    ytick_vals = range(start=0, stop=maximum(dP_dts), length=5)\n    ytick_labels = [@sprintf(\"%.3e\", val) for val in ytick_vals]\n\n    Dict(:dP_dts =&gt; dP_dts, :ytick_vals =&gt; ytick_vals, :ytick_labels =&gt; ytick_labels)\nend\n\nkms = [1e-3, 2e-3, 4e-3]\nlabels = [\"Km=1e-3\" \"Km=2e-3\" \"Km=4e-3\"]\ncurves = [mm_curve(1.0e-4, km) for km in kms]\nys = [curve[:dP_dts] for curve in curves]\n\nxtick_vals = range(start=0, stop=maximum(substrate_concentrations), length=5)\nxtick_labels = [@sprintf(\"%.3e\", val) for val in xtick_vals]\n\nytick_vals = curves[1][:ytick_vals]\nytick_labels = curves[1][:ytick_labels]\n\nplot(substrate_concentrations, ys, label=labels, xlims=(0, 1.1e-2), xlabel=\"[S] (M)\", ylabel=\"v (M/min)\", xticks=(xtick_vals, xtick_labels), yticks=(ytick_vals, ytick_labels), linewidth=3)"
  },
  {
    "objectID": "posts/hospital-visits/index.html",
    "href": "posts/hospital-visits/index.html",
    "title": "PowerBI: Hospital Visits",
    "section": "",
    "text": "You’ve been asked to build a high-level KPI report for the executive team, based on a subset of patient records. The purpose of the report is to give stakeholders visibility into the hospital’s recent performance, and answer the following questions:\n\nHow many patients have been admitted or readmitted over time?\nHow long are patients staying in the hospital, on average?\nHow much is the average cost per visit?\nHow many procedures are covered by insurance?"
  },
  {
    "objectID": "posts/hospital-visits/index.html#suggested-analysis",
    "href": "posts/hospital-visits/index.html#suggested-analysis",
    "title": "PowerBI: Hospital Visits",
    "section": "",
    "text": "You’ve been asked to build a high-level KPI report for the executive team, based on a subset of patient records. The purpose of the report is to give stakeholders visibility into the hospital’s recent performance, and answer the following questions:\n\nHow many patients have been admitted or readmitted over time?\nHow long are patients staying in the hospital, on average?\nHow much is the average cost per visit?\nHow many procedures are covered by insurance?"
  },
  {
    "objectID": "posts/hospital-visits/index.html#power-bi-dashboards",
    "href": "posts/hospital-visits/index.html#power-bi-dashboards",
    "title": "PowerBI: Hospital Visits",
    "section": "Power BI dashboards",
    "text": "Power BI dashboards\n\nDashboard 1\n\n\n\nDashboard 2"
  },
  {
    "objectID": "posts/hospital-visits/index.html#results",
    "href": "posts/hospital-visits/index.html#results",
    "title": "PowerBI: Hospital Visits",
    "section": "Results",
    "text": "Results\n\nThis is an analysis of a synthetic data set created to simulate a hospital in Massachusetts.\nOver the span of years from Jan 2011 to Jan 2022, there were 27,900 encounters, 974 distinct patients, the average claim was $3,640, encounters averaged approximately 7 hours 18 minutes, and 68% of encounters were insured.\nWhile ambulatory encounters were most common and drove hourly encounter trends, inpatient encounters took the most time on average.\nThe average encounter time of 7 hours 18 minutes was increased by a spike of encounters in February 2014, a year with encounters longer than average.\nMedicare, Medicaid, Humana, and Aetna were the top four insurance carriers, accounting for 63% of encounters.\n37% of encounters, however, were uninsured.\nOther insurance carriers accounted for the other 10% of encounters.\nThe top procedures were assessments of health and social needs, substance use, depression screening, and renal dialysis.\n70% of patients were white, 17% were Black, 9% Asian, with the balance being other races.\nThe hospital served patients from around the Boston Metropolitan Area."
  },
  {
    "objectID": "posts/hospital-visits/index.html#citation",
    "href": "posts/hospital-visits/index.html#citation",
    "title": "PowerBI: Hospital Visits",
    "section": "Citation",
    "text": "Citation\n\nJason Walonoski, Mark Kramer, Joseph Nichols, Andre Quina, Chris Moesel, Dylan Hall, Carlton Duffett, Kudakwashe Dube, Thomas Gallagher, Scott McLachlan, Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record, Journal of the American Medical Informatics Association, Volume 25, Issue 3, March 2018, Pages 230–238, https://doi.org/10.1093/jamia/ocx079\nCurated by Maven Analytics"
  },
  {
    "objectID": "posts/hospital-visits/index.html#tools-used",
    "href": "posts/hospital-visits/index.html#tools-used",
    "title": "PowerBI: Hospital Visits",
    "section": "Tools Used",
    "text": "Tools Used\n\nPower BI"
  },
  {
    "objectID": "posts/two-non-interacting-particles/index.html",
    "href": "posts/two-non-interacting-particles/index.html",
    "title": "Python: Quantum mechanics: two non-interacting particles",
    "section": "",
    "text": "This code and these plots explore Molecular Modelling for Beginners, 2nd Ed. by Alan Hinchliffe Section 12.6, pages 181 to 184. In this Section, the author explores the concepts of particle indistinguishability, symmetric, and antisymmetric wavefunctions. I recommend you read the text for a complete treatment of these concepts. However, I cover the essentials here.\nWe can write a wavefunction and total energy for two non-interacting particles in a one-dimensional box as:\n\\[ \\psi_{n_A,n_B}(x_A, x_B) = \\frac{2}{L} \\sin\\Bigl(\\frac{n_A \\pi x_A}{L}\\Bigr) \\sin\\Bigl(\\frac{n_B \\pi x_B}{L}\\Bigr) \\]\n\\[ E_{n_A,n_B} = (n_{A}^2+n_{B}^2) \\frac{h^2}{8mL^2} \\]\n\\[ n_A, n_B = 1, 2, 3,\\dots \\]\n\\[ 0 \\leqslant x_A \\leqslant L \\]\n\\[ 0 \\leqslant x_B \\leqslant L \\]\nWe need a wavefunction which preserves an indistinguishability requirement. We achieve this by adding or subtracting two wavefunctions. Adding gives us a symmetric wavefunction; subtracting gives us an antisymmetric wavefunction. Symmetric total wavefunctions maintain their sign if we swap the underlying individual wavefunctions. On the other hand, antisymmetric wavefunctions change their sign when we switch the underlying individual wavefunctions. This gives us two more equations, each with an appropriate normalization constant.\nSymmetric for energy levels 1 and 2 (Eqn. 12.19 from Hinchliffe):\n\\[ \\psi_s(x_A, x_B) = \\sqrt\\frac{1}{2} \\bigl(\\psi_{1,2}(x_A, x_B) + \\psi_{2,1}(x_A, x_B)\\bigr) \\]\nAntisymmetric for energy levels 1 and 2 (Eqn. 12.20 from Hinchliffe):\n\\[ \\psi_a(x_A, x_B) = \\sqrt\\frac{1}{2} \\bigl(\\psi_{1,2}(x_A, x_B) - \\psi_{2,1}(x_A, x_B)\\bigr) \\]"
  },
  {
    "objectID": "posts/two-non-interacting-particles/index.html#python-code",
    "href": "posts/two-non-interacting-particles/index.html#python-code",
    "title": "Python: Quantum mechanics: two non-interacting particles",
    "section": "Python code",
    "text": "Python code\nHere is the main class that runs the plots.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom math import sin, sqrt, pi\n\nclass TwoNonInteractingInABox:\n    \"\"\"\n    This class models two non-interacting particles in a one dimensional box.\n    \"\"\"\n    \n    def __init__(self, mass=1, length=1):\n        \"\"\"\n        Mass and length can be in whatever units you like, but\n        make sure these units are consistent with each other\n        to give meaningful results.\n        \n        This class assumes the mass of each particle is the same.\n        \n        Parameters\n        ----------\n        mass: float\n            The mass of each particle\n            \n        length: float\n            The length of the one-dimensional box\n        \"\"\"\n        self.mass = mass\n        self.length = length\n    \n    def wavefunction(self, na, nb, xa, xb):\n        \"\"\"\n        Returns the value of the wavefunction of the two non-interacting\n        particles.\n        \n        For xa and xb:\n        \n        0 &lt; xa &lt; self.length\n        0 &lt; xb &lt; self.length\n        \n        Parameters\n        ----------\n        na: int\n            The quantum number of particle a.\n        \n        nb: int\n            The quantum number of particle b.\n            \n        xa: float\n            The position of particle a.\n        \n        xb: float\n            The position of particle b.\n        \n        Returns\n        -------\n        float\n            value of the wavefunction.\n        \"\"\"\n        return 2 / self.length * sin(na * pi * xa / self.length) * sin(nb * pi * xb / self.length)\n    \n    def symmetric(self, na, nb, xa, xb):\n        \"\"\"\n        Returns the value symmetric wavefunction of the two non-interacting\n        particles.\n        \n        na: int\n            Quantum number of particle a.\n            \n        nb: int\n            Quantum number of particle b.\n            \n        xa: float\n            Position of particle a.\n            \n        xb: float\n            Position of particle b\n            \n        Returns\n        -------\n        float\n            Value of the symmetric wavefunction\n        \"\"\"\n        return sqrt(0.5) * (self.wavefunction(na, nb, xa, xb) + self.wavefunction(nb, na, xa, xb))\n    \n    def antisymmetric(self, na, nb, xa, xb):\n        \"\"\"\n        Returns the value antisymmetric wavefunction of the two non-interacting\n        particles.\n        \n        na: int\n            Quantum number of particle a.\n            \n        nb: int\n            Quantum number of particle b.\n            \n        xa: float\n            Position of particle a.\n            \n        xb: float\n            Position of particle b\n            \n        Returns\n        -------\n        float\n            Value of the antisymmetric wavefunction\n        \"\"\"\n        return sqrt(0.5) * (self.wavefunction(na, nb, xa, xb) - self.wavefunction(nb, na, xa, xb))\n    \n    def prob_density(self, na, nb, symmetric=True, points=100):\n        \"\"\"\n        Returns all the arrays to plot surface or contour plots of the \n        probability density as a function of xa and xb (the positions of\n        each particle).\n        \n        Parameters\n        ----------\n        na: int\n            Quantum number of particle a\n            \n        nb: int\n            Quantum number of particle b\n            \n        symmetric: bool\n            True if the symmetric probability density is needed. False if the\n            antisymmetric desnity is needed.\n        \n        points: int\n            How many points along each axis to sample on the surface.\n        \n        Returns\n        -------\n        np.array, np.array, np.array\n            First array is the 1d array of xa points. Second array is 1d array of\n            xb points. Third array is two dimensional array of probability density\n            at the intersection of the positions of the two particles.\n        \"\"\"\n        xas = np.linspace(0.0, self.length, points)\n        xbs = np.linspace(0.0, self.length, points)\n        zs = np.zeros((points, points), np.float64)\n        for ixa, xa in enumerate(xas):\n            for ixb, xb in enumerate(xbs):\n                if symmetric:\n                    zs[ixa, ixb] = self.symmetric(na, nb, xa, xb) ** 2\n                else:\n                    zs[ixa, ixb] = self.antisymmetric(na, nb, xa, xb) ** 2\n        return xas, xbs, zs\n\n\nSymmetric wavefunction squared\nSurface plot and contour plot of the probability density of two interacting particles.\nThese plots expand upon Fig. 12.12 in Hinchliffe, with an important exception: These plots are of the symmetric wavefunction squared, which matches the caption of the Fig. 12.12 in Hinchliffe. However, the plot in the textbook is not of the squared wavefunction; rather, it is a plot of the wavefunction that is not squared, as can be seen in the book with the negative values on the contour lines. Hence, the plot in the book has a misleading caption. I follow the text’s caption in this example, which makes my contour plots disagree with image in the text’s figure.\n\nbox = TwoNonInteractingInABox(mass=1.0, length=1.0)\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), subplot_kw={\"projection\": \"3d\"})\nnas = [1, 2, 3]\nnbs = [1, 2, 3]\nfor ina, na in enumerate(nas):\n    for inb, nb in enumerate(nbs):\n        xas, xbs, zs = box.prob_density(na=na, nb=nb, symmetric=False)\n        xs, ys = np.meshgrid(xas, xbs)\n        ax = axs[ina, inb]\n        ax.set_xlabel('xa', size=15)\n        ax.set_ylabel('xb', size=15)\n        ax.set_zlabel(f'(Ψa{na},{nb})^2', size=15, color='b')\n        ax.set_title(f'na={na}, nb={nb}', size=20, color='r')\n        ax.plot_surface(ys, xs, zs, cmap=cm.coolwarm, linewidth=0, antialiased=True)\n\n\n\n\n\n\n\n\n\nbox = TwoNonInteractingInABox(mass=1.0, length=1.0)\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), sharex=True, sharey=True)\nnas = [1, 2, 3]\nnbs = [1, 2, 3]\nfor ina, na in enumerate(nas):\n    for inb, nb in enumerate(nbs):\n        xas, xbs, zs = box.prob_density(na=na, nb=nb, symmetric=False)\n        xs, ys = np.meshgrid(xas, xbs)\n        ax = axs[ina, inb]\n        ax.set_xlabel('xa', size=15)\n        ax.set_ylabel('xb', size=15)\n        ax.set_title(f'(Ψa{na},{nb})^2', size=15, color='b')\n        cs = ax.contour(ys, xs, zs, cmap=cm.coolwarm, antialiased=True)\n        ax.clabel(cs, inline=True, fontsize=10)\n\n\n\n\n\n\n\n\n\n\nAntisymmetric wavefunction squared\nSurface plot and contour plot of the probability density of two interacting particles.\nThese plots expand upon Fig. 12.13 in Hinchliffe, which depcits the square of the antisymmetric wavefunction. In contrast to Figure 12.12 in the text, the caption in Fig. 12.13 agrees with the square of wavefunction which is plotted. Hence the values in these plots match those in the text.\nNote in the plots where “na=nb,” the probability density is zero. This satisifies the Pauli exclusion principle for antisymmetric wavefunctions. Our antisymmetric probability densities go to zero when we attempt to place the two particles in the same quantum state.\n\nbox = TwoNonInteractingInABox(mass=1.0, length=1.0)\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), subplot_kw={\"projection\": \"3d\"})\nnas = [1, 2, 3]\nnbs = [1, 2, 3]\nfor ina, na in enumerate(nas):\n    for inb, nb in enumerate(nbs):\n        xas, xbs, zs = box.prob_density(na=na, nb=nb, symmetric=False)\n        xs, ys = np.meshgrid(xas, xbs)\n        ax = axs[ina, inb]\n        ax.set_xlabel('xa', size=15)\n        ax.set_ylabel('xb', size=15)\n        ax.set_zlabel(f'(Ψa{na},{nb})^2', size=15, color='b')\n        ax.set_title(f'na={na}, nb={nb}', size=20, color='r')\n        ax.plot_surface(ys, xs, zs, cmap=cm.coolwarm, linewidth=0, antialiased=True)\n\n\n\n\n\n\n\n\n\nbox = TwoNonInteractingInABox(mass=1.0, length=1.0)\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15), sharex=True, sharey=True)\nnas = [1, 2, 3]\nnbs = [1, 2, 3]\nfor ina, na in enumerate(nas):\n    for inb, nb in enumerate(nbs):\n        xas, xbs, zs = box.prob_density(na=na, nb=nb, symmetric=True)\n        xs, ys = np.meshgrid(xas, xbs)\n        ax = axs[ina, inb]\n        ax.set_xlabel('xa', size=15)\n        ax.set_ylabel('xb', size=15)\n        ax.set_title(f'(Ψs{na},{nb})^2', size=15, color='b')\n        cs = ax.contour(ys, xs, zs, cmap=cm.coolwarm, antialiased=True)\n        ax.clabel(cs, inline=True, fontsize=10)"
  },
  {
    "objectID": "posts/organicdl/index.html",
    "href": "posts/organicdl/index.html",
    "title": "PyTorch: Deep Learning Organic Chemistry",
    "section": "",
    "text": "Convolutional neural networks (CNNs) are a deep learning technology to use for classifying images. For this demonstration I used images from an introductory organic chemistry class. My problem was one of binary classification: could the CNN distinguish images with a structure called a benzene ring from images without a benzene ring? While I encountered challenges of working with a small dataset (with 205 images in each class), I did train the CNN to 76% accuracy on the test data (see complete metrics below)."
  },
  {
    "objectID": "posts/organicdl/index.html#introduction",
    "href": "posts/organicdl/index.html#introduction",
    "title": "PyTorch: Deep Learning Organic Chemistry",
    "section": "",
    "text": "Convolutional neural networks (CNNs) are a deep learning technology to use for classifying images. For this demonstration I used images from an introductory organic chemistry class. My problem was one of binary classification: could the CNN distinguish images with a structure called a benzene ring from images without a benzene ring? While I encountered challenges of working with a small dataset (with 205 images in each class), I did train the CNN to 76% accuracy on the test data (see complete metrics below)."
  },
  {
    "objectID": "posts/organicdl/index.html#methods",
    "href": "posts/organicdl/index.html#methods",
    "title": "PyTorch: Deep Learning Organic Chemistry",
    "section": "Methods",
    "text": "Methods\n\nTechnology used, source code, and image source\nI used PyTorch for this project. I obtained 410 images of molecular structures by taking photographs at various angles under many lighting conditions from the text Organic Chemistry, 6th Edition by John McMurry along with its accompanying student solutions manual. For each image, I cropped out all the text and converted the images to grayscale so that the classifier would not learn color as an indicator in the classification task.\n\n\nDataset description and classification problem\nA detailed description of the organic chemistry involved in the training, test and validation datasets is beyond the scope of this article; however, a quick explanation about the data, as well as why the data have meaning in chemistry applications, should prove useful in understanding the CNN setup described here. An important part of studying chemical molecules involves breaking them into smaller substructures and analyzing each substructure individually. By understanding how each part works in the entire chemical structure, one can understand the properties of the compound as a whole. One important strucuture in organic chemistry is called a “benzene ring”. When the ring is found in a standalone configuration, with no other atoms surrounding it, it forms the compound benzene. Here is a diagram of benzene:\n\n\nOther parts of a molecule can be attached to these benzene rings, as shown below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther compounds do not contain these rings, as seen in the following molecules:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe task for the CNN was binary classification to distinguish images of molecules with benzene rings from those without benzene rings.\n\n\n\nNetwork architecture\nBelow is a snippet of Python code that defines the network (the complete code is at the bottom of this document). Basically, it consists of frozen VGG16 layers with linear classification layers on top of the VGG16 layers.\nclass Net(nn.Module):\n    def __init__(self, pretrained=True, freeze=True):\n        super(Net, self).__init__()\n        self.vgg16_layers = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        if freeze:\n            for param in self.vgg16_layers.parameters():\n                param.requires_grad = False\n        dummy_input = torch.randn(1, 3, 256, 256) # Assume input size is 256x256\n        output_size = self.vgg16_layers(dummy_input).view(1, -1).shape[1]\n        self.classifier = nn.Sequential(\n            nn.Linear(output_size, 256),\n            nn.Linear(256, 128),\n            nn.Linear(128, 64),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.vgg16_layers(x)\n        x = torch.flatten(x, start_dim=1)\n        return self.classifier(x)\n\n\nSize of dataset\nIn each of the train and test datasets I kept the classes roughly balanced, as shown in the table below:\n\n\n\nSet\nPositive (benzene ring) class\nNegative class\n\n\n\n\nTrain\n151\n149\n\n\nTest\n57\n53\n\n\n\n\n\nInitial Training Loss History (Binary Crossentropy)\n\nFor the initial runs to determine how many epochs create the best validation loss, I trained the model for 50 epochs. Ultiamtely, I chose 9 epochs for the final training because that was the lowest training loss before it the model started to overfit."
  },
  {
    "objectID": "posts/organicdl/index.html#results",
    "href": "posts/organicdl/index.html#results",
    "title": "PyTorch: Deep Learning Organic Chemistry",
    "section": "Results",
    "text": "Results\n\nBinary classification scores:\n\n\n\nMetric\nScore\n\n\n\n\nAccuracy\n0.7636\n\n\nPrecision\n0.7067\n\n\nRecall\n0.9298\n\n\nF1\n0.8030\n\n\n\n\n\nFalse negative cases\n\n\n\n\n\n\n\nImage\nWhat the model got wrong\n\n\n\n\n\n-OH and -NO2 groups confused the model.\n\n\n\n-NO2 groups confused the model.\n\n\n\n-CH3 and -Cl groups confused the model.\n\n\n\n-OH and -NO2 groups confused the model.\n\n\n\n\n\nFalse positive cases\n\n\n\n\n\n\n\nImage\nWhat the model got wrong\n\n\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA pentagon with single lines does not represent alternating double bonds.\n\n\n\nA pentagon with single lines does not represent alternating double bonds.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA single double bond is not correct.\n\n\n\nThis one is way off base.\n\n\n\nA single double bond is not correct.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA single double bond is not correct.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA single double bond is not correct.\n\n\n\nA pentagon with an O in it is incorrect.\n\n\n\nA hexagon with single lines does not represent alternating double bonds.\n\n\n\nA single double bond is not correct.\n\n\n\nA single double bond is not correct.\n\n\n\nThis one is way off base.\n\n\n\nA double bond and a triple bond with a hexagon are not correct."
  },
  {
    "objectID": "posts/organicdl/index.html#discussion",
    "href": "posts/organicdl/index.html#discussion",
    "title": "PyTorch: Deep Learning Organic Chemistry",
    "section": "Discussion",
    "text": "Discussion\nFalse positives dominated the model’s classification errors, where it seemed to confuse single-line hexagons and pentagons for benzene rings. On the flase negative side, too many groups off the benzene ring confused the model into false negative cases. If I were to do this project again in the future, I would want to get many more images for training and testing. I think that this model had problems due to the small dataset."
  },
  {
    "objectID": "posts/organicdl/index.html#python-source",
    "href": "posts/organicdl/index.html#python-source",
    "title": "PyTorch: Deep Learning Organic Chemistry",
    "section": "Python source",
    "text": "Python source\nThis is the source code that trained the network in a Google Colab notebook. Note that I did a lot of performance monitoring (to ensure I was using the GPU to its full potential) that I did not show in this document.\n\"\"\"OrganicDL Classifier V03.ipynb\n\nAutomatically generated by Colab.\n\nOriginal file is located at\n    REDACTED\n\n## Organic DL v02: Classifying images of molecules with a VGG16 based classifier using PyTorch\n\nFirst, setup the environment by determining the CUDA version, installing the dependencies with pip, and verifying GPU availability.\n\"\"\"\n\n!nvcc --version\n\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') &gt;= 0:\n    print('Not connected to a GPU')\nelse:\n    print(gpu_info)\n\nfrom psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\nif ram_gb &lt; 20:\n    print('Not using a high-RAM runtime')\nelse:\n    print('You are using a high-RAM runtime!')\n\n!pip install torchmetrics\n\n\"\"\"### Import necessary modules\"\"\"\n\nimport os\nimport random\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom torchmetrics import Accuracy, Precision, Recall, F1Score\nimport torchvision.models as models\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom google.colab import drive\n\n\"\"\"### Report GPU environment\"\"\"\n\nprint(\"CUDA Available:\", torch.cuda.is_available())\nprint(\"CUDA Version:\", torch.version.cuda)\nprint(\"Current GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n\n\"\"\"### Force PyTorch to use CUDA device 0\"\"\"\n\n# Check if CUDA is available\nprint(\"CUDA available:\", torch.cuda.is_available())\n\n# If available, get device count and device names\nif torch.cuda.is_available():\n    print(\"Device count:\", torch.cuda.device_count())\n    for i in range(torch.cuda.device_count()):\n        print(f\"Device {i} name:\", torch.cuda.get_device_name(i))\n\n# Set default device to GPU if available, otherwise CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n# To force operations to use GPU, move tensors to the device\nx = torch.rand(5, 3)\nx = x.to(device)  # This moves the tensor to GPU if available\n\n# To verify a tensor is on GPU\nprint(\"Tensor is on CUDA:\", x.is_cuda)\n\n\"\"\"### Set the RNG seeds\"\"\"\n\ndef set_seed(seed=42):\n    \"\"\"Set all seeds to make results reproducible\"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n    print(f\"Random seed set as {seed}\")\n\nset_seed()\n\n\"\"\"### Mount Google Drive\n\nTraining, test data is on Google Drive.\n\"\"\"\n\ndrive.mount('/content/drive')\n\n\"\"\"### Prepare data augmentation and image loader\"\"\"\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(45),\n    transforms.ToTensor(),\n    transforms.Resize((256, 256)),\n])\n\ntest_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((256, 256)),\n])\n\ndataset_train = ImageFolder('/content/drive/MyDrive/Colab Data/OrganicDL Train Test/train', transform=train_transforms)\ndataset_test = ImageFolder('/content/drive/MyDrive/Colab Data/OrganicDL Train Test/test', transform=test_transforms)\ndataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=32, shuffle=True)\ndataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=True)\n\n\"\"\"### Look at sample images\n\nOne image each from the negative (non-benzene) and positive (benzene) classes.\n\"\"\"\n\nfirst_negative_image_idx = None\nfirst_positive_image_idx = None\n\nfor idx, (image, label) in enumerate(dataset_train):\n    if first_negative_image_idx is None and label == 0:\n        first_negative_image_idx = idx\n        break\n\nfor idx, (image, label) in enumerate(dataset_train):\n    if first_positive_image_idx is None and label == 1:\n        first_positive_image_idx = idx\n        break\n\nimage, label = dataset_train[first_negative_image_idx]\nplt.imshow(image.permute(1, 2, 0))\nplt.title(f\"Label: {label} (negative class)\")\nplt.savefig('/content/drive/MyDrive/Colab Data/OrganicDL Results/negative_class_image.png')\nplt.show()\n\nimage, label = dataset_train[first_positive_image_idx]\nplt.imshow(image.permute(1, 2, 0))\nplt.title(f\"Label: {label} (Positive Class)\")\nplt.savefig('/content/drive/MyDrive/Colab Data/OrganicDL Results/positive_class_image.png')\nplt.show()\n\n\"\"\"### Define the `Net` model class that will perform the classification\"\"\"\n\nclass Net(nn.Module):\n    def __init__(self, pretrained=True, freeze=True):\n        super(Net, self).__init__()\n        self.vgg16_layers = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        if freeze:\n            for param in self.vgg16_layers.parameters():\n                param.requires_grad = False\n        dummy_input = torch.randn(1, 3, 256, 256) # Assume input size is 256x256\n        output_size = self.vgg16_layers(dummy_input).view(1, -1).shape[1]\n        self.classifier = nn.Sequential(\n            nn.Linear(output_size, 256),\n            nn.Linear(256, 128),\n            nn.Linear(128, 64),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.vgg16_layers(x)\n        x = torch.flatten(x, start_dim=1)\n        return self.classifier(x)\n\n\"\"\"### Train the model, part 1\n\nTrain the model for a lot of epochs to find where it starts to overfit. Below, a fresh model will be trained for final evaluation.\n\"\"\"\n\ndef train_model(num_epochs=16):\n    # Enable benchmarking for better CUDA performance\n    torch.backends.cudnn.benchmark = True\n\n    # Make a new model\n    model = Net().to(device)\n\n    # Move criterion to device\n    criterion = nn.BCELoss().to(device)\n    print(f\"Model is on GPU: {next(model.parameters()).is_cuda}\")\n    optimizer = optim.Adam(model.parameters(), lr=1.0e-3)\n\n    # Print batch size information\n    print(f\"Batch size: {dataloader_train.batch_size}\")\n    print(f\"Training dataset size: {len(dataloader_train.dataset)}\")\n    print(f\"Steps per epoch: {len(dataloader_train)}\")\n\n    epoch_running_losses = []\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        # Reset GPU stats for this epoch\n        torch.cuda.reset_peak_memory_stats()\n\n        for i, data in enumerate(dataloader_train, 0):\n            inputs, labels = data\n            inputs = inputs.float().to(device)\n            labels = labels.float().view(-1, 1).to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n            # Monitor GPU usage every 10 batches\n            if i % 5 == 0:\n                print(f\"Batch {i}, GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n\n        epoch_running_loss = running_loss / len(dataloader_train)\n        epoch_running_losses.append(epoch_running_loss)\n        print(f\"Epoch {epoch + 1}, Loss: {epoch_running_loss}\")\n        print(f\"Peak GPU memory in epoch: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n\n    # Final GPU stats\n    print(f\"Final GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n\n    # Return the final model and training losses.\n    return model, epoch_running_losses\n\ninitial_model, initial_epoch_running_losses = train_model(num_epochs=50)\n\n\"\"\"### History of training losses\"\"\"\n\nxs = np.arange(1, len(initial_epoch_running_losses) + 1)\nplt.plot(xs, initial_epoch_running_losses)\nplt.title(\"Initial Training Loss History (Binary Crossentropy)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.savefig('/content/drive/MyDrive/Colab Data/OrganicDL Results/initial_training_loss_history.png')\nplt.show()\n\n\"\"\"### Train final model\n\nTrain the final model, stopping at the epoch where the model starts to overfit.\n\"\"\"\n\nfinal_model, final_epoch_running_losses = train_model(num_epochs=9)\n\n\"\"\"### Evaluate the final trained model\"\"\"\n\n# Move metrics to device\nmetric_precision = Precision(task=\"binary\").to(device)\nmetric_recall = Recall(task=\"binary\").to(device)\nmetric_f1 = F1Score(task=\"binary\").to(device)\nmetric_accuracy = Accuracy(task=\"binary\").to(device)\n\n# Prepare for evaluation\nfinal_model.eval()\ntorch.cuda.reset_peak_memory_stats()\neval_start = time.time()\n\n# Check dataloader batch size\nprint(f\"Evaluation batch size: {dataloader_test.batch_size}\")\nprint(f\"Test dataset size: {len(dataloader_test.dataset)}\")\nprint(f\"Evaluation steps: {len(dataloader_test)}\")\n\n# On-CPU list of images that are misclassified so they can be saved for\n# diagnostic purposes\nmisclassified_images = []\n\n# Main evaluation loop\nwith torch.no_grad():\n    for i, data in enumerate(dataloader_test, 0):\n        # Move data to GPU\n        inputs, labels = data\n        inputs = inputs.float().to(device)  # Ensure correct data type\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = final_model(inputs)\n\n        # Proper handling for binary classification\n        if outputs.shape[1] == 1:  # If output is [batch_size, 1] (sigmoid output)\n            preds = (outputs &gt; 0.5).int().squeeze()\n        else:  # If output is [batch_size, 2] (two-class output)\n            _, preds = torch.max(outputs, 1)\n\n        # Update metrics directly with tensors (keeping computation on GPU)\n        metric_precision.update(preds, labels)\n        metric_recall.update(preds, labels)\n        metric_f1.update(preds, labels)\n        metric_accuracy.update(preds, labels)\n\n        # Print sample predictions (only for first few batches to avoid excessive GPU-CPU transfers)\n        if i &lt; 2:\n            print(f\"\\nSample predictions from batch {i}:\")\n            for j in range(min(5, len(preds))):\n                print(f\"  Prediction: {preds[j].item()}, Label: {labels[j].item()}\")\n\n        # Add misclassified images/data to the list\n        misclassified = (preds != labels).nonzero(as_tuple=True)[0]\n        for idx in misclassified:\n            misclassified_images.append((inputs[idx].cpu(), preds[idx].item(), labels[idx].item()))\n\n        # Periodically report GPU memory usage\n        if i % 10 == 0:\n            print(f\"Batch {i}, GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n\n# Force synchronization to ensure GPU operations complete\ntorch.cuda.synchronize()\n\n# Compute final metrics (results stay on GPU until final .compute() call)\nprecision = metric_precision.compute().item()\nrecall = metric_recall.compute().item()\nf1 = metric_f1.compute().item()\naccuracy = metric_accuracy.compute().item()\n\n# Report performance\neval_time = time.time() - eval_start\nprint(\"\\nEvaluation Results:\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Evaluation completed in {eval_time:.2f} seconds\")\nprint(f\"Peak GPU memory: {torch.cuda.max_memory_allocated() / 1024**2:.2f} MB\")\n\n# Save misclassififed images\ncounter = 0\nfor misclassified_image in misclassified_images:\n    counter += 1\n    image, prediction, label = misclassified_image\n    plt.imshow(image.permute(1, 2, 0))\n    plt.title(f\"Prediction: {prediction}, True Label: {label}\")\n    plt.savefig(f'/content/drive/MyDrive/Colab Data/OrganicDL Results/misclassified_image_{prediction}_{label}_{counter}.png')\n    print(f\"Saved misclassified image {counter} with prediction {prediction} and label {label}\")"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html",
    "href": "posts/simple-gradient-descent/index.html",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "This is a simple polynomial gradient descent. It is a naive implmentation that is not optimized or vectorized. It is meant to be a simple demo on how gradient descent can be accomplished. To keep it simple, it uses base Python. I don’t intend for it to be used for anything important!\n\n\nThe “target” polynomial that will be fitted is defined as Eqn. 1:\n\\[ y_i = \\sum_{i=0}^{n} a_i x^i \\]\nThe “estimate” polynomial that will be fitted is defined the same way but with a hat of the y Eqn. 2:\n\\[ \\hat y_i = \\sum_{i=0}^{n} a_i x^i \\]\nTo fit the polynomial the code will minimize the residual sum of squares (RSS) function Eqn. 3:\n\\[ RSS = \\sum_{i=0}^{n} (y_i - \\hat y_i)^2 \\]\nGradient desscent needs the partial derivative of the RSS with respect to each ceofficient, which is Eqn. 4:\n\\[ {\\partial \\over \\partial a_k}  = -2 \\sum_{i=1}^n \\left(y_i-\\hat y_i\\right) x_i^{k} \\]\nTo define each polynomial, I will gather the coefficients into a vector Eqn. 5:\n\\[ \\mathbf a = [a_1, a_2, ..., a_i] \\]\n\n\n\nThe code in this example is built around functions, so I will define these functions in the first part of the notebook and use these functions in the second part of the notebook.\n\n\nSince I am not using numpy, I am making my own linspace function. Later, this function will make the x-axis to evaluate the polynomials with.\n\ndef linspace(min_val, max_val, npoints):\n    \"\"\"\n    Creates a list of floating point values between the minimum and\n    maximum values, with the number of points (+1) to cover the entire\n    span specified.\n    \n    Parameters\n    ----------\n    min_val\n        The minimum value in the range as a float.\n        \n    max_val\n        The maximum value in the range as a float.\n        \n    npoints\n        The number of points in the range as a float.\n        \n    Returns\n    -------\n    list[float]\n        A list of floating point values as specified.\n    \"\"\"\n    \n    return [min_val+(max_val-min_val)/npoints*i for i in range(npoints+1)]\n\n\n\n\nThis will evaluate a list of y values from a polynomial according to Eqns 1 and 2 with an x-axis and vector of coefficients.\n\ndef polynomial(x_axis, coeffs):\n    \"\"\"\n    Evaluates a polynomial along a given axis given coefficients.\n    \n    Parameters\n    ----------\n    x_axis:\n         A list of floats that is the x-axis to supply as x_i.\n         \n    coeffs:\n        The coefficients of the polynomial as defined in the vector\n        in Eqn. 4.\n        \n    Returns\n    -------\n    list[float]\n        Returns a list of floats that are the values of the polynomial.\n    \"\"\"\n    \n    ys = []\n    for x in x_axis:\n        y = 0.\n        for i, coeff in enumerate(coeffs):\n            y += coeff * x**i\n        ys.append(y)\n    return ys\n\n\n\n\nThis function will calculate the error between the target and estimate polynomials according to Eqn. 3.\n\ndef rss(x_axis, y_coeffs, y_hat_coeffs):\n    \"\"\"\n    Calculates the RSS as defined in Eqn 3 between the two polynomials \n    specified in Eqns 1 or 2.\n    \n    Parameters\n    ----------\n    x_axis\n        The x-axis as a list of floats with which to compare the \n        polynomials.\n        \n    y_coeffs:\n        The coefficients as a list of floats for the target polynomial.\n        \n    y_hat_coeffs\n        The coefficients as a list of floats for the estimate polynomial.\n        \n    Returns\n    -------\n    float\n        An RSS value between the target and estimate.\n    \"\"\"\n    \n    target_ys = polynomial(x_axis, y_coeffs)\n    estimate_ys = polynomial(x_axis, y_hat_coeffs)\n    return sum((target_y - estimate_y)**2 for target_y, estimate_y in zip(target_ys, estimate_ys))\n\n\n\n\nThis function calculates the components of the gradient of the RSS error between the target and estimate coefficients. It implements Eqn 4.\n\ndef gradient(x_axis, target_ys, estimate_ys, y_hat_coeffs):\n    \"\"\"\n    Calculates the gradient of the error between the target and estimate\n    polynomials and returns the components of the gradient in a list of\n    floats.\n    \n    Parameters\n    ----------\n    x_axis\n        The x axis as a list of floats.\n    \n    target_ys\n        List of floats that is the target polynomial y values.\n        \n    estimate_ys\n        List of floats that is the estimate polynomial y values.\n        \n    y_hat_coeffs\n        The estimate coefficients as a list of floats.\n        \n    Returns\n    -------\n    list[float]\n        The components of the gradient as a list of floats.\n    \"\"\"\n    \n    components = []\n    for k, _ in enumerate(y_hat_coeffs):\n        component = 0.\n        for i, (target_y, estimate_y) in enumerate(zip(target_ys, estimate_ys)):\n            component += (target_y - estimate_y) * x_axis[i] ** k\n        components.append(-2 * component)\n    return components\n\n\n\n\nThis function uses the gradient to iteravely refine the estimate coefficients to move the estimate closer to the target. It returns the history of the RSS values along the way.\n\ndef gradient_descent(x_axis, target_ys, target_coeffs, y_hat_coeffs_initial, learn_rate=1e-6, max_iter=1000, rss_threshold=50.):\n    \"\"\"\n    Performs gradient descent optimization to make the estimate coefficients\n    converge to values that give a polynomial function with a shape similar\n    to the target values.\n    \n    Training continues until max iterations are reached or the RSS diminishes\n    below the given threshold.\n    \n    Parameters\n    ----------\n    x_axis\n        List of floats that is the x-axis for the polynomials.\n        \n    target_ys\n        List of floats that is the target polynomial.\n        \n    y_hat_coeffs_initial\n        The initial guess for the estimate coefficients\n        \n    learn_rate\n        The rate at which to descend the gradient during fitting. Higher numbers\n        descend quicker but may not find the true minimum.\n        \n    max_iter\n        Integer of the maximum number of iterations the algorithm will attempt.\n        Used to prevent infinite loops.\n        \n    rss_threshold\n        If RSS diminishes below this threshold, training iterations will stop.\n        \n    Returns\n    -------\n    list[dict]\n        A list of dictionaries containing the training history at each iteration.\n    \"\"\"\n    \n    fit_history = []\n    y_hat_coeffs = y_hat_coeffs_initial[:]\n    for i in range(max_iter):\n        estimate_ys = polynomial(x_axis, y_hat_coeffs)\n        estimate_rss = rss(x_axis, target_coeffs, y_hat_coeffs)\n        fit_history.append({\n            'rss': estimate_rss,\n            'y_hat_coeffs': y_hat_coeffs[:]\n        })\n        if estimate_rss &lt; rss_threshold:\n            break\n        current_gradient = gradient(x_axis, target_ys, estimate_ys, y_hat_coeffs)\n        y_hat_coeffs = [y_hat_coeff-learn_rate*gi for y_hat_coeff, gi in zip(y_hat_coeffs, current_gradient)]\n    return fit_history\n\n\n\n\n\n\n\n\ncommon_x_axis = linspace(-5., 5., 100)\n\n\n\n\nThe target polynomial will be defined by a vector called target_coeffs. The coefficients for the initial estimated coefficients will be estimate_coeffs. target_coeffs will not change since they represent the truth in the estimate polynomial. The estiamte_coeffs will be iteratively updated as the estimate moves closer to the target.\n\ntarget_coeffs = [-2.0, 0.0, 2.5, 1.2]\nestimate_coeffs = [-2.5, 0.0, 2.0, -1.7]\n\n\n\n\n\n\n\nFor the remainder of this notebook, the target polynomial will be in blue and the estimate polynomial will be in orange. I will include the initial RSS in the title.\n\ntarget_0 = polynomial(common_x_axis, target_coeffs)\nestimate_0 = polynomial(common_x_axis, estimate_coeffs)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.scatter(common_x_axis, target_0, color='blue', alpha=0.6, label='target')\nax.scatter(common_x_axis, estimate_0, color='orange', alpha=0.6, label='estimate')\nax.legend()\n\n\n\n\n\n\n\n\nAs a reminder, here are the target coefficients and the initial estimate coefficients:\n\nprint(f'Target coefficients {target_coeffs}')\nprint(f'Initial estimate coefficients {estimate_coeffs}')\nprint(f'Initial RSS {rss(common_x_axis, target_coeffs, estimate_coeffs)}')\n\nTarget coefficients [-2.0, 0.0, 2.5, 1.2]\nInitial estimate coefficients [-2.5, 0.0, 2.0, -1.7]\nInitial RSS 2015004.0007105006\n\n\n\n\n\n\ngradient_descent_history = gradient_descent(common_x_axis, target_0, target_coeffs, estimate_coeffs)\n\n\n\n\nAs gradient descent fits the estimate coefficients, the RSS should drop with each iteration.\n\nrss_history = [step['rss'] for step in gradient_descent_history]\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.set_yscale('log')\nax.plot(list(range(len(rss_history))), rss_history, color='green')\nax.set_title(f'RSS vs Iteration')\nax.set_ylabel('RSS')\nax.set_xlabel('iteration')\n\nText(0.5, 0, 'iteration')\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a graphical representation of how close the fit is after training.\n\nestimate_final = polynomial(common_x_axis, gradient_descent_history[-1]['y_hat_coeffs'])\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.scatter(common_x_axis, target_0, color='blue', alpha=0.6, label='target')\nax.scatter(common_x_axis, estimate_final, color='orange', alpha=0.6, label='estimate')\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nThese are the numeric results of the training.\n\nfinal_estimate_coeffs = gradient_descent_history[-1]['y_hat_coeffs']\ninitial_rss = gradient_descent_history[0]['rss']\nprint(f'Training iterations {len(gradient_descent_history)}')\nprint(f'Target coefficients {target_coeffs}')\nprint(f'Final estimate coefficients {final_estimate_coeffs}')\nprint(f'Initial RSS {initial_rss}')\nprint(f'Final RSS {rss(common_x_axis, target_coeffs, final_estimate_coeffs)}')\n\nTraining iterations 88\nTarget coefficients [-2.0, 0.0, 2.5, 1.2]\nFinal estimate coefficients [-2.4649992311439837, 0.1551299749689758, 2.4784435198598582, 1.1914759552905416]\nInitial RSS 2015004.0007105006\nFinal RSS 48.4555948418813\n\n\n\n\n\n\n\nGöbel, Börge. Computational Physics, Section 3.\nJames, Gareth et al. An Introduction to Statistical Learning with Applications in R, Eqn 3.16."
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#some-equations",
    "href": "posts/simple-gradient-descent/index.html#some-equations",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "The “target” polynomial that will be fitted is defined as Eqn. 1:\n\\[ y_i = \\sum_{i=0}^{n} a_i x^i \\]\nThe “estimate” polynomial that will be fitted is defined the same way but with a hat of the y Eqn. 2:\n\\[ \\hat y_i = \\sum_{i=0}^{n} a_i x^i \\]\nTo fit the polynomial the code will minimize the residual sum of squares (RSS) function Eqn. 3:\n\\[ RSS = \\sum_{i=0}^{n} (y_i - \\hat y_i)^2 \\]\nGradient desscent needs the partial derivative of the RSS with respect to each ceofficient, which is Eqn. 4:\n\\[ {\\partial \\over \\partial a_k}  = -2 \\sum_{i=1}^n \\left(y_i-\\hat y_i\\right) x_i^{k} \\]\nTo define each polynomial, I will gather the coefficients into a vector Eqn. 5:\n\\[ \\mathbf a = [a_1, a_2, ..., a_i] \\]"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#custom-functions",
    "href": "posts/simple-gradient-descent/index.html#custom-functions",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "The code in this example is built around functions, so I will define these functions in the first part of the notebook and use these functions in the second part of the notebook.\n\n\nSince I am not using numpy, I am making my own linspace function. Later, this function will make the x-axis to evaluate the polynomials with.\n\ndef linspace(min_val, max_val, npoints):\n    \"\"\"\n    Creates a list of floating point values between the minimum and\n    maximum values, with the number of points (+1) to cover the entire\n    span specified.\n    \n    Parameters\n    ----------\n    min_val\n        The minimum value in the range as a float.\n        \n    max_val\n        The maximum value in the range as a float.\n        \n    npoints\n        The number of points in the range as a float.\n        \n    Returns\n    -------\n    list[float]\n        A list of floating point values as specified.\n    \"\"\"\n    \n    return [min_val+(max_val-min_val)/npoints*i for i in range(npoints+1)]\n\n\n\n\nThis will evaluate a list of y values from a polynomial according to Eqns 1 and 2 with an x-axis and vector of coefficients.\n\ndef polynomial(x_axis, coeffs):\n    \"\"\"\n    Evaluates a polynomial along a given axis given coefficients.\n    \n    Parameters\n    ----------\n    x_axis:\n         A list of floats that is the x-axis to supply as x_i.\n         \n    coeffs:\n        The coefficients of the polynomial as defined in the vector\n        in Eqn. 4.\n        \n    Returns\n    -------\n    list[float]\n        Returns a list of floats that are the values of the polynomial.\n    \"\"\"\n    \n    ys = []\n    for x in x_axis:\n        y = 0.\n        for i, coeff in enumerate(coeffs):\n            y += coeff * x**i\n        ys.append(y)\n    return ys\n\n\n\n\nThis function will calculate the error between the target and estimate polynomials according to Eqn. 3.\n\ndef rss(x_axis, y_coeffs, y_hat_coeffs):\n    \"\"\"\n    Calculates the RSS as defined in Eqn 3 between the two polynomials \n    specified in Eqns 1 or 2.\n    \n    Parameters\n    ----------\n    x_axis\n        The x-axis as a list of floats with which to compare the \n        polynomials.\n        \n    y_coeffs:\n        The coefficients as a list of floats for the target polynomial.\n        \n    y_hat_coeffs\n        The coefficients as a list of floats for the estimate polynomial.\n        \n    Returns\n    -------\n    float\n        An RSS value between the target and estimate.\n    \"\"\"\n    \n    target_ys = polynomial(x_axis, y_coeffs)\n    estimate_ys = polynomial(x_axis, y_hat_coeffs)\n    return sum((target_y - estimate_y)**2 for target_y, estimate_y in zip(target_ys, estimate_ys))\n\n\n\n\nThis function calculates the components of the gradient of the RSS error between the target and estimate coefficients. It implements Eqn 4.\n\ndef gradient(x_axis, target_ys, estimate_ys, y_hat_coeffs):\n    \"\"\"\n    Calculates the gradient of the error between the target and estimate\n    polynomials and returns the components of the gradient in a list of\n    floats.\n    \n    Parameters\n    ----------\n    x_axis\n        The x axis as a list of floats.\n    \n    target_ys\n        List of floats that is the target polynomial y values.\n        \n    estimate_ys\n        List of floats that is the estimate polynomial y values.\n        \n    y_hat_coeffs\n        The estimate coefficients as a list of floats.\n        \n    Returns\n    -------\n    list[float]\n        The components of the gradient as a list of floats.\n    \"\"\"\n    \n    components = []\n    for k, _ in enumerate(y_hat_coeffs):\n        component = 0.\n        for i, (target_y, estimate_y) in enumerate(zip(target_ys, estimate_ys)):\n            component += (target_y - estimate_y) * x_axis[i] ** k\n        components.append(-2 * component)\n    return components\n\n\n\n\nThis function uses the gradient to iteravely refine the estimate coefficients to move the estimate closer to the target. It returns the history of the RSS values along the way.\n\ndef gradient_descent(x_axis, target_ys, target_coeffs, y_hat_coeffs_initial, learn_rate=1e-6, max_iter=1000, rss_threshold=50.):\n    \"\"\"\n    Performs gradient descent optimization to make the estimate coefficients\n    converge to values that give a polynomial function with a shape similar\n    to the target values.\n    \n    Training continues until max iterations are reached or the RSS diminishes\n    below the given threshold.\n    \n    Parameters\n    ----------\n    x_axis\n        List of floats that is the x-axis for the polynomials.\n        \n    target_ys\n        List of floats that is the target polynomial.\n        \n    y_hat_coeffs_initial\n        The initial guess for the estimate coefficients\n        \n    learn_rate\n        The rate at which to descend the gradient during fitting. Higher numbers\n        descend quicker but may not find the true minimum.\n        \n    max_iter\n        Integer of the maximum number of iterations the algorithm will attempt.\n        Used to prevent infinite loops.\n        \n    rss_threshold\n        If RSS diminishes below this threshold, training iterations will stop.\n        \n    Returns\n    -------\n    list[dict]\n        A list of dictionaries containing the training history at each iteration.\n    \"\"\"\n    \n    fit_history = []\n    y_hat_coeffs = y_hat_coeffs_initial[:]\n    for i in range(max_iter):\n        estimate_ys = polynomial(x_axis, y_hat_coeffs)\n        estimate_rss = rss(x_axis, target_coeffs, y_hat_coeffs)\n        fit_history.append({\n            'rss': estimate_rss,\n            'y_hat_coeffs': y_hat_coeffs[:]\n        })\n        if estimate_rss &lt; rss_threshold:\n            break\n        current_gradient = gradient(x_axis, target_ys, estimate_ys, y_hat_coeffs)\n        y_hat_coeffs = [y_hat_coeff-learn_rate*gi for y_hat_coeff, gi in zip(y_hat_coeffs, current_gradient)]\n    return fit_history"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#define-the-target-and-estimate-polynomials",
    "href": "posts/simple-gradient-descent/index.html#define-the-target-and-estimate-polynomials",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "common_x_axis = linspace(-5., 5., 100)\n\n\n\n\nThe target polynomial will be defined by a vector called target_coeffs. The coefficients for the initial estimated coefficients will be estimate_coeffs. target_coeffs will not change since they represent the truth in the estimate polynomial. The estiamte_coeffs will be iteratively updated as the estimate moves closer to the target.\n\ntarget_coeffs = [-2.0, 0.0, 2.5, 1.2]\nestimate_coeffs = [-2.5, 0.0, 2.0, -1.7]"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#fitting-the-polynomial",
    "href": "posts/simple-gradient-descent/index.html#fitting-the-polynomial",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "For the remainder of this notebook, the target polynomial will be in blue and the estimate polynomial will be in orange. I will include the initial RSS in the title.\n\ntarget_0 = polynomial(common_x_axis, target_coeffs)\nestimate_0 = polynomial(common_x_axis, estimate_coeffs)\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.scatter(common_x_axis, target_0, color='blue', alpha=0.6, label='target')\nax.scatter(common_x_axis, estimate_0, color='orange', alpha=0.6, label='estimate')\nax.legend()\n\n\n\n\n\n\n\n\nAs a reminder, here are the target coefficients and the initial estimate coefficients:\n\nprint(f'Target coefficients {target_coeffs}')\nprint(f'Initial estimate coefficients {estimate_coeffs}')\nprint(f'Initial RSS {rss(common_x_axis, target_coeffs, estimate_coeffs)}')\n\nTarget coefficients [-2.0, 0.0, 2.5, 1.2]\nInitial estimate coefficients [-2.5, 0.0, 2.0, -1.7]\nInitial RSS 2015004.0007105006\n\n\n\n\n\n\ngradient_descent_history = gradient_descent(common_x_axis, target_0, target_coeffs, estimate_coeffs)\n\n\n\n\nAs gradient descent fits the estimate coefficients, the RSS should drop with each iteration.\n\nrss_history = [step['rss'] for step in gradient_descent_history]\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.set_yscale('log')\nax.plot(list(range(len(rss_history))), rss_history, color='green')\nax.set_title(f'RSS vs Iteration')\nax.set_ylabel('RSS')\nax.set_xlabel('iteration')\n\nText(0.5, 0, 'iteration')\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a graphical representation of how close the fit is after training.\n\nestimate_final = polynomial(common_x_axis, gradient_descent_history[-1]['y_hat_coeffs'])\nfig, ax = plt.subplots(nrows=1, ncols=1)\nax.scatter(common_x_axis, target_0, color='blue', alpha=0.6, label='target')\nax.scatter(common_x_axis, estimate_final, color='orange', alpha=0.6, label='estimate')\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nThese are the numeric results of the training.\n\nfinal_estimate_coeffs = gradient_descent_history[-1]['y_hat_coeffs']\ninitial_rss = gradient_descent_history[0]['rss']\nprint(f'Training iterations {len(gradient_descent_history)}')\nprint(f'Target coefficients {target_coeffs}')\nprint(f'Final estimate coefficients {final_estimate_coeffs}')\nprint(f'Initial RSS {initial_rss}')\nprint(f'Final RSS {rss(common_x_axis, target_coeffs, final_estimate_coeffs)}')\n\nTraining iterations 88\nTarget coefficients [-2.0, 0.0, 2.5, 1.2]\nFinal estimate coefficients [-2.4649992311439837, 0.1551299749689758, 2.4784435198598582, 1.1914759552905416]\nInitial RSS 2015004.0007105006\nFinal RSS 48.4555948418813"
  },
  {
    "objectID": "posts/simple-gradient-descent/index.html#references",
    "href": "posts/simple-gradient-descent/index.html#references",
    "title": "Python: Simple Gradient Descent",
    "section": "",
    "text": "Göbel, Börge. Computational Physics, Section 3.\nJames, Gareth et al. An Introduction to Statistical Learning with Applications in R, Eqn 3.16."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alicia's Side Projects",
    "section": "",
    "text": "AI & Deep Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ai.html",
    "href": "ai.html",
    "title": "AI & Deep Learning",
    "section": "",
    "text": "PyTorch: Deep Learning Organic Chemistry\n\n\n\n\n\n\nAlicia\n\n\nMar 12, 2025\n\n\n\n\n\n\nNo matching items"
  }
]